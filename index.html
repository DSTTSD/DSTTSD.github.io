<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"dsttsd.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.8.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta property="og:type" content="website">
<meta property="og:title" content="DSTの杂货铺">
<meta property="og:url" content="https://dsttsd.github.io/index.html">
<meta property="og:site_name" content="DSTの杂货铺">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="DST">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://dsttsd.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>DSTの杂货铺</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">DSTの杂货铺</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">学习笔记</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">3</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">3</span></a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">11</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="DST"
      src="https://avatars.githubusercontent.com/u/50662067?s=400&u=b552d8b742d1e685ed0ddcc6a97d9f697535fa6b&v=4">
  <p class="site-author-name" itemprop="name">DST</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/DSTTSD" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;DSTTSD" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dsttsd.github.io/2022/06/28/RL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/50662067?s=400&u=b552d8b742d1e685ed0ddcc6a97d9f697535fa6b&v=4">
      <meta itemprop="name" content="DST">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DSTの杂货铺">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/06/28/RL/" class="post-title-link" itemprop="url">蘑菇书笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2022-06-28 12:15:55 / 修改时间：15:06:30" itemprop="dateCreated datePublished" datetime="2022-06-28T12:15:55+08:00">2022-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">强化学习</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="绪论">绪论</h1>
<figure>
<img src="https://s2.loli.net/2022/06/28/vwkt1XV8lnHK6dL.png" alt="" /><figcaption>image-20220502141142624.png</figcaption>
</figure>
<h4 id="基本问题">基本问题</h4>
<ol type="1">
<li><p>强化学习的基本结构是什么？</p>
<p>​ 智能体与环境的交互。（当智能体在环境中得到当前时刻的状态后，其会基于此状态 输出一个动作，这个动作会在环境中被执行并输出下一个状态和当前的这个动作得到的奖励。智能体在环 境里存在的目标是最大化期望累积奖励。）</p></li>
<li><p>强化学习相对于监督学习为什么训练过程会更加困难？（与监督学习的不同）</p>
<ol type="1">
<li>强化学习处理的大多是序列数据，其很难像监督学习的样本一样满足独立同分布条件。</li>
<li>强化学习有奖励的延迟，即智能体的动作作用在环境中时，环境对于智能体状态的奖励存在延迟， 使得反馈不实时</li>
<li>监督学习有正确的标签，模型可以通过标签修正自己的预测来更新模型，而强化学习相当于一个 “试错”的过程，其完全根据环境的“反馈”更新对自己最有利的动作。</li>
</ol></li>
<li><p>强化学习的基本特征有哪些？</p>
<ol type="1">
<li>有试错探索过程</li>
<li>会从环境中获得延迟奖励</li>
<li>数据都是时间关联的</li>
<li>智能体的动作会影响它从环境中得到的反馈</li>
</ol></li>
<li><p>近几年强化学习发展迅速的原因有哪些？</p>
<p>算力的提升、深度强化学习端到端训练</p></li>
<li><p>状态和观测有什么关系？</p>
<p>状态是对环境的完整描述，不会隐藏环境信息。观测是对状态的部分描述，可能会遗漏一些信息。</p></li>
<li><p>一个强化学习智能体由什么组成？</p>
<p>策略函数、价值函数、模型（智能体对当前环境状态的理解）</p></li>
<li><p>根据强化学习智能体的不同，我们可以将其分为哪几类？</p>
<p>基于价值的智能体、基于策略的智能体、演员-评论员</p></li>
<li><p>基于策略迭代和基于价值迭代的强化学习方法有什么区别？</p>
<p>基于策略迭代的强化学习方法，智能体会制定一套动作策略，即确定在给定状态下需要采取何种 动作，并根据该策略进行操作。强化学习算法直接对策略进行优化，使得制定的策略能够获得最大的奖励； 基于价值迭代的强化学习方法，智能体不需要制定显式的策略，它维护一个价值表格或价值函数，并通过 这个价值表格或价值函数来选取价值最大的动作。</p>
<p>基于价值迭代的方法只能应用在离散的环境下，例如围棋或某些游戏领域，对于行为集合规模庞 大或是动作连续的场景，如机器人控制领域，其很难学习到较好的结果（此时基于策略迭代的方法能够根 据设定的策略来选择连续的动作)。</p></li>
</ol>
<h4 id="面试问题">面试问题</h4>
<h5 id="友善的面试官-看来你对于强化学习还是有一定了解的呀那么可以用一句话谈一下你对于强化学习的认识吗">友善的面试官: 看来你对于强化学习还是有一定了解的呀，那么可以用一句话谈一下你对于强化学习的认识吗？</h5>
<p>​ 强化学习包含环境、动作和奖励 3 部分，其本质是智能体通过与环境的交互，使其做出的动作对应的决策得到的总奖励最大，或者说是期望最大。</p>
<h5 id="友善的面试官-请问你认为强化学习监督学习和无监督学习三者有什么区别呢">友善的面试官: 请问，你认为强化学习、监督学习和无监督学习三者有什么区别呢？</h5>
<pre><code>    1. 强化学习和无监督学习是不需要有标签样本的，而监督学习需要许多有标签样本来进行模型的构建和训练。
    2. 对于强化学习与无监督学习，无监督学习直接基于给定的数据进行建模，寻找数据或特征中隐藏的结构，一般对应聚类问题；强化学习需要通过延迟奖励学习策略来得到模型与目标的距离，这个 距离可以通过奖励函数进行定量判断，这里我们可以将奖励函数视为正确目标的一个稀疏、延迟形式。
    3. 强化学习处理的多是序列数据，样本之间通常具有强相关性，但其很难像监督学习的样本一样满足独 立同分布条件。</code></pre>
<h5 id="友善的面试官-根据你的理解你认为强化学习的使用场景有哪些呢">友善的面试官: 根据你的理解，你认为强化学习的使用场景有哪些呢？</h5>
<p>“多序列决策问题”，或者说是对应的模型未知，需要通过学习逐渐逼近真实模型的 问题。并且当前的动作会影响环境的状态，即具有马尔可夫性的问题。同时应满足所有状态是可重复到达 的条件，即满足可学习条件。</p>
<h5 id="友善的面试官-请问强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢">友善的面试官: 请问强化学习中所谓的损失函数与深度学习中的损失函数有什么区别呢？</h5>
<p>深度学习中的损失函数的目的是使预测值和真实值之间的差距尽可能小，而强化学习中的损失函数的目的是使总奖励的期望尽可能大。</p>
<h5 id="友善的面试官-你了解有模型和免模型吗两者具体有什么区别呢">友善的面试官: 你了解有模型和免模型吗？两者具体有什么区别呢？</h5>
<p>是否需要对真实的环境进行建模，免模型方法不需要对环境进行建模，直 接与真实环境进行交互即可，所以其通常需要较多的数据或者采样工作来优化策略，这也使其对于真实环 境具有更好的泛化性能；而有模型方法需要对环境进行建模，同时在真实环境与虚拟环境中进行学习，如 果建模的环境与真实环境的差异较大，那么会限制其泛化性能。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dsttsd.github.io/2022/02/25/week%2012/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/50662067?s=400&u=b552d8b742d1e685ed0ddcc6a97d9f697535fa6b&v=4">
      <meta itemprop="name" content="DST">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DSTの杂货铺">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/25/week%2012/" class="post-title-link" itemprop="url">第十二次周报</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-25 12:15:55" itemprop="dateCreated datePublished" datetime="2022-02-25T12:15:55+08:00">2022-02-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-06-28 12:09:27" itemprop="dateModified" datetime="2022-06-28T12:09:27+08:00">2022-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%91%A8%E6%8A%A5/" itemprop="url" rel="index"><span itemprop="name">周报</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>内容：</p>
<ul>
<li><p>毕设内容</p></li>
<li><p>NCE &amp; InfoNCE</p></li>
</ul>
<p>下周内容：</p>
<ul>
<li>cs224n 生成模型</li>
</ul>
<h4 id="nce">NCE</h4>
<h5 id="多分类问题">多分类问题</h5>
<p>nlp中常常将语言模型当做一个多分类问题，经过softmax后的概率可以被看做给出上下文c，下一个单词w的条件概率分布<img src="https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28w%7Cc%29" alt="[公式]" /></p>
<p>这里进入softmax以前的结果用<img src="https://www.zhihu.com/equation?tex=s_%7B%5Ctheta%7D%28w%2Cc%29" alt="[公式]" />表示，那么</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=+%5Cbegin%7Baligned%7D+p_%7B%5Ctheta%7D%28w%7Cc%29%26%3D+%5Cfrac%7Bexp%28s_%7B%5Ctheta%7D%28w%2Cc%29%29%7D%7B%5Csum_%7Bw%5E%5Cprime+%5Cin+V%7Dexp%28s_%7B%5Ctheta%7D%28w%2Cc%29%29%7D+%5C%5C+%26%3D+%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D%7BZ%28c%29%7D+%5Cend%7Baligned%7D+%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p><span class="math inline">\(Z(C)\)</span>常被看做”配分函数“或”归一化因子“，单词库大小<img src="https://www.zhihu.com/equation?tex=%7CV%7C" alt="[公式]" />常常是很大的，因此该项往往很难直接计算。（如果把配分项看做参数学习，会趋向于零完全失效)</p>
<p>回顾极大似然估计：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D++%5Carg%5Cmax+%5Cmathcal%7BL%7D_%7BMLE%7D+%26%3D+%5Carg%5Cmax+%5Csum_%7Bw+%5Csim+%5Ctilde%7Bp%7D%28w%7Cc%29%7D+%5Clog+p_%7B%5Ctheta%7D%28w+%5Cmid+c%29+%5C%5C+%26%3D%5Carg%5Cmax+%5Cmathbb+E_%7Bw+%5Csim+%5Ctilde%7Bp%7D%28w%7Cc%29%7D+%5Clog%7B%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D%7BZ%28c%29%7D%7D+%5Cend%7Baligned%7D+%5" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>对<span class="math inline">\(\theta\)</span>求导：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D%5Cmathcal%7BL%7D_%7B%5Cmathrm%7BMLE%7D%7D%26%3D%5Cmathbb+E_%7Bw+%5Csim+%5Ctilde%7Bp%7D%28w%7Cc%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D%5Clog%7B%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D%7BZ%28c%29%7D%7D+%5C%5C+%26%3D%5Cmathbb+E_%7Bw+%5Csim+%5Ctilde%7Bp%7D%28w%7Cc%29%7D+%5Cleft%5B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D++%5Clog%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D-+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D%5Clog%7BZ%28c%29%7D+%5Cright%5D+%5C%5C+%26%3D%5Cmathbb+E_%7Bw+%5Csim+%5Ctilde%7Bp%7D%28w%7Cc%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D++%5Clog%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D+-++%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+%5Clog%7BZ%28c%29%7D+%5Cend%7Baligned%7D+%5" alt="" /><figcaption>[公式]</figcaption>
</figure>
<h5 id="nce对于采样的建模">NCE对于采样的建模</h5>
<p>Noise Contrastive Estimation（噪声对比估计）的<strong>核心思想就是通过学习数据分布样本和噪声分布样本之间的区别，从而发现数据中的一些特性</strong>，将问题转换成了一个二分类问题，分类器能够对数据样本和噪声样本进行二分类。</p>
<p>假设一个特定上下文 <img src="https://www.zhihu.com/equation?tex=c" alt="[公式]" /> 的数据分布为 <img src="https://www.zhihu.com/equation?tex=%5Ctilde%7Bp%7D%28w%7Cc%29" alt="[公式]" />，从该分布采样的样本为正样本，记<img src="https://www.zhihu.com/equation?tex=D%3D1" alt="[公式]" />，另一个与上下文无关的噪声分布为<img src="https://www.zhihu.com/equation?tex=q%28w%29" alt="[公式]" /></p>
<p>，从该分布取出的是负样本，记为 <img src="https://www.zhihu.com/equation?tex=D%3D0" alt="[公式]" />假设现在取出了若干正负样本组成一个混合分布 <img src="https://www.zhihu.com/equation?tex=p%28w%7Cc%29" alt="[公式]" />，<strong>负样本与正样本</strong>之比为k（正样本<img src="https://www.zhihu.com/equation?tex=k_d" alt="[公式]" />个，负样本<img src="https://www.zhihu.com/equation?tex=k_n" alt="[公式]" />个)，那么有：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+p%28D%3D0%7Cw%2Cc%29+%26%3D%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7B%5Ctilde%7Bp%7D%28w+%5Cmid+c%29%2Bk+%5Ctimes+q%28w%29%7D+%5C%5C+%5C%5C++p%28D%3D1%7Cw%2Cc%29+%26%3D%5Cfrac%7B%5Ctilde%7Bp%7D%28w+%5Cmid+c%29%7D%7B%5Ctilde%7Bp%7D%28w+%5Cmid+c%29%2Bk+%5Ctimes+q%28w%29%7D+%5Cend%7Baligned%7D+%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>假设标签 <img src="https://www.zhihu.com/equation?tex=D_t" alt="[公式]" /> （属于哪一个分布）为伯努利分布，那么它的对数似然函数（取负值为交叉熵损失)：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cmathcal%7BL%7D%5Ec_%7B%5Cmathrm%7BNCE%7D%7D+%26%3D%5Csum_%7Bt%3D1%7D%5E%7Bk_d%2Bk_n%7D+%5Cleft%5B+D_t+%5Clog+P%28D%3D1%7Cw_t%2Cc%29+%2B%281-D_t%29+%5Clog+P%28D%3D0%7Cw_t%2Cc%29+%5Cright%5D+%5C%5C+%26%3D%5Csum_%7Bt%3D1%7D%5E%7Bk_d%7D%5Clog+P%28D%3D1%7Cw_t%2Cc%29+%2B+%5Csum_%7Bt%3D1%7D%5E%7Bk_n%7D+%5Clog+P%28D%3D0%7Cw_t%2Cc%29+%5C%5C+%26%3D%5Csum_%7Bt%3D1%7D%5E%7Bk_d%7D%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D+%2B+%5Csum_%7Bt%3D1%7D%5E%7Bk_n%7D+%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D+%5C%5C+%5Cend%7Baligned%7D+%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>目标函数需要除以正样本数目（利用大数定律可以看做是期望）：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+J%5Ec_%7BNCE%7D+%26%3D%5Cfrac%7B1%7D%7Bk_d%7D%5Csum_%7Bt%3D1%7D%5E%7Bk_d%7D%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D+%2B+%5Cfrac%7Bk%7D%7Bk_n%7D+%5Csum_%7Bt%3D1%7D%5E%7Bk_n%7D+%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D+%5C%5C+%26%3D%5Cmathbb%7BE%7D_%7Bw+%5Csim+%5Ctilde%7Bp%7D%28w%7Cc%29%7D+%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D+%2B+k+%5Cmathbb%7BE%7D_%7Bw+%5Csim+q%28w%29%7D+%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D+%5Cend%7Baligned%7D+%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<h5 id="nce原理">NCE原理</h5>
<p>将目标函数对<span class="math inline">\(\theta\)</span> 求导：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+J%5Ec_%7BNCE%7D%28%5Ctheta%29%26%3D++%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D++%5Cleft%5B%5Cmathbb%7BE%7D_%7Bw+%5Csim+%5Ctilde%7Bp%7D%28w%7Cc%29%7D%7B%5Clog%7B%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D%7D%7D+%2B+k%5Cmathbb%7BE%7D_%7Bw+%5Csim+q%28w%29%7D+%7B%5Clog%7B%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D%7D%7D+%5Cright%5D+%5C%5C+%26%3D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+%5Csum_%7Bw%7D+%5Ctilde%7Bp%7D%28w%7Cc%29+%5Clog%7B%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D%7D+%2B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+k%5Csum_%7Bw%7Dq%28w%29+%5Clog%7B%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D%7D+%5C%5C+%26%3D%5Csum_%7Bw%7D+%5Ctilde%7Bp%7D%28w%7Cc%29%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D++%5Clog%7B%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D%7D+%2B+k%5Csum_%7Bw%7Dq%28w%29+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D++%5Clog%7B%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D%7D+%5C%5C+%5Cend%7Baligned%7D+%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>对于上面两项继续求导：</p>
<ol type="1">
<li><figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+log%7B%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28%7Bw%7D%29%7D%7D++%26%3D+-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7Dlog%7B%281%2B%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D%29%7D+%5C%5C+%26%3D-%5Cfrac%7B1%7D%7B1%2B%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D%7D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D+%5C%5C+%26%3D-%5Cfrac%7B1%7D%7B1%2B%5Cfrac%7Bk+%5Ctimes+q%28%7Bw%7D%29%7D%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D%7D+%28k+%5Ctimes+q%28w%29%29+%5Cfrac%7B-1%7D+%7B%5Bu_%7B%5Ctheta%7D%28w%2Cc%29%5D%5E2%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+%5Cfrac%7B1%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%7D+%5C%5C+%26%3D%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D+%5Cfrac%7B1%7D%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+%5Cfrac%7B1%7D%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D+%5C%5C+%26%3D%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+log%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D+%5Cend%7Baligned%7D+%5C" alt="" /><figcaption>[公式]</figcaption>
</figure></li>
<li><figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+log%7B%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D%7D++%26%3D-%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7Dlog%7B%281%2B%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D%7Bk+%5Ctimes+q%28w%29%7D%29%7D+%5C%5C+%26%3D-%5Cfrac%7B1%7D%7B1%2B%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D%7Bk+%5Ctimes+q%28w%29%7D%7D%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D%7Bk+%5Ctimes+q%28w%29%7D+%5C%5C+%26%3D-%5Cfrac%7B1%7D%7B1%2B%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D%7Bk+%5Ctimes+q%28w%29%7D%7D+%5Cfrac%7B1%7D%7Bk+%5Ctimes+q%28w%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7Du_%7B%5Ctheta%7D%28w%2Cc%29+%5C%5C+%26%3D-%5Cfrac%7B1%7D%7Bu_%7B%5Ctheta%7D%28w%2Cc%29+%2B+k+%5Ctimes+q%28w%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7Du_%7B%5Ctheta%7D%28w%2Cc%29+%5C%5C+%26%3D-%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D%7Bu_%7B%5Ctheta%7D%28w%2Cc%29+%2B+k+%5Ctimes+q%28w%29%7D+%5Cfrac%7B1%7D%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7Du_%7B%5Ctheta%7D%28w%2Cc%29+%5C%5C+%26%3D-%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D%7Bu_%7B%5Ctheta%7D%28w%2Cc%29+%2B+k+%5Ctimes+q%28w%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7Dlog%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D+%5C%5C+%5Cend%7Baligned%7D+%5C" alt="" /><figcaption>[公式]</figcaption>
</figure></li>
</ol>
<p>将上述式子带回目标函数求导结果：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+J%5Ec_%7BNCE%7D%28%5Ctheta%29+%26%3D%5Csum_%7Bw%7D+%5Ctilde%7Bp%7D%28w%7Cc%29%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D++%5Clog%7B%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D%7D+%2B+k%5Csum_%7Bw%7Dq%28w%29+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D++%5Clog%7B%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D%7D+%5C%5C+%26%3D%5Csum_%7Bw%7D+%5Ctilde%7Bp%7D%28w%7Cc%29+%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+%5Clog%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D+-+k%5Csum_%7Bw%7Dq%28w%29+%5Cfrac%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D%7Bu_%7B%5Ctheta%7D%28w%2Cc%29+%2B+k+%5Ctimes+q%28w%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D%5Clog%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D+%5C%5C+%26%3D%5Csum_%7Bw%7D+%5Ctilde%7Bp%7D%28w%7Cc%29+%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+%5Clog%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D+-+%5Csum_%7Bw%7D+u_%7B%5Ctheta%7D%28w%2Cc%29++%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2Cc%29+%2B+k+%5Ctimes+q%28w%29%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D%5Clog%7Bu_%7B%5Ctheta%7D%28w%2Cc%29%7D+%5C%5C+%26%3D%5Csum_w%7B%5Cleft%5B%5Cfrac%7Bk+%5Ctimes+q%28w%29%7D%7Bu_%7B%5Ctheta%7D%28w%2C+c%29%2Bk+%5Ctimes+q%28w%29%7D+%5Cleft%28%5Ctilde%7Bp%7D%28w%7Cc%29-+u_%7B%5Ctheta%7D%28w%2Cc%29%5Cright%29%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7Dlog+u_%7B%5Ctheta%7D%28w%2Cc%29%5Cright%5D%7D+%5Cend%7Baligned%7D+%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>此处存在两个细节：</p>
<ol type="1">
<li><p>假设配分项<img src="https://www.zhihu.com/equation?tex=Z%28c%29%5Capprox1" alt="[公式]" />（对于参数很多的神经网络来说，配分项每次求梯度时可看做固定的，我们将 <img src="https://www.zhihu.com/equation?tex=z_c" alt="[公式]" /> 固定为 1 对每个 <img src="https://www.zhihu.com/equation?tex=c" alt="[公式]" /> 仍是有效的)。</p>
<p>$</p>
<span class="math display">\[\begin{aligned}
p_{\theta}(w \mid c) &amp;=\frac{\exp \left(s_{\theta}(w, c)\right)}{\sum_{w^{\prime} \in V} \exp \left(s_{\theta}(w, c)\right)} \\
&amp;=\exp \left(s_{\theta}(w, c)\right)\times \exp \left(\theta_{0}\right)\\ 
\end{aligned}\]</span>
<p>$</p>
<p>上述式子对<span class="math inline">\(\theta\)</span> 求导只有第一项的内容，因此后面部分看做是1在梯度更新时是有利的。</p>
<p>如此一来：<img src="https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28w%7Cc%29%3Du_%7B%5Ctheta%7D%28w%2Cc%29" alt="[公式]" /></p></li>
<li><p>当<span class="math inline">\(k\to \infty\)</span>时，<span class="math inline">\(\frac{k \times q(w)}{p_{\theta}(w, c)+k \times q(w)}\)</span>的极限为1。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Clim_%7Bk+%5Cto+%5Cinfty%7D+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7D+J%5Ec_%7BNCE%7D%28%5Ctheta%29+%26%3D%5Clim_%7Bk+%5Cto+%5Cinfty%7D+%5Csum_w%7B%5Cleft%5B%5Cfrac%7Bq%28w%29%7D%7B+%5Cfrac%7Bp_%7B%5Ctheta%7D%28w%7Cc%29%7D%7Bk%7D%2Bq%28w%29%7D+%5Cleft%28%5Ctilde%7Bp%7D%28w%7Cc%29-+p_%7B%5Ctheta%7D%28w%7Cc%29%5Cright%29%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7Dlog+u_%7B%5Ctheta%7D%28w%2Cc%29%5Cright%5D%7D+%5C%5C+%26%3D+%5Csum_w%7B%5Cleft%5B%5Cleft%28%5Ctilde%7Bp%7D%28w%7Cc%29-+p_%7B%5Ctheta%7D%28w%7Cc%29%5Cright%29%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Ctheta%7Dlog+u_%7B%5Ctheta%7D%28w%2Cc%29%5Cright%5D%7D+%5Cend%7Baligned%7D+%5Ctag+%7B24%7D" alt="" /><figcaption>[公式]</figcaption>
</figure></li>
</ol>
<h5 id="实现">实现</h5>
<p>实现时：一般正例采样一次，负例采样k次。</p>
<h3 id="infonce-cpc">InfoNce  CPC</h3>
<p>https://arxiv.org/abs/1807.03748</p>
<h5 id="任务和目的">任务和目的</h5>
<p>CPC(对比预测编码) 就是一种通过无监督任务来学习(编码)高维数据的特征表示(representation)，而通常采取的无监督策略就是根据上下文预测未来或者缺失的信息（nlp中早有分布式表示的概念）。 <img src="https://www.zhihu.com/equation?tex=p%28x_%7Bt%2Bk%7D%7Cc_t%29" alt="[公式]" /> 表示根据当前上下文 <img src="https://www.zhihu.com/equation?tex=c_t" alt="[公式]" /> 预测 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]" /> 个时刻后的数据 <img src="https://www.zhihu.com/equation?tex=x_%7Bt%2Bk%7D" alt="[公式]" /> （假设是像文本、语音中那样的<strong>序列数据</strong>)，可以通过<strong>最大化当前上下文</strong> <img src="https://www.zhihu.com/equation?tex=c_t" alt="[公式]" /> <strong>和要未来的数据</strong> <img src="https://www.zhihu.com/equation?tex=x_%7Bt%2Bk%7D" alt="[公式]" /> <strong>之间的互信息</strong>来构建预测任务。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+I%28x_%7Bt%2Bk%7D+%3B+c_t%29%3D%5Csum_%7Bx%2C+c%7D+p%28x_%7Bt%2Bk%7D%2C+c_t%29+%5Clog+%5Cfrac%7Bp%28x_%7Bt%2Bk%7D+%5Cmid+c_t%29%7D%7Bp%28x_%7Bt%2Bk%7D%29%7D+%5Cend%7Baligned%7D+%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>其中联合分布本身是intractable的，但是我们可以从<img src="https://www.zhihu.com/equation?tex=%5Cfrac%7Bp%28x_%7Bt%2Bk%7D+%5Cmid+c_t%29%7D%7Bp%28x_%7Bt%2Bk%7D%29%7D" alt="[公式]" />入手，这个比例可以被看做密度比（<strong>即贝叶斯中的似然比，给定上下文c第k个单词为<span class="math inline">\(x_{t+k}\)</span>的概率，比一般群体多了多少</strong>)，分子 <img src="https://www.zhihu.com/equation?tex=p%28x_%7Bt%2Bk%7D%7Cc_t%29" alt="[公式]" /> 就相当于 <img src="https://www.zhihu.com/equation?tex=p_d" alt="[公式]" /> ，是我们想得到的目标函数；分母 <img src="https://www.zhihu.com/equation?tex=p%28x_%7Bt%2Bk%7D%29" alt="[公式]" /> 就相当于 <img src="https://www.zhihu.com/equation?tex=p_n" alt="[公式]" /> ，是用来进行对比的参考分布(噪声)。因此类似NCE，将问题转化为二分类问题：</p>
<ol type="1">
<li>从条件 <img src="https://www.zhihu.com/equation?tex=p%28x_%7Bt%2Bk%7D+%5Cmid+c_t%29" alt="[公式]" /> 中取出数据称为“正样本”，它是根据上下文 <img src="https://www.zhihu.com/equation?tex=c_t" alt="[公式]" /> 所做出的预测数据，将它和这个上下文一起组成“正样本对”，类别标签设为 1。</li>
<li>将从 <img src="https://www.zhihu.com/equation?tex=p%28x_%7Bt%2Bk%7D%29" alt="[公式]" /> 中取出的样本称为“负样本”，它是与当前上下文 <img src="https://www.zhihu.com/equation?tex=c_t" alt="[公式]" /> 没有必然关系的随机数据，将它和这个上下文 <img src="https://www.zhihu.com/equation?tex=c_t" alt="[公式]" /> 一起组成“负样本对”，类别标签设为 0。</li>
<li>正样本也就是与 <img src="https://www.zhihu.com/equation?tex=c_t" alt="[公式]" /> 间隔固定步长 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]" /> 的数据，根据 NCE 中说明的设定，正样本选取 1 个；因为在 NCE 中证明了噪声分布与数据分布越接近越好，所以负样本就直接在当前序列中随机选取（只要不是那一个正样本就行），负样本数量越多越好。</li>
</ol>
<p>同样可以按照NCE中的写法：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+p%28x_%7Bt%2Bk%7D%7Cc_t%29%26%3D+%5Cfrac%7Bexp%28s_%7B%5Ctheta%7D%28x_%7Bt%2Bk%7D%2Cc_t%29%29%7D%7B%5Csum_%7Bx_j+%5Cin+X%7Dexp%28s_%7B%5Ctheta%7D%28x_%7Bj%7D%2Cc_t%29%29%7D+%5C%5C+%5Cend%7Baligned%7D+%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p><img src="https://www.zhihu.com/equation?tex=s_%7B%5Ctheta%7D%28x%2Cc%29" alt="[公式]" /> 是一个 socring function ，输出的分数用来量化 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]" /> 在上下文 <img src="https://www.zhihu.com/equation?tex=c" alt="[公式]" /> 中匹配性（NCE例子中是神经网络)， CPC 文章中用余弦相似度来量化，并且将 <img src="https://www.zhihu.com/equation?tex=exp%28s_%7B%5Ctheta%7D%28x_%7Bt%2Bk%7D%2Cc_t%29%29" alt="[公式]" /> 定义为 <img src="https://www.zhihu.com/equation?tex=+f_k%28x_%7Bt%2Bk%7D%2Cc_t%29" alt="[公式]" />：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+p%28x_%7Bt%2Bk%7D%7Cc_t%29%26%3D+%5Cfrac%7Bf_k%28x_%7Bt%2Bk%7D%2Cc_t%29%7D%7B%5Csum_%7Bx_j+%5Cin+X%7Df_k%28x_%7Bj%7D%2Cc_t%29%7D+%5C%5C+%5Cend%7Baligned%7D+%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>写为交叉熵损失，得到：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cmathcal%7BL%7D_%7BN%7D%26%3D-%5Csum_%7BX%7D%5Cleft%5Bp%28x%2Cc%29%5Clog+%5Cfrac%7Bf_%7Bk%7D%5Cleft%28x_%7Bt%2Bk%7D%2C+c_%7Bt%7D%5Cright%29%7D%7B%5Csum_%7Bx_%7Bj%7D+%5Cin+X%7D+f_%7Bk%7D%5Cleft%28x_%7Bj%7D%2C+c_%7Bt%7D%5Cright%29%7D%5Cright%5D+%5C%5C+%26%3D-%5Cmathbb%7BE%7D_X%5Cleft%5B%5Clog+%5Cfrac%7Bf_%7Bk%7D%5Cleft%28x_%7Bt%2Bk%7D%2C+c_%7Bt%7D%5Cright%29%7D%7B%5Csum_%7Bx_%7Bj%7D+%5Cin+X%7D+f_%7Bk%7D%5Cleft%28x_%7Bj%7D%2C+c_%7Bt%7D%5Cright%29%7D%5Cright%5D+%5C%5C+%5Cend%7Baligned%7D+%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<h5 id="与互信息的关系">与互信息的关系</h5>
<p>对于互信息的利用一般分为两类：</p>
<ol type="1">
<li><p>优化互信息上界（upper bound）,一般作为正则项在pretrain使用，优化下游任务（较少）。</p>
<p>Cheng P, Hao W, Dai S, et al. Club: A contrastive log-ratio upper bound of mutual information[C]//International conference on machine learning. PMLR, 2020: 1779-1788.</p></li>
<li><p>优化lower bound, 互信息本身是intractable，因此转化为优化lower bound</p>
<p>Poole B, Ozair S, Van Den Oord A, et al. On variational bounds of mutual information[C]//International Conference on Machine Learning. PMLR, 2019: 5171-5180.</p></li>
</ol>
<p>InfoNCE是第二种互信息估计，最小化 InfoNCE，也就等价于最大化 <img src="https://www.zhihu.com/equation?tex=x_%7Bt%2Bk%7D" alt="[公式]" /> 和 <img src="https://www.zhihu.com/equation?tex=c_t" alt="[公式]" /> 之间互信息的下限：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cmathcal%7BL%7D_%7B%5Cmathrm%7BN%7D%7D%5E%7B%5Cmathrm%7Bopt%7D%7D+%26%3D-%5Cunderset%7BX%7D%7B%5Cmathbb%7BE%7D%7D+%5Clog+%5Cleft%5B%5Cfrac%7B%5Cfrac%7Bp%5Cleft%28x_%7Bt%2Bk%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bt%2Bk%7D%5Cright%29%7D%7D%7B%5Cfrac%7Bp%5Cleft%28x_%7Bt%2Bk%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bt%2Bk%7D%5Cright%29%7D%2B%5Csum_%7Bx_%7Bj%7D+%5Cin+X_%7B%5Cmathrm%7Bneg%7D%7D%7D+%5Cfrac%7Bp%5Cleft%28x_%7Bj%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bj%7D%5Cright%29%7D%7D%5Cright%5D+%5C%5C+%26%3D%5Cunderset%7BX%7D%7B%5Cmathbb%7BE%7D%7D+%5Clog+%5Cleft%5B1%2B%5Cfrac%7Bp%5Cleft%28x_%7Bt%2Bk%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bt%2Bk%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D+%5Csum_%7Bx_%7Bj%7D+%5Cin+X_%7B%5Cmathrm%7Bneg%7D%7D%7D+%5Cfrac%7Bp%5Cleft%28x_%7Bj%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bj%7D%5Cright%29%7D%5Cright%5D+%5C%5C+%26+%5Capprox+%5Cunderset%7BX%7D%7B%5Cmathbb%7BE%7D%7D+%5Clog+%5Cleft%5B1%2B%5Cfrac%7Bp%5Cleft%28x_%7Bt%2Bk%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bt%2Bk%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%28N-1%29+%5Cunderset%7Bx_%7Bj%7D%7D%7B%5Cmathbb%7BE%7D%7D+%5Cfrac%7Bp%5Cleft%28x_%7Bj%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bj%7D%5Cright%29%7D%5Cright%5D+%5C%5C+%26%3D%5Cunderset%7BX%7D%7B%5Cmathbb%7BE%7D%7D+%5Clog+%5Cleft%5B1%2B%5Cfrac%7Bp%5Cleft%28x_%7Bt%2Bk%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bt%2Bk%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%28N-1%29%5Cright%5D+%5C%5C+%26+%5Cgeq+%5Cunderset%7BX%7D%7B%5Cmathbb%7BE%7D%7D+%5Clog+%5Cleft%5B%5Cfrac%7Bp%5Cleft%28x_%7Bt%2Bk%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bt%2Bk%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D+N%5Cright%5D+%5C%5C+%26%3D-I%5Cleft%28x_%7Bt%2Bk%7D%2C+c_%7Bt%7D%5Cright%29%2B%5Clog+%28N%29+%5Cend%7Baligned%7D+%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<h5 id="与mine关系">与MINE关系</h5>
<p>这篇论文中作者还将infoNCE与MINE(另一个互信息估计)做了比较，infoNCE比MINE多了个常数。对于复杂任务，两者效果都很好，但对于简单任务MINE会不稳定。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dsttsd.github.io/2022/02/18/week%2011/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/50662067?s=400&u=b552d8b742d1e685ed0ddcc6a97d9f697535fa6b&v=4">
      <meta itemprop="name" content="DST">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DSTの杂货铺">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/18/week%2011/" class="post-title-link" itemprop="url">第十一次周报</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-18 12:20:55" itemprop="dateCreated datePublished" datetime="2022-02-18T12:20:55+08:00">2022-02-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-06-28 12:02:56" itemprop="dateModified" datetime="2022-06-28T12:02:56+08:00">2022-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%91%A8%E6%8A%A5/" itemprop="url" rel="index"><span itemprop="name">周报</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>内容：</p>
<ul>
<li><p>louis philippe morency 《多模态机器学习》 完成</p></li>
<li><p>kaggle fashion recommendation competition:</p>
<ol type="1">
<li>利用swing transformers提取了图片特征</li>
<li></li>
</ol></li>
<li><p>论文阅读：</p>
<ol type="1">
<li>sequential recommendation</li>
<li>boosting相关</li>
</ol></li>
</ul>
<h2 id="sequential-recommendation">1 sequential recommendation</h2>
<h3 id="session-based-recommendations-with-recurrent-neural-networksiclr-2016">Session-based recommendations with recurrent neural networks（ICLR 2016）</h3>
<h4 id="背景">背景：</h4>
<p>GRU4rec是使用神经网络做序列推荐的经典基于会话的推荐方法（基本根据物品推荐，缺少用户侧信息），这篇文章出现前主要有基于物品的协同过滤和基于马尔可夫决策过程的方法。</p>
<ul>
<li>基于物品的协同过滤，需要维护一张物品的相似度矩阵，当用户在一个session中点击了某一个物品时，基于相似度矩阵得到相似的物品推荐给用户。这种方法简单有效，并被广泛应用，但是这种方法只把用户上一次的点击考虑进去，而没有把前面多次的点击都考虑进去。</li>
<li>基于马尔可夫决策过程（MDP）的推荐方法，其主要学习的是状态转移概率，即点击了物品A之后，下一次点击的物品是B的概率，并基于这个状态转移概率进行推荐。这样的缺陷主要是随着物品的增加，建模所有的可能的点击序列是十分困难的</li>
</ul>
<h4 id="主要内容">主要内容:</h4>
<p><img src="https://s2.loli.net/2022/03/01/5SIPc3DrZxXTUju.png" /></p>
<p>输入：对于一个Session中的点击序列<span class="math inline">\(x=[x_1,x_2,x_3...x_{r-1},x_r]\)</span>，依次将<span class="math inline">\(x_1,x_2,x_3...x_{r-1},x_r\)</span>输入到模型中。</p>
<p>模型：序列中的每一个物品xt被转换为one-hot，随后转换成其对应的embedding，经过N层GRU单元后，经过一个全联接层得到下一次每个物品被点击的概率。</p>
<p>loss：</p>
<ol type="1">
<li>bpr：<span class="math inline">\({\cal L}_{s}=-\frac{1}{N_{S}}\cdot\sum_{j=1}^{N_{S}}{\sigma}\left(\hat{r}_{s,i}\,-\,\hat{r}_{s,j}\right)\)</span>。经典bpr让正样本和负样本之间得分差尽可能大。</li>
<li>top1：<span class="math inline">\({\cal L}_{s}=\frac{1}{N_{S}}\cdot\sum_{j=1}^{N_{S}}{\sigma}\left(\hat{r}_{s,j}\,-\,\hat{r}_{s,i}\right)\,+\,\sigma\,\left(\hat{r}_{s,j}^{2}\right)\)</span>第一项让正负样本之间差尽可能大（与bpr稍微有区别），第二项对负样本添加了正则项。</li>
</ol>
<p>trick：</p>
<ol type="1">
<li><p>parallel：为了更好的并行计算，论文采用了 mini-batch 的处理，即把不同的session拼接起来（看纵向）</p>
<p><img src="https://s2.loli.net/2022/03/01/8VgQimsqyUWKtAo.png" /></p></li>
<li><p>sampling on output：物品数量如果过多的话，模型输出的维度过多，计算量会十分庞大，因此在实践中一般采取负采样的方法。论文采用了取巧的方法来减少采样需要的计算量，即选取了同一个batch 中其他 sequence 下一个点击的 item作为负样本，用这些正负样本来训练整个神经网络。</p></li>
</ol>
<h3 id="recurrent-neural-networks-with-top-k-gains-for-session-based-recommendationscikm-2018">Recurrent Neural Networks with Top-k Gains for Session-based Recommendations（CIKM 2018）</h3>
<h4 id="背景-1">背景：</h4>
<p>这篇文章从采样和损失函数角度对GRU4rec进行了优化，即很多序列推荐论文中的GRU4rec+对比算法。</p>
<h4 id="主要内容-1">主要内容</h4>
<h5 id="采样">采样</h5>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/19052848-72a51dcc63c1a72a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/992/format/webp" alt="" /><figcaption>img</figcaption>
</figure>
<p>在GRU4rec基础上，除了batch sampling(从batch中快速得到负样本)，作者引入了额外的负样本（所有batch共享的）。</p>
<h5 id="loss">loss</h5>
<ol type="1">
<li><p>cross-entropy：</p>
<p><img src="https://math.jianshu.com/math?formula=L_%7Bxe%7D%20%3D%20-log%5C%2Cs_i%20%3D%20-log%5C%2C%5Cfrac%7Be%5E%7Br_i%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20e%5E%7Br_j%7D%7D" alt="L_{xe} = -log,s_i = -log," /> 当存在一个 <img src="https://math.jianshu.com/math?formula=r_k%20%3E%3E%20r_i" alt="r_k &gt;&gt; r_i" /> 时，<img src="https://math.jianshu.com/math?formula=s_i" alt="s_i" /> 趋近于 <img src="https://math.jianshu.com/math?formula=0" alt="0" />，<img src="https://math.jianshu.com/math?formula=log%5C%2C0" alt="log,0" /> 未定义，造成结果不稳定。针对这个问题论文提出了2种代替 <img src="https://math.jianshu.com/math?formula=-log%5C%2Cs_i" alt="-log,s_i" /> 的计算方式：</p>
<ul>
<li><img src="https://math.jianshu.com/math?formula=-log(s_i%20%2B%20%5Cvarepsilon%20)" alt="-log(s_i + )" />，其中 <img src="https://math.jianshu.com/math?formula=%5Cvarepsilon%20%3D%2010%5E%7B-24%7D" alt="= 10^{-24}" />。</li>
<li>去掉<span class="math inline">\(r_i\)</span>对应的log，降低其为0情况<img src="https://math.jianshu.com/math?formula=-r_i%20%2B%20log%5C%2C%5Csum_%7Bj%3D1%7D%5E%7BN%7D%20e%5E%7Br_j%7D" alt="-r_i + log,_{j=1}^{N} e^{r_j}" /></li>
</ul></li>
<li><p>ranking losses:</p>
<ul>
<li><strong>TOP1</strong> <img src="https://math.jianshu.com/math?formula=L_%7Btop1%7D%20%3D%20%5Cfrac%20%7B1%7D%7BN_s%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN_s%7D%20%5Csigma%20(r_%7Bj%7D%20-%20r_%7Bi%7D)%20%2B%20%5Csigma%20(r_%7Bj%7D%20%5E2)" alt="L_{top1} =  {j=1}^{N_s} (r{j} - r_{i}) + (r_{j} ^2)" /></li>
<li><strong>BPR</strong> [L_{bpr} =-  _{j=1}^{N_s} log((r_i - r_j))](https://math.jianshu.com/math?formula=L_%7Bbpr%7D%20%3D-%20%5Cfrac%7B1%7D%7BN_s%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN_s%7D%20log(%5Csigma%20(r_i%20-%20r_j))</li>
</ul>
<p>其中，<img src="https://math.jianshu.com/math?formula=N_s" alt="N_s" /> 为负样本量大小，<img src="https://math.jianshu.com/math?formula=r_k" alt="r_k" /> 为 item <code>k</code> 的分数，<code>i</code> 代表正样本，<code>j</code> 代表负样本。</p>
<p>这两种损失函数主要缺点是会发生梯度消失的现象，如当 <img src="https://math.jianshu.com/math?formula=r_%7Bj%7D%20%3C%3C%20r_i" alt="r_{j} &lt;&lt; r_i" /> 时，<img src="https://math.jianshu.com/math?formula=%5Csigma%20(r_%7Bj%7D%20-%20r_%7Bi%7D)" alt="(r_{j} - r_{i})" /> 和 <img src="https://math.jianshu.com/math?formula=1-%5Csigma%20(r_i%20-%20r_j)" alt="1-(r_i - r_j)" /> 都会趋近于 <img src="https://math.jianshu.com/math?formula=0" alt="0" />，从而导致梯度消失（负样本得分远小于正样本）；同时对负样本取平均值会加速这种梯度消失的现象(样本量越多，平均值越小)。</p>
<p>[ = -  <em>{j=1}^{N_s} (r</em>{j} - r_{i}) (1- (r_{j} - r_{i}))](https://math.jianshu.com/math?formula=%5Cfrac%7B%5Cpartial%20L_%7Btop1%7D%7D%7B%5Cpartial%20r_i%7D%20%3D%20-%20%5Cfrac%20%7B1%7D%7BN_s%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN_s%7D%20%5Csigma%20(r_%7Bj%7D%20-%20r_%7Bi%7D)%20(1-%20%5Csigma%20(r_%7Bj%7D%20-%20r_%7Bi%7D))</p>
<p>[ =-  _{j=1}^{N_s} (1-(r_i - r_j))](https://math.jianshu.com/math?formula=%5Cfrac%7B%5Cpartial%20L_%7Bbpr%7D%7D%7B%5Cpartial%20r_i%7D%20%3D-%20%5Cfrac%7B1%7D%7BN_s%7D%20%5Csum_%7Bj%3D1%7D%5E%7BN_s%7D%20(1-%5Csigma%20(r_i%20-%20r_j))</p></li>
<li><p>ranking-max losses:</p>
<p>为了克服梯度消失的情况，作者提出了ranking-max的loss框架：</p>
<p><span class="math inline">\({\cal L}_{\mathrm{pairwise-max}}\left(r_{i},\{r_{j}\}_{j=1}^{N_{S}}\right)={\cal L}_{\mathrm{pairwise}}(r_{i},{\bf m}_{\mathrm{ax}}\,r_{j})\)</span></p></li>
</ol>
<ul>
<li><strong>TOP1-max</strong> [L_{top1-max} = <em>{j=1}^{N_s} s_j((r</em>{j} - r_{i}) + (r_{j} ^2))](https://math.jianshu.com/math?formula=L_%7Btop1-max%7D%20%3D%20%5Csum_%7Bj%3D1%7D%5E%7BN_s%7D%20s_j(%5Csigma%20(r_%7Bj%7D%20-%20r_%7Bi%7D)%20%2B%20%5Csigma%20(r_%7Bj%7D%20%5E2))</li>
</ul>
<p>[ = - <em>{j=1}^{N_s}s_j (r</em>{j} - r_{i}) (1- (r_{j} - r_{i}))](https://math.jianshu.com/math?formula=%5Cfrac%7B%5Cpartial%20L_%7Btop1-max%7D%7D%7B%5Cpartial%20r_i%7D%20%3D%20-%20%5Csum_%7Bj%3D1%7D%5E%7BN_s%7Ds_j%20%5Csigma%20(r_%7Bj%7D%20-%20r_%7Bi%7D)%20(1-%20%5Csigma%20(r_%7Bj%7D%20-%20r_%7Bi%7D)) 其中 maxsoft score <img src="https://math.jianshu.com/math?formula=s_j" alt="s_j" /> 相当于权重（对每个负样本加权重，sj和为1），当 <img src="https://math.jianshu.com/math?formula=r_j" alt="r_j" /> 较小时，<img src="https://math.jianshu.com/math?formula=s_j" alt="s_j" /> 也会较小(趋近于 0)，样本 <img src="https://math.jianshu.com/math?formula=j" alt="j" /> 类似于被忽略，所以不会减少整体的梯度。</p>
<ul>
<li><strong>BPR-max</strong> <img src="https://math.jianshu.com/math?formula=L_%7Bbpr-max%7D%20%3D-%20log%20%5Csum_%7Bj%3D1%7D%5E%7BN_s%7D%20s_j%20%5Csigma%20(r_i%20-%20r_j)" alt="L_{bpr-max} =- log _{j=1}^{N_s} s_j (r_i - r_j)" /> <span class="math inline">\({\frac{\partial{\cal L}_{\mathrm{bpr}-\mathrm{max}}}{\partial r_{k}}}=s_{k}-{\frac{s_{k}\sigma^{2}(r_{i}-r_{k})}{\sum_{j=1}^{N_{S}}s_{j}\sigma(r_{i}-r_{j})}}\)</span> 这里的梯度信息可以看做是单个梯度的加权平均值，<img src="https://math.jianshu.com/math?formula=s_j%20%5Csigma%20(r_i%20-%20r_j)" alt="s_j (r_i - r_j)" /> 相当于权重。当 <img src="https://math.jianshu.com/math?formula=r_i" alt="r_i" /> 较小时，权重分布较为均匀，实际得分高的将会得到更多关注；当 <img src="https://math.jianshu.com/math?formula=r_i" alt="r_i" /> 较大时，得分高的才会产生较大的权重，从而得到更多关注。这有利于模型的训练。</li>
</ul>
<p><strong>TOP1-max</strong> 和 <strong>BPR-max</strong> 的梯度信息均和 softmax score 成比例，这意味着只有 score 较大的 item 会被更新，这样的好处在于模型训练过程中会一直推动 target 往排序列表的顶部前进，而常规的 TOP1 和 BPR 在 target 快接近顶部的时候，平均梯度信息更小了，更新几乎停滞，这样很难将 target 推至顶部。</p>
<ul>
<li>BPR-max with score regularization 受 TOP1 添加正则项的启发，对 BPR 添加正则项，同样能提高模型的表现。 <img src="https://math.jianshu.com/math?formula=L_%7Bbpr-max%7D%20%3D-%20log%20%5Csum_%7Bj%3D1%7D%5E%7BN_s%7D%20s_j%20%5Csigma%20(r_i%20-%20r_j)%20%2B%20%5Clambda%20%5Csum_%7Bj%3D1%7D%5E%7BN_s%7D%20s_j%20r_j%5E2" alt="L_{bpr-max} =- log {j=1}^{N_s} s_j (r_i - r_j) + {j=1}^{N_s} s_j r_j^2" /> 其中，<img src="https://math.jianshu.com/math?formula=%5Clambda" alt="" /> 为正则项参数。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"># 传统bpr</span><br><span class="line">class BPRLoss(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(BPRLoss, self).__init__()</span><br><span class="line"></span><br><span class="line">    def forward(self, logit):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Args:</span><br><span class="line">            logit (BxB): Variable that stores the logits for the items in the mini-batch</span><br><span class="line">                         The first dimension corresponds to the batches, and the second</span><br><span class="line">                         dimension corresponds to sampled number of items to evaluate</span><br><span class="line">                         B,N</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # differences between the item scores</span><br><span class="line">        diff = logit.diag().view(-1, 1).expand_as(logit) - logit</span><br><span class="line">        # final loss</span><br><span class="line">        loss = -torch.mean(F.logsigmoid(diff))</span><br><span class="line">        return loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class BPR_max(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(BPR_max, self).__init__()</span><br><span class="line">    def forward(self, logit):</span><br><span class="line">    	# 通过softmax表示sj</span><br><span class="line">        logit_softmax = F.softmax(logit, dim=1)</span><br><span class="line">        diff = logit.diag().view(-1, 1).expand_as(logit) - logit</span><br><span class="line">        loss = -torch.log(torch.mean(logit_softmax * torch.sigmoid(diff)))</span><br><span class="line">        return loss</span><br><span class="line"># top1loss</span><br><span class="line"></span><br><span class="line">class TOP1Loss(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(TOP1Loss, self).__init__()</span><br><span class="line">    def forward(self, logit):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Args:</span><br><span class="line">            logit (BxB): Variable that stores the logits for the items in the mini-batch</span><br><span class="line">                         The first dimension corresponds to the batches, and the second</span><br><span class="line">                         dimension corresponds to sampled number of items to evaluate</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)</span><br><span class="line">        loss = torch.sigmoid(diff).mean() + torch.sigmoid(logit ** 2).mean()</span><br><span class="line">        return loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class TOP1_max(nn.Module):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(TOP1_max, self).__init__()</span><br><span class="line"></span><br><span class="line">    def forward(self, logit):</span><br><span class="line">        logit_softmax = F.softmax(logit, dim=1)</span><br><span class="line">        diff = -(logit.diag().view(-1, 1).expand_as(logit) - logit)</span><br><span class="line">        loss = torch.mean(logit_softmax * (torch.sigmoid(diff) + torch.sigmoid(logit ** 2)))</span><br><span class="line">        return loss</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="基于注意力的方法-transformer-based-methods"><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/273480841">基于注意力的方法</a>&amp; transformer-based methods：</h2>
<h3 id="sasrec-self-attentive-sequential-recommendationicdm-2018">SASRec: Self-Attentive Sequential Recommendation(ICDM 2018)</h3>
<h4 id="背景-2">背景：</h4>
<p>之前的序列建模方法：</p>
<ul>
<li>基于MC的方法，通过一个简单的假设，进行信息的状态转移。在高稀疏数据下表现好，但在更复杂的场景中很难捕捉到有用的信息，文章实验也证明了这点；</li>
<li>基于RNN的方法有很强的表达能力，但是训练需要大量的数据，在密集型数据集下表现得更好，且因为每一个隐藏状态都必须依赖于前一个，运行效率较低；</li>
</ul>
<p><strong>注意力机制背后的思想是连续的输出都依赖于某个输入的“相关”部分，而模型应该连续地关注这些输入</strong>，在推荐领域，AFM、DIN模型已经得到了很好的应用,详见<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/273480841">基于注意力的方法</a></p>
<h4 id="主要内容-2">主要内容：</h4>
<figure>
<img src="https://mmbiz.qpic.cn/mmbiz_png/qP8JRnW6T3rZdKL35xvIMhubE5MDAnn48HDssvd8koHhKozB9Hv9YhgodW9NJ0CxthsSXypYibTSGbAC735cAMw/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="" /><figcaption>图片</figcaption>
</figure>
<ol type="1">
<li>Postional embedding(PE):即transformer中的位置编码表示先后顺序。对于稀疏数据集可不加pe，因为用户没有太多的记录，因此购买顺序没有太大关系。</li>
</ol>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cwidehat%7B%5Cmathbf%7BE%7D%7D%3D%5Cleft%5B%5Cbegin%7Barray%7D%7Bc%7D%5Cmathbf%7BM%7D_%7Bs_%7B1%7D%7D%2B%5Cmathbf%7BP%7D_%7B1%7D+%5C%5C+%5Cmathbf%7BM%7D_%7Bs_%7B2%7D%7D%2B%5Cmathbf%7BP%7D_%7B2%7D+%5C%5C+%5Ccdots+%5C%5C+%5Cmathbf%7BM%7D_%7Bs_%7Bn%7D%7D%2B%5Cmathbf%7BP%7D_%7Bn%7D%5Cend%7Barray%7D%5Cright%5D+%5C%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<ol start="2" type="1">
<li><p>Self-Attention层</p>
<p>原transformer中self-attention(K=V):</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Attention%28%5Cmathbf%7BQ%2CK%2CV%7D%29%3Dsoftmax%28%5Cfrac%7B%5Cmathbf%7BQK%5ET%7D%7D%7B%5Csqrt%7Bd%7D%7D%29%5Cmathbf%7BV%7D+%5C%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>在本文中对应的是<img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BS%7D%3DSA%28%5Chat%7B%5Cmathbf%7BE%7D%7D%29%3DAttention%28%5Chat%7B%5Cmathbf%7BE%7D%7D%5Cmathbf%7BW%7D%5EQ%2C+%5Chat%7B%5Cmathbf%7BE%7D%7D%5Cmathbf%7BW%7D%5EK%2C+%5Chat%7B%5Cmathbf%7BE%7D%7D%5Cmathbf%7BW%7D%5EV%29+%5C%5C" alt="[公式]" /></p>
<p>此时K!=V,作者认为这样可以让模型更加灵活，比如对于&lt;query q, key k&gt;以及&lt; key k， query q&gt;就可以有不同的表示。</p>
<p>此外， 考虑到不同维度隐藏特征的非线性交互，本文与Transformer一样，在self-attention之后，采取两层的前馈网络：</p></li>
<li><p>stack transformer</p>
<p><strong>一般而言叠加多个自注意力机制层能够学习更复杂的特征转换</strong>，但会存在一些问题：</p>
<ol type="1">
<li><p>模型更容易过拟合；</p></li>
<li><p>训练过程不稳定（梯度消失问题等）；</p></li>
<li><p>模型需要学习更多的参数以及需要更长的时间训练；</p>
<p>因此，作者在自注意力机制层和前馈网络<strong>「加入残差连接、Layer Normalization、Dropout」</strong>来抑制模型的过拟合。（依旧按照transformer设置）</p></li>
</ol>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bc%7D%5Cmathbf%7BS%7D%5E%7B%28b%29%7D%3D%5Coperatorname%7BSA%7D%5Cleft%28%5Cmathbf%7BF%7D%5E%7B%28b-1%29%7D%5Cright%29+%5C%5C+%5Cmathbf%7BF%7D_%7Bi%7D%5E%7B%28b%29%7D%3D%5Coperatorname%7BFFN%7D%5Cleft%28%5Cmathbf%7BS%7D_%7Bi%7D%5E%7B%28b%29%7D%5Cright%29%2C+%5Cquad+%5Cforall+i+%5Cin%5C%7B1%2C2%2C+%5Cldots%2C+n%5C%7D%5Cend%7Barray%7D+%5C%5C" alt="" /><figcaption>[公式]</figcaption>
</figure></li>
<li><p>prediction（与训练好的item embeddings做点积，实际实现时用的都是同一个item embedding）</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=r_%7Bi%2Ct%7D%3D%5Cmathbf%7BF%7D%5E%7B%28b%29%7D_t%5Cmathbf%7BN%7D%5ET_i+%5C%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>作者还尝试通过引入user embedding，但是没有提高性能：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=r_%7Bu%2C+i%2C+t%7D%3D%5Cleft%28%5Cmathbf%7BU%7D_%7Bu%7D%2B%5Cmathbf%7BF%7D_%7Bt%7D%5E%7B%28b%29%7D%5Cright%29+%5Cmathbf%7BM%7D_%7Bi%7D%5E%7BT%7D+%5C%5C" alt="" /><figcaption>[公式]</figcaption>
</figure></li>
</ol>
<h4 id="对比bstkdd2019">对比BST（KDD2019）</h4>
<p>Behavior Sequence Transformer for E-commerce Recommendation in Alibaba这篇论文同样是使用了transformer应用在序列推荐中，但是BST面向排序阶段（SAS面向召回阶段），把序列推荐看成一个CTR任务， 引入了（other features: 用户基本特征、物品基本特征、上下文信息等），模型结构：</p>
<figure>
<img src="https://imgconvert.csdnimg.cn/aHR0cHM6Ly9tbWJpei5xcGljLmNuL21tYml6X3BuZy9xUDhKUm5XNlQzcmZFbXJyckU1Qm5xWGRjaWFqcFRhVEVxNmVPaEM3TWc4ZHN5V2ljNExkSkd3akNKUEV6SHlaNWxRVFVRUjQ1NFJhVHVlSG84aWJRTWpaUS82NDA?x-oss-process=image/format,png" alt="" /><figcaption>img</figcaption>
</figure>
<h3 id="fdsa-feature-level-deeper-self-attention-network-for-sequential-recommendationijcai-2019">FDSA: Feature-level Deeper Self-Attention Network for Sequential Recommendation(IJCAI 2019)</h3>
<h5 id="背景-3">背景：</h5>
<p>SAS只考虑物品之间的顺序模式，而忽略了有助于捕获用户细粒度兴趣偏好的特征之间的顺序模式。 事实上，我们的日常item-item关系是十分重要的。例如，用户在买了衣服之后更可能买鞋子，显示出了下一个产品的类别与当前产品的类别具有高度的相关性。</p>
<p>作者将用户对结构化属性(例如类别)不断变化的需求称为显式的特征转换。此外，物品还可能包含一些非结构化属性，如描述文本或图像，这些属性表示物品的更多细节。因此，作者希望从这些非结构化属性中挖掘用户潜在的特征级别模式，称之为隐式特征转换。</p>
<p>因此FDSA通过对物品序列和特征序列分别应用不同的自注意块，对显式和隐式特征转换进行建模。为了获得隐式的特征转换，增加了一种vanilla注意机制，以帮助基于特征的自注意力块自适应地从各种物品属性中选择重要的特征。</p>
<h5 id="主要工作">主要工作：</h5>
<figure>
<img src="https://img-blog.csdnimg.cn/20190927111217835.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM1MTQ4NzU4,size_16,color_FFFFFF,t_70" alt="" /><figcaption>在这里插入图片描述</figcaption>
</figure>
<p>FDSA由5部分组成，Embedding layer, Vanilla attention layer, Item-based self-attention block, Feature-based self-attention block, 以及FFN。</p>
<ol type="1">
<li><p>首先将稀疏的物品以及物品的离散属性映射到低维稠密向量（embedding）。对物品的文本属性，利用主题模型提取关键词，然后利用word2vec获得关键词的文本向量表示。</p></li>
<li><p>物品的特征通常是异构的，利用传统注意力从物品的各种特征中选择重要的特征。</p>
<figure>
<img src="https://img-blog.csdnimg.cn/20190927113214137.png" alt="" /><figcaption>在这里插入图片描述</figcaption>
</figure>
<p>$ vec(c_i), vec(b_i) <span class="math inline">\(为物品类别和品牌的稠密向量表示，\)</span> vec(item_i^{text}) $ 表示文本i的文本特征表示。</p>
<p>注意力得分为：<img src="https://img-blog.csdnimg.cn/20190927113937867.png" alt="在这里插入图片描述" /></p></li>
<li><p>物品级和序列级特征分别输入transformer，最后拼接在一起得到最后预测结果。</p></li>
</ol>
<h3 id="bert4rec-sequential-recommendation-with-bidirectional-encoder-representations-from-transformercikm-2019">BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer(CIKM 2019)</h3>
<p>将Bert应用到推荐，完全相同的模型（没有next sentence prediction，因为无意义）。</p>
<figure>
<img src="https://img-blog.csdnimg.cn/20190621143944488.png" alt="" /><figcaption>img</figcaption>
</figure>
<h3 id="s3-rec-self-supervised-learning-for-sequential-recommendation-with-mimcikm2020">S3-Rec: Self-Supervised Learning for Sequential Recommendation with MIM(CIKM2020)</h3>
<h5 id="背景-4">背景：</h5>
<ol type="1">
<li><p>现有的神经网络模型利用预测损失来学习模型参数或者嵌入表征，但是这样的训练受限于数据稀疏问题。</p></li>
<li><p>各类数据之间的关系没有被充分挖掘出来——上下文数据和序列数据之间的关联或融合一直不太好捕捉并用于序列推荐（pretrain对于提高性能有影响，说明直接单一目标函数训练是不够的）。</p></li>
</ol>
<h5 id="主要内容-3">主要内容：</h5>
<p>S3-rec利用原有数据的相关性来构建自监督信号，并通过预训练方法来增强数据表示，以改善序列推荐。<strong>它利用互信息最大化（MIM）原理来学习属性，物品，子序列和序列之间的相关性。</strong></p>
<h6 id="互信息最大化">互信息最大化</h6>
<p>互信息：信息论中，互信息是不确定性减少的度量。让互信息最大化，就是让不确定性尽可能降低，充分挖掘数据之间的关系。</p>
<figure>
<img src="https://images2018.cnblogs.com/blog/1078607/201805/1078607-20180502145330252-1542574208.png" alt="" /><figcaption>img</figcaption>
</figure>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/16043538-ee698fab4084cf24.png?imageMogr2/auto-orient/strip%7CimageView2/2/format/webp" alt="" /><figcaption>img</figcaption>
</figure>
<p>互信息（KL(p(x,y)||p(x)p(y))）一般通过NCE实现（NCE来自NLP，是互信息的下限https://arxiv.org/pdf/1807.03748.pdf）：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cmathcal%7BL%7D_%7B%5Cmathrm%7BN%7D%7D%5E%7B%5Cmathrm%7Bopt%7D%7D+%26%3D-%5Cunderset%7BX%7D%7B%5Cmathbb%7BE%7D%7D+%5Clog+%5Cleft%5B%5Cfrac%7B%5Cfrac%7Bp%5Cleft%28x_%7Bt%2Bk%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bt%2Bk%7D%5Cright%29%7D%7D%7B%5Cfrac%7Bp%5Cleft%28x_%7Bt%2Bk%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bt%2Bk%7D%5Cright%29%7D%2B%5Csum_%7Bx_%7Bj%7D+%5Cin+X_%7B%5Cmathrm%7Bneg%7D%7D%7D+%5Cfrac%7Bp%5Cleft%28x_%7Bj%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bj%7D%5Cright%29%7D%7D%5Cright%5D+%5C%5C+%26%3D%5Cunderset%7BX%7D%7B%5Cmathbb%7BE%7D%7D+%5Clog+%5Cleft%5B1%2B%5Cfrac%7Bp%5Cleft%28x_%7Bt%2Bk%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bt%2Bk%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D+%5Csum_%7Bx_%7Bj%7D+%5Cin+X_%7B%5Cmathrm%7Bneg%7D%7D%7D+%5Cfrac%7Bp%5Cleft%28x_%7Bj%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bj%7D%5Cright%29%7D%5Cright%5D+%5C%5C+%26+%5Capprox+%5Cunderset%7BX%7D%7B%5Cmathbb%7BE%7D%7D+%5Clog+%5Cleft%5B1%2B%5Cfrac%7Bp%5Cleft%28x_%7Bt%2Bk%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bt%2Bk%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%28N-1%29+%5Cunderset%7Bx_%7Bj%7D%7D%7B%5Cmathbb%7BE%7D%7D+%5Cfrac%7Bp%5Cleft%28x_%7Bj%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bj%7D%5Cright%29%7D%5Cright%5D+%5C%5C+%26%3D%5Cunderset%7BX%7D%7B%5Cmathbb%7BE%7D%7D+%5Clog+%5Cleft%5B1%2B%5Cfrac%7Bp%5Cleft%28x_%7Bt%2Bk%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bt%2Bk%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D%28N-1%29%5Cright%5D+%5C%5C+%26+%5Cgeq+%5Cunderset%7BX%7D%7B%5Cmathbb%7BE%7D%7D+%5Clog+%5Cleft%5B%5Cfrac%7Bp%5Cleft%28x_%7Bt%2Bk%7D%5Cright%29%7D%7Bp%5Cleft%28x_%7Bt%2Bk%7D+%5Cmid+c_%7Bt%7D%5Cright%29%7D+N%5Cright%5D+%5C%5C+%26%3D-I%5Cleft%28x_%7Bt%2Bk%7D%2C+c_%7Bt%7D%5Cright%29%2B%5Clog+%28N%29+%5Cend%7Baligned%7D+%5Ctag+%7B30%7D" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>https://zhuanlan.zhihu.com/p/334772391</p>
<p>https://zhuanlan.zhihu.com/p/413681189</p>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/16043538-82d966ccaff78508.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/509/format/webp" alt="" /><figcaption>img</figcaption>
</figure>
<h6 id="模型">模型</h6>
<figure>
<img src="https://img-blog.csdnimg.cn/2021051822085535.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzM1OTMxMg==,size_16,color_FFFFFF,t_70" alt="" /><figcaption>在这里插入图片描述</figcaption>
</figure>
<ol type="1">
<li><p><strong>Modeling Item-Attribute Correlation</strong>：最大化物品和属性间的互信息。<img src="https://upload-images.jianshu.io/upload_images/16043538-a4208c396f7c559d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/421/format/webp" alt="img" /></p>
<p>对于每个物品，属性都提供其细粒度信息。 因此通过对物品-属性相关性进行建模来融合物品和属性级别的信息。以这种方式，期望将有用的属性信息融入到物品表示中。</p>
<p>用双线性模型建模属性向量之间相关性：</p>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/16043538-0c20b082f2f2cf41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/289/format/webp" alt="" /><figcaption>img</figcaption>
</figure>
<p>NCEloss：</p>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/16043538-f6642ec271df6b4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/518/format/webp" alt="" /><figcaption>img</figcaption>
</figure></li>
<li><p><strong>Modeling Sequence-Item Correlation</strong></p>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/16043538-9bf488b559a47c1b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/336/format/webp" alt="" /><figcaption>img</figcaption>
</figure>
<p>和bert4rec， 一样在每个训练步骤中，随机掩盖输入序列中的一部分物品（即，将它们替换为特殊标记“ [mask]”）。 然后，基于两个方向上的上下文从原始序列中预测被mask的物品。</p>
<p>对应loss（F代表mask位置预测的embedding）：</p>
<figure>
<img src="https:////upload-images.jianshu.io/upload_images/16043538-fadd088808210086.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/449/format/webp" alt="" /><figcaption>img</figcaption>
</figure>
<figure>
<img src="https:////upload-images.jianshu.io/upload_images/16043538-cda3206968c6dfa1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/299/format/webp" alt="" /><figcaption>img</figcaption>
</figure></li>
<li><p><strong>Modeling Sequence-Attribute Correlation</strong></p>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/16043538-9bee464d531568be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/340/format/webp" alt="" /><figcaption>img</figcaption>
</figure>
<p>融合序列上下文和物品属性信息（现有方法很少这样关联的）：</p>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/16043538-61068321a60405e0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/310/format/webp" alt="" /><figcaption>img</figcaption>
</figure>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/16043538-5e8ba7e439ae0ab6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/454/format/webp" alt="" /><figcaption>img</figcaption>
</figure></li>
<li><p><strong>Modeling Sequence-Segment Correlation</strong></p>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/16043538-0fa45ae04a43c429.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/260/format/webp" alt="" /><figcaption>img</figcaption>
</figure>
<p>物品序列与单词序列之间的主要<strong>区别在于单个物品项目可能与周围环境没有高度关联</strong>。例如，用户仅仅因为某些产品就在购买中而购买了它们，严格的上下文信息并不是决定因素。因此作者还考虑了子序列（序列模式，不是考虑严格位置关系，考虑相对位置关系）。</p>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/16043538-1244892c30654e84.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/313/format/webp" alt="" /><figcaption>img</figcaption>
</figure>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/16043538-99312a5a87bb1042.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/432/format/webp" alt="" /><figcaption>img</figcaption>
</figure></li>
</ol>
<h6 id="训练">训练</h6>
<p><strong>模型训练包括预训练和微调两个阶段，预训练过程中利用上面的4个Loss即4个子任务通过无掩码的自注意力机制进行训练，得到高质量的物品表征和属性表征</strong>。</p>
<p>微调阶段再来使用pairwise loss训练：</p>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/16043538-51b2cd90e20e34a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/475/format/webp" alt="" /><figcaption>img</figcaption>
</figure>
<h2 id="boosting">2 BOOSTING</h2>
<h4 id="gb">GB</h4>
<p>梯度提升（Gradient Boost）核心思想是，用多个弱分类器（比如生成的子树）来构建一个强分类器（最终集成出来的树）。每一棵树（以回归树为例）学习的是之前所有树结论和的<strong>残差，</strong>这个残差就是一个加预测值后能得真实值的累加量。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=f%28%5Cvec%7B%5Cmathbf+%7B+x+%7D%7D%29+%3D+f%28%5Cvec%7B%5Cmathbf+%7B+x+%7D%7D_%7BK%7D%29+%3D+%5Csum_%7Bk+%3D+1%7D%5E%7BK%7D%7Bh_%7Bk%7D+%28%5Cvec%7B%5Cmathbf+%7B+x+%7D%7D%3B%5CTheta_%7Bk%7D%29+%7D" alt="" /><figcaption>[公式]</figcaption>
</figure>
<h4 id="gbt">GBT</h4>
<p>梯度提升树(GBT)的一个核心思想是<strong>利用损失函数的负梯度在当前模型的值作为残差的近似值</strong>，本质上是对损失函数进行一阶泰勒展开，从而拟合一个回归树。</p>
<p>一阶泰勒：<img src="https://www.zhihu.com/equation?tex=f%28x+%2B+%5CDelta+x%29+%5Csimeq+f%28x%29+%2B+f%27%28x%29+%5CDelta+x" alt="[公式]" /></p>
<p>对应损失函数展开：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=L+%5Cleft%28+%7B+y+%7D+%2C+f+_+%7B+k+%7D+%28+%5Cvec+%7B+%5Cmathbf+%7B+x+%7D+%7D+%29+%5Cright%29+%3D+L+%5Cleft%28+%7B+y+%7D+%2C+f+_+%7B+k+-+1+%7D+%28+%5Cvec+%7B+%5Cmathbf+%7B+x+%7D+%7D+%29+%2B+h+_+%7B+k+%7D+%5Cleft%28+%5Cvec+%7B+%5Cmathbf+%7B+x+%7D+%7D+%3B+%5CTheta+_+%7B+k+%7D+%5Cright%29+%5Cright%29+%3D+L+%5Cleft%28+%7B+y+%7D+%2C+f+_+%7B+k+-+1+%7D+%28+%5Cvec+%7B+%5Cmathbf+%7B+x+%7D+%7D+%29+%5Cright%29+%2B+%5Cfrac+%7B+%5Cpartial+L+%5Cleft%28+%7B+y+%7D+%2C+f+_+%7B+k+-+1+%7D+%28+%5Cvec+%7B+%5Cmathbf+%7B+x+%7D+%7D+%29+%5Cright%29+%7D+%7B+%5Cpartial+f+_+%7B+k+-+1+%7D+%28+%5Cvec+%7B+%5Cmathbf+%7B+x+%7D+%7D+%29+%7D+h+_+%7B+k+%7D+%5Cleft%28+%5Cvec+%7B+%5Cmathbf+%7B+x+%7D+%7D+%3B+%5CTheta+_+%7B+k+%7D+%5Cright%29++%5C%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5CDelta+L+%3D+L+%5Cleft%28+%7B+y+%7D+%2C+f+_+%7B+k+%7D+%28+%5Cvec+%7B+%5Cmathbf+%7B+x+%7D+%7D+%29+%5Cright%29+-+L+%5Cleft%28+%7B+y+%7D+%2C+f+_+%7B+k+-+1+%7D+%28+%5Cvec+%7B+%5Cmathbf+%7B+x+%7D+%7D+%29+%5Cright%29+%3D+%5Cfrac+%7B+%5Cpartial+L+%5Cleft%28+%7B+y+%7D+%2C+f+_+%7B+k+-+1+%7D+%28+%5Cvec+%7B+%5Cmathbf+%7B+x+%7D+%7D+%29+%5Cright%29+%7D+%7B+%5Cpartial+f+_+%7B+k+-+1+%7D+%28+%5Cvec+%7B+%5Cmathbf+%7B+x+%7D+%7D+%29+%7D+h+_+%7B+k+%7D+%5Cleft%28+%5Cvec+%7B+%5Cmathbf+%7B+x+%7D+%7D+%3B+%5CTheta+_+%7B+k+%7D+%5Cright%29+%5C%5C" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>其中对于平方或指数损失函数，就是通常意义上的残差。对于其他普通函数，残差是导数的近似值。用于回归模型时，是<strong>梯度提升回归树GBRT；</strong>梯度提升树用于分类模型时，是<strong>梯度提升决策树<code>GBDT</code></strong>；二者的区别主要是损失函数不同。</p>
<h4 id="xgboost">XGBoost</h4>
<h4 id="算法">算法</h4>
<h5 id="目标函数">目标函数：</h5>
<figure>
<img src="https://www.zhihu.com/equation?tex=L%5Cleft%28+%5Cphi+%5Cright%29%3D%5Csum_%7Bi%7D%7Bl%5Cleft%28+%5Chat+y_%7Bi%7D+%2Cy_%7Bi%7D%5Cright%29%7D%2B%5Csum_%7Bk%7D%7B%5COmega%5Cleft%28+f_%7Bk%7D+%5Cright%29%7D" alt="" /><figcaption>[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5COmega%5Cleft%28+f+%5Cright%29%3D%5CUpsilon+T%2B%5Cfrac%7B1%7D%7B2%7D%5Clambda+%7C%7Cw%7C%7C%5E2" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>相比GBDT引入了正则项， <img src="https://www.zhihu.com/equation?tex=T" alt="[公式]" /> 表示给定一颗树的叶节点数,决策树定义为 <img src="https://www.zhihu.com/equation?tex=f_k%28x%29%3Dw_%7Bq%28x%29%7D" alt="[公式]" />，<img src="https://www.zhihu.com/equation?tex=x" alt="[公式]" /> 为某一样本，这里的 <img src="https://www.zhihu.com/equation?tex=q%28x%29" alt="[公式]" /> 表示该样本所在的叶子结点，<img src="https://www.zhihu.com/equation?tex=w_q" alt="[公式]" /> 为叶子结点权重 <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]" />，所以得出 <img src="https://www.zhihu.com/equation?tex=w_%7Bq%28x%29%7D" alt="[公式]" /> 为每个样本的取值 <img src="https://www.zhihu.com/equation?tex=w" alt="[公式]" />（即预测值）。所以 <img src="https://www.zhihu.com/equation?tex=%7C%7Cw%7C%7C%5E2" alt="[公式]" /> 表示每颗树叶节点上的输出分数的平方(相当于L2正则)</p>
<p>在通常的模型中针对这类目标函数可以使用梯度下降的方式进行优化，但注意到 <img src="https://www.zhihu.com/equation?tex=f_%7Bt%7D" alt="[公式]" /> 表示的是一颗树，而非一个数值型的向量，所以不能使用梯度下降的方式去优化该目标函数。在此作者提出了使用 <strong>前向分步算法</strong>（additive manner）。</p>
<p>目标函数为： <img src="https://www.zhihu.com/equation?tex=L%5E%7B%5Cleft%28+t+%5Cright%29%7D%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Bl%5Cleft%28y_%7Bi%7D+%2C%5Chat+y_%7Bi%7D%5E%7B%5Cleft%28+t-1+%5Cright%29%7D%2Bf_%7Bt%7D%28x_%7Bi%7D%29%5Cright%29%7D%2B%5COmega%5Cleft%28+f_%7Bt%7D+%5Cright%29" alt="[公式]" /></p>
<p>其中第 i 个样本在第 t 颗树的预测值 (<img src="https://www.zhihu.com/equation?tex=%5Chat+y_%7Bi%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D" alt="[公式]" />)等于样本 i 在前 t-1 棵树的预测值( <img src="https://www.zhihu.com/equation?tex=%5Chat+y_%7Bi%7D%5E%7B%5Cleft%28+t-1+%5Cright%29%7D" alt="[公式]" /> ) 加当前第 t 颗树预测值( <img src="https://www.zhihu.com/equation?tex=f_%7Bt%7D%28x_%7Bi%7D%29" alt="[公式]" /> ) 公式表达为： <img src="https://www.zhihu.com/equation?tex=%5Chat+y_%7Bi%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D+%3D+%5Chat+y_%7Bi%7D%5E%7B%5Cleft%28+t-1+%5Cright%29%7D%2Bf_%7Bt%7D%28x_%7Bi%7D%29" alt="[公式]" /></p>
<p>对于该函数的优化，在 XGBoost 中使用了泰勒展开式，与 GDBT 不同的是 XGBoost 使用了泰勒二次展开式。</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=f%28x+%2B+%5CDelta+x%29+%5Csimeq+f%28x%29+%2B+f%27%28x%29+%5CDelta+x+%2B+%5Cfrac%7B1%7D%7B2%7D+f%27%27%28x%29+%5CDelta+x%5E%7B2%7D" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>使 <img src="https://www.zhihu.com/equation?tex=g_%7Bi%7D%3D%5Cpartial_%7B%5Chat+y%5E%7B%28t-1%29%7D%7Dl%28y_%7Bi%7D%2C%5Chat+y%5E%7B%28t-1%29%7D%29%2C+%5Cspace+h_%7Bi%7D%3D%5Cpartial_%7B%5Chat+y_%7Bi%7D%5E%7B%28t-1%29%7D%7D%5E%7B2%7Dl%28y_%7Bi%7D%2C%5Chat+y%5E%7B%28t-1%29%7D%29" alt="[公式]" /></p>
<p>这里是针对 <img src="https://www.zhihu.com/equation?tex=f_%7Bt%7D%28x_%7Bi%7D%29" alt="[公式]" /> 的求导，所以可以将 <img src="https://www.zhihu.com/equation?tex=l%28y_%7Bi%7D%2C%5Chat+y_%7Bi%7D%5E%7B%28t-1%29%7D%29" alt="[公式]" /> 部分看成常数去掉。</p>
<p>得出简化后的损失函数：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=L%5E%7B%28t%29%7D%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%5Bg_%7Bi%7Df_%7Bt%7D%28x_%7Bi%7D%29%2B%5Cfrac%7B1%7D%7B2%7Dh_%7Bi%7Df_%7Bt%7D%5E%7B2%7D%28x_%7Bi%7D%29%5D%7D%2B%5COmega%28f_%7Bt%7D%29+%5C%5C%3D+%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%5Bg_%7Bi%7Df_%7Bt%7D%28x_%7Bi%7D%29%2B%5Cfrac%7B1%7D%7B2%7Dh_%7Bi%7Df_%7Bt%7D%5E%7B2%7D%28x_%7Bi%7D%29%5D%7D%2B%5CUpsilon+T%2B%5Cfrac%7B1%7D%7B2%7D%5Clambda+%5Csum_%7Bj%3D1%7D%5E%7BT%7D%7Bw_%7Bj%7D%5E2%7D+%5C%5C%3D%5Csum_%7Bj%3D1%7D%5E%7BT%7D%5B%28%5Csum_%7Bi%5Cin+I_%7Bj%7D%7D+++%7Bg_%7Bi%7D%29w_%7Bj%7D%2B%5Cfrac%7B1%7D%7B2%7D%28%5Csum_%7Bi%5Cin+I_%7Bj%7D%7D%7Bh_%7Bi%7D%2B%5Clambda%7D%29w_%7Bj%7D%5E%7B2%7D%7D%5D%2B%5Cgamma+T++++" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>如果确定了树的结构，为了使目标函数最小，可以令其导数为0，解得每个叶节点<span class="math inline">\(j\)</span>的最优预测分数为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%E4%BB%A4+%5Cfrac%7B%5Cpartial+L%5E%7B%28t%29%7D%7D%7B%5Cpartial+w_%7Bj%7D%7D%3D0+%5C%5C%5CRightarrow+%28%5Csum_%7Bi%5Cin+I_%7Bj%7D%7D+++%7Bg_%7Bi%7D%29%2B%28%5Csum_%7Bi%5Cin+I_%7Bj%7D%7D%7Bh_%7Bi%7D%2B%5Clambda%7D%29w_%7Bj%7D%7D%3D0%5C%5C%5CRightarrow+%28%5Csum_%7Bi%5Cin+I_%7Bj%7D%7D%7Bh_%7Bi%7D%2B%5Clambda%7D%29w_%7Bj%7D%3D-%5Csum_%7Bi%5Cin+I_%7Bj%7D%7D%7Bg_%7Bi%7D%7D%5C%5C%5CRightarrow+w_%7Bj%7D%5E%7B%2A%7D%3D-%5Cfrac%7B%5Csum_%7Bi%5Cin+I_%7Bj%7D%7D%7Bg_%7Bi%7D%7D%7D%7B%5Csum_%7Bi%5Cin+I_%7Bj%7D%7D%7Bh_%7Bi%7D%2B%5Clambda%7D%7D+" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>将 <img src="https://www.zhihu.com/equation?tex=w%5E%7B%2A%7D" alt="[公式]" /> 代入简化后的目标函数得到最小损失为：<img src="https://www.zhihu.com/equation?tex=L%5E%7B%28t%29%7D%3D-%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bj%3D1%7D%5E%7BT%7D%7B%5Cfrac%7B%28%5Csum_%7Bi%5Cin+I_%7Bj%7D%7D%7Bg_%7Bi%7D%29%5E2%7D%7D%7B%5Csum_%7Bi%5Cin+I_%7Bj%7D%7D%7Bh_%7Bi%7D%2B%5Clambda%7D%7D%7D%2B%5CUpsilon+T" alt="[公式]" /></p>
<p>另<img src="https://www.zhihu.com/equation?tex=G_%7Bi%7D%3D%5Csum_%7Bi%5Cin+I_%7Bj%7D%7D%7Bg_%7Bi%7D%7D%2CH_%7Bi%7D%3D%5Csum_%7Bi%5Cin+I_%7Bj%7D%7D%7Bh_%7Bi%7D%7D" alt="[公式]" /> 则有：<img src="https://www.zhihu.com/equation?tex=L%5E%7B%28t%29%7D%3D-%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bj%3D1%7D%5E%7BT%7D%7B%5Cfrac%7BG_%7Bj%7D%5E%7B2%7D%7D%7BH_%7Bj%7D%2B%5Clambda%7D%7D%2B%5CUpsilon+T" alt="[公式]" /></p>
<p>直观上看，判断一棵树的好坏，就可以根据上面的<span class="math inline">\(L^{(t)}\)</span>进行判断，loss越小（<strong>注意loss中的负号</strong>），代表树的结构越好。</p>
<p>作者在划分时用了<strong>贪心算法</strong>。假设在某节点分裂了左 <img src="https://www.zhihu.com/equation?tex=I_%7BL%7D" alt="[公式]" /> ，右节点 <img src="https://www.zhihu.com/equation?tex=I_%7BR%7D" alt="[公式]" /> 且满足 如果确定了树的结构（即q(x)确定），为了使目标函数最小，可以令其导数为0，解得每个叶节点的最有预测分数为：<img src="https://www.zhihu.com/equation?tex=I+%3D+I_%7BL%7D+%5Ccup+I_%7BR%7D" alt="[公式]" /> ,</p>
<p>那么分裂后增益 可以表示为： <img src="https://www.zhihu.com/equation?tex=L_%7Bsplit%7D%3D%5Cfrac%7B1%7D%7B2%7D%5B%5Cfrac%7BG_%7BL%7D%5E%7B2%7D%7D%7BH_%7BL%7D%2B%5Clambda%7D%2B%5Cfrac%7BG_%7BR%7D%5E%7B2%7D%7D%7BH_%7BR%7D%2B%5Clambda%7D-%5Cfrac%7B%28G_%7BL%7D%2BG_%7BR%7D%29%5E2%7D%7BH_%7BL%7D%2BH_%7BR%7D%2B%5Clambda%7D%5D-%5Cgamma" alt="[公式]" /></p>
<p>该值越大，说明分裂后能使目标函数减少越多，就越好。此方法被用于 XGBoost 判断最优特征以及最优切分点。</p>
<h5 id="shrinkage-列降采样"><strong>Shrinkage</strong> <strong>&amp; 列降采样</strong></h5>
<ul>
<li><strong>Shrinkage</strong> 策略（权重衰减，类似 LSTM）。比如把整个训练过程看作一个时间序列，离当前树时间点越靠前的权重对当前权重的累加影响越小，这个衰减就是论文里的参数 <img src="https://www.zhihu.com/equation?tex=%5Ceta" alt="[公式]" /> 控制的。（时间做平滑)</li>
<li>特征降采样的方法来避免过拟合，只是这里的降采样使用的是<strong>列降采样</strong>（与随机森林做法一样每次的输入特征不是全部特征，不仅能降低过拟合，还能减少计算），它的另外一个好处是可以方便加速并行化</li>
</ul>
<h5 id="分裂点">分裂点</h5>
<p>每个树生成的问题都要考虑到两个问题是：1）按照哪个维度 / 顺序组合切 2）如何判断*<strong>最佳切分点*</strong>。</p>
<ol type="1">
<li><p><strong>贪心算法</strong></p>
<p>从树的根节点开始，对每个叶节点枚举所有的可用特征。此处文中指出：对该节下的数据需令其按特征值进行排序，这样做可以使计算分裂收益值更加高效，*<strong>分裂收益值*</strong> 计算如下：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=Gain+%3D+L_%7Bsplit%7D%3D%5Cfrac%7B1%7D%7B2%7D%5B%5Cfrac%7BG_%7BL%7D%5E%7B2%7D%7D%7BH_%7BL%7D%2B%5Clambda%7D%2B%5Cfrac%7BG_%7BR%7D%5E%7B2%7D%7D%7BH_%7BR%7D%2B%5Clambda%7D-%5Cfrac%7B%28G_%7BL%7D%2BG_%7BR%7D%29%5E2%7D%7BH_%7BL%7D%2BH_%7BR%7D%2B%5Clambda%7D%5D-%5Cgamma" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>然后选择该轮最大收益值作为*<strong>最佳分裂点*</strong> ，使在该节点行分裂出左右两个新的叶节点，并重新分配新节点上的样本。至此一个循环结束，继续按同样流程递归迭代直至满足条件停止。在特征维度较大的时候，内存会不够。</p></li>
<li><p><strong>近似值</strong></p>
<p>作者提出将连续特征变量按照特征值分布进行分桶操作的方法，这样只需要计算每个桶中的统计信息就可以求出最佳分裂点的最佳分裂收益值。</p>
<p>在确定分裂点时作者提出了两种方法：Global、Local。</p>
<p><strong><em>Global</em></strong> 表示在生成树<strong>之前</strong>进行候选切分点（candidate splits）的计算，且在整个计算过程中只做<strong>一次</strong>操作。在以后的节点划分时都使用已经提前计算好的候选切分点；<strong>Local</strong> 则是在每次节点划分时才进行候选切分点的计算。Global 适合在取大量切分点下使用； Local 更适用于深度较大的树结构。</p></li>
</ol>
<h4 id="lightgbm">LightGBM</h4>
<p>https://zhuanlan.zhihu.com/p/99069186</p>
<p>对xgboost的优化：</p>
<ul>
<li><p>基于Histogram的决策树算法。</p>
<p>直方图方式进行分桶， 将特征离散化（与xgboost一样仅考虑非零特征）。</p>
<figure>
<img src="https://img-blog.csdn.net/2018062717561447?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2phc29ud2FuZ18=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" /><figcaption>CSDN图标</figcaption>
</figure>
<p>Histogram（直方图）做差加速。一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍。</p></li>
<li><p>单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销。</p>
<p>GOSS排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。</p></li>
<li><p>互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。</p></li>
<li><p>带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法：</p>
<p>该策略每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度；Leaf-wise的缺点是：可能会长出比较深的决策树，产生过拟合。因此LightGBM会在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p></li>
<li><p>直接支持类别特征(Categorical Feature)</p></li>
<li><p>支持高效并行</p></li>
<li><p>Cache命中率优化</p></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dsttsd.github.io/2022/02/11/week%2010/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/50662067?s=400&u=b552d8b742d1e685ed0ddcc6a97d9f697535fa6b&v=4">
      <meta itemprop="name" content="DST">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DSTの杂货铺">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/11/week%2010/" class="post-title-link" itemprop="url">第十次周报</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-11 12:20:55" itemprop="dateCreated datePublished" datetime="2022-02-11T12:20:55+08:00">2022-02-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-06-28 14:05:15" itemprop="dateModified" datetime="2022-06-28T14:05:15+08:00">2022-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%91%A8%E6%8A%A5/" itemprop="url" rel="index"><span itemprop="name">周报</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>week 10 内容：</p>
<ul>
<li><p>louis philippe morency 《多模态机器学习》 90%</p></li>
<li><p>kaggle fashion recommendation competition:</p>
<ol type="1">
<li>完成EDA、数据预处理</li>
<li>用流行度、item相关性、最近购买的item三种方法，进行了提交（baseline）</li>
</ol></li>
<li><p>论文阅读：</p>
<ol type="1">
<li>BGSP、BKM聚类</li>
<li>fashion recommendation相关</li>
<li>boosting相关（待完善）</li>
</ol></li>
</ul>
<h2 id="co-clustering">1 Co-clustering</h2>
<h3 id="bipartite-spectral-graph-partitionbgsp-spectralcoclustering2001">Bipartite spectral graph partition（BGSP）-SpectralCoclustering（2001）</h3>
<p>https://scikit-learn.org/stable/modules/biclustering.html#spectral-biclustering</p>
<p>共同聚类，也称为双聚类，该方法在数据矩阵中同时聚类样本集和特征集。由于同时利用了样本类群和特征类群之间的关系，该方法相比传统聚类性能更优异。受互联网技术发展的影响，共聚类已成为近年来数据挖掘领域的一个研究热点。从分类角度，共聚类有两种类型：</p>
<ol type="1">
<li><p>棋盘格形式的共聚类：如下图，聚类完成后矩阵呈现棋盘格形式，该方法假设数据矩阵中每一个元素都属于一个共聚类簇。</p>
<p><img src="https://sklearn.apachecn.org/docs/master/img/4e0d8935ff82f26fc3a46a3202bd1fa3.jpg" /></p></li>
<li><p>对角共聚类：如下图，聚类完成后类簇位于矩阵对角线上，该方法仅考虑数据中的部分数据形成共聚类簇。</p></li>
</ol>
<p><img src="https://sklearn.apachecn.org/docs/master/img/9812effbd6ddac1053fd0b63ebe8c2fb.jpg" /></p>
<p>对于很多有着稀疏矩阵的应用来说，对角共聚类往往是更好的选择。一方面，对于棋盘格式的共聚类而言，稀疏矩阵中大部分的条目是为0的，对于共聚类没有贡献。另一方面，在实际的应用中，数据矩阵往往掺杂着许多噪声，这些噪声极大第降低了共聚性能。</p>
<p>在基于对角的共聚类方法中，最著名的是二部谱图划分(BSGP)方法(较快)。然而，由于BSGP在求解过程中涉及奇异值分解，因此在计算上无法用于大型数据集，这严重限制了BSGP在实际应用中的范围。此外，BSGP基于谱图的双分区Ncuts。在处理多分区问题时，首先需要将原离散问题放宽为一个连续问题，然后使用k-means算法输出离散结果。在这种离散-连续-离散变换过程中，最终得到的优化问题与BSGP的原始目标问题有很大的偏差，会损害BSGP的性能。</p>
<p>BSGP(Bipartite spectrum graph partitioning)共聚类，首先构建二部图<span class="math inline">\(G=\{V, A\}\)</span>,V是点集， 设<span class="math inline">\(X\in{R}^{d \times n}\)</span></p>
<p><span class="math inline">\(\boldsymbol{A}=\left[\begin{array}{cc} \boldsymbol{0} &amp; \boldsymbol{X} \\ \boldsymbol{X}^{T} &amp; \boldsymbol{0} \end{array}\right]\)</span></p>
<p>feature cluster: <span class="math inline">\(\boldsymbol{P}=\left[\boldsymbol{p}_{1 \cdot}^{T}, \cdots, \boldsymbol{p}_{d \cdot}^{T}\right]^{T} \in\{0,1\}^{d \times c}\)</span></p>
<p>sample cluster: <span class="math inline">\(\boldsymbol{Q}=\left[\boldsymbol{q}_{1 \cdot}^{T}, \cdots, \boldsymbol{q}_{d \cdot}^{T}\right]^{T} \in\{0,1\}^{n \times c}\)</span></p>
<p>对聚类完成后的cluser定义（保证cluster中的元素比其他划分方法更好）：</p>
<p>feature cluster：</p>
<p><span class="math inline">\(\Omega_{l}=\{x_{i\cdot}:\sum_{j\in\Theta_{l}}X_{i j}\geq\sum_{i\in\Theta_{k}}X_{i j},\forall k=1,\cdot\cdot\cdot,c\}\)</span></p>
<p>sample cluster：</p>
<p><span class="math inline">\(\Theta_{l}=\{x_j:\sum_{i\in\Omega,}X_{i j}\geq\sum_{i\in\Omega_{i}}X_{i j},\forall k=1,\cdot\cdot\cdot,c\}\)</span></p>
<p>很容易看出<span class="math inline">\(\Omega_l\)</span>与<span class="math inline">\(\theta_l\)</span>之间存在递归关系，同时这也决定了BSGP基与diagonal co-cluster。</p>
<p><strong>二划分的优化：</strong></p>
<p>BGSP为了找到G最小的ncuts，优化如下的问题：</p>
<p><span class="math inline">\(\min _{\boldsymbol{y}} \frac{\boldsymbol{y}^{T} \boldsymbol{L} \boldsymbol{y}}{\boldsymbol{y}^{T} \boldsymbol{D} \boldsymbol{y}}, \text { s.t. } \boldsymbol{y} \in\{-1,1\}^{(d+n) \times 1}\)</span></p>
<p>其中，<span class="math inline">\(D\)</span>是度矩阵<span class="math inline">\(D_{ii} = \textstyle\sum_{k}A_{i k}\)</span></p>
<p>拉普拉斯矩阵： <span class="math inline">\(L = D - A\)</span></p>
<p><strong>partition :</strong></p>
<p><span class="math inline">\(y=[p^{T},q^{T}]^{T},p\in\{1,-1\}^{d\times1},q\in\{1,-1\}^{n\times1}\)</span> 是indicator vector</p>
<p>由于上述优化目标是NP-complete problem,进行松弛：</p>
<p><span class="math inline">\(\operatorname*{min}_{q\neq0}{\frac{q^{T}L q}{q^{T}D q}},s.t.q^{T}D e=0\)</span></p>
<p>其中<span class="math inline">\(e\)</span>是<span class="math inline">\((d + n) \times 1\)</span>全1的向量,考虑到数据矩阵A的结构，可使用SVD计算<span class="math inline">\(D_1^{-1/2}XD_2^{-1/2}\)</span>的第二小奇异值对应的向量来求解。<span class="math inline">\(D1, D2\)</span>满足：</p>
<p><span class="math inline">\(\boldsymbol{D}=\left[\begin{array}{ll} \boldsymbol{D}_{1} &amp; \\ &amp; \boldsymbol{D}_{2} \end{array}\right]\)</span></p>
<p><strong>多个类群的扩展：</strong></p>
<p>对于含有c个类群的情况，需要计算<span class="math inline">\(l = log_2(c)\)</span>个left vectors U和right vectors V</p>
<p>构成一个L维度的数据集：</p>
<p><span class="math inline">\(\boldsymbol{Z}=\left[\begin{array}{l} \boldsymbol{D}_{1}^{-1 / 2} \boldsymbol{U} \\ \boldsymbol{D}_{2}^{-1 / 2} \boldsymbol{V} \end{array}\right]\)</span></p>
<p>最后，需要用k-means算法对新的数据集Z进行聚类，其中Z的前d个结果对应X的特征聚类结果，后n个结果对应样本聚类结果。</p>
<p>BGSP中的问题：</p>
<ol type="1">
<li>需要一个后处理步骤来输出聚类结果</li>
<li>涉及奇异值分解(SVD)，其复杂度大于<span class="math inline">\(O(n^2d)\)</span>。</li>
</ol>
<h4 id="bilateral-k-means-algorithm-for-fast-co-clusteringijcai-2017">Bilateral k-Means Algorithm for Fast Co-Clustering（IJCAI 2017）</h4>
<h5 id="优化目标">优化目标</h5>
<p>BGSP中多类群的优化目标为：</p>
<p><span class="math inline">\(\operatorname*{min}_{Y}\sum_{k=1}^{c}\frac{y_{k}^{T}L y_{k}}{y_{k}^{T}D y_{k}},s.t.{\cal Y}\in\Phi_{(d+n)\times c}\)</span></p>
<p>由于<span class="math inline">\(y_k^{T}Dy_k = Y^TDY\)</span>，因此优化目标转化为：</p>
<p><span class="math inline">\(\operatorname*{min}_{Y}Tr(Y^{I}LY\{Y^{I}D Y)^{-1}),s.t.Y\in\Phi_{(m+n)\chi c}\)</span></p>
<p>由于<span class="math inline">\(Y^{T}=[P^{T},Q^{T}]\)</span>，因此可以把A拆为X，即<span class="math inline">\(T r(I-Y^{T}A Y(Y^{T}D Y)^{-1}) = T r(I-2P^{T}X Q(Y^{T}D Y)^{-1})\)</span></p>
<p>优化目标转变为：</p>
<p><span class="math inline">\(\operatorname*{min}_{P.Q}T r(-P^{T}X Q(Y^{T}D Y)^{-1})\)</span></p>
<p><span class="math inline">\(s.t.P\in\Phi_{d\times c},Q\in\Phi_{n\times c}\)</span></p>
<p>为了得到二范数，添加<span class="math inline">\(T r((Y^{T}D Y)^{-1}P^{T}P(Y^{T}D Y)^{-1}Q^{I}Q)\)</span>以及<span class="math inline">\(T r(X^{T}X)\)</span>两项，得到优化目标：</p>
<p><span class="math inline">\(\operatorname*{min}_{P.Q}\|X-P(Y^{T}D Y)^{-1}Q^{T}\|_{F}^{2}\)</span></p>
<p><span class="math inline">\(s.t.P\in\Phi_{d\times c},Q\in\Phi_{n\times c}\)</span></p>
<p><span class="math inline">\((Y^{T}D Y)^{-1}\)</span>是对角阵，求逆矩阵的开销很大，为了简化，用对角阵S代替，S也是待优化的参数：</p>
<p><span class="math inline">\(\operatorname*{min}_{P,O_{S}}\|X-P S Q^{T}\|_{F}^{2}\)</span></p>
<p><span class="math inline">\(s.t.P\in\Phi_{d\times c},Q\in\Phi_{n\times c},\;S\in d i a g\)</span></p>
<h5 id="优化方法">优化方法</h5>
<p>使用两个命题来简化目标：</p>
<p><img src="https://s2.loli.net/2022/02/21/J7Y4mU2xIDECZjf.png" /></p>
<p><span class="math inline">\(J_{1}=||X-P S Q^{T}||_{F}^{2} =T r(X^{T}X)-2T r(Q^{T}X^{T}P S) +T r(S P^{T}P S Q^{T}Q)\)</span></p>
<p>由于P和Q是indicator matrix， 所以<span class="math inline">\(P^{T}P\)</span>以及<span class="math inline">\(Q^{T}Q\)</span>是对角阵，</p>
<p>由第一个命题：</p>
<p><span class="math inline">\(T r(Q^{T}X^{T}P S)=f({\cal S})^{T}f(P^{T}X Q)\)</span></p>
<p>由第二个命题可知：</p>
<p><span class="math inline">\(T_{r}(S P^{T}P S Q^{T}Q)=T r(S P^{T}P Q^{T}Q S)=f(S)^{T}(P^{T}P Q^{T}Q)f(S)\)</span></p>
<p>用<span class="math inline">\(H\)</span>表示<span class="math inline">\(P^T PQ^T Q\)</span>, <span class="math inline">\(s\)</span>表示<span class="math inline">\(f(s)\)</span> <span class="math inline">\(r\)</span>表示<span class="math inline">\(f(P^T XQ)\)</span>,那么有：</p>
<p><span class="math inline">\(J_{1}=T r(X^{T}X)-2{\bf r}^{T}{\bf s}+{\bf s}^{T}{H}s\)</span></p>
<ol type="1">
<li>求解s:</li>
</ol>
<p><span class="math inline">\({\frac{\partial J_{1}}{\partial s}}=2(H s-r)=0\)</span></p>
<p><span class="math inline">\(s=H^{-1}r\)</span></p>
<p>由于H是对角阵，求逆很方便。</p>
<ol start="2" type="1">
<li><p>固定P、S,求Q：</p>
<p><span class="math inline">\(R = PS\)</span></p>
<p>对每个sample进行如下优化：</p>
<p><span class="math inline">\(\operatorname*{min}_{Q}\|x_{\cdot i}-R q_{i\cdot}^{T}\|_{F}^{2}\)</span></p>
<p>由于q是indicator，只会有一个1，那么就根据如下方式得出(<span class="math inline">\(r_k\)</span>是<span class="math inline">\(R\)</span>的第k列)：</p>
<figure>
<img src="http://www.latex2png.com/pngs/45eedfdea729adcb99813491c1f12488.png" alt="" /><figcaption>img</figcaption>
</figure></li>
<li><p>固定Q S，求P:</p>
<p><span class="math inline">\(L=SQ^T\)</span></p>
<p><span class="math inline">\(\operatorname*{min}_{p}||x_{j}.-p^{T}_{j}L||_{F}^{2}\)</span></p></li>
</ol>
<p>​ <img src="http://www.latex2png.com/pngs/e1c3ea973a3069699d7951b9bbe22027.png" alt="img" /></p>
<p>​ <span class="math inline">\(l_k\)</span>代表了L的第k行</p>
<p><img src="https://s2.loli.net/2022/02/21/JMNgB9svXOaiLCI.png" /></p>
<h2 id="fasion-recommendation">2 Fasion recommendation</h2>
<p>回顾《A Survey on Neural Recommendation: From Collaborative Filtering to Information-rich Recommendation》中，fashion recommendation是对Image information建模的代表任务，其目的在于增强可解释性。</p>
<h4 id="deepstyle-learning-user-preferences-for-visual-recommendationsigir-2017">DeepStyle: Learning User Preferences for Visual Recommendation（SIGIR 2017）</h4>
<p><img src="https://s2.loli.net/2022/02/25/kjMqRmi4fVXntbg.png" /></p>
<p>这篇文章将图片信息经过预训练好的cnn的到视觉embedding，聚类后发现相同类别的服装被聚类在一起（比如鞋子），而这与推荐任务不太符合，因为相似风格的商品往往会被同一个人同时购买（皮鞋配西装），但在视觉特征空间中却并不相似，这就为提升推荐效果带来了难度。因此作者提出一个商品（item）由风格（style）和类别（category）组成，即item = style + cate. 因此如上图，作者从商品的视觉特征向量中减除了该商品对应类别的隐含表达（类别的平均向量），进而得到了商品的风格特征向量.随后将向量输入到BPR框架中进行训练（<strong>对每个user采样正负商品样本对（正样本表示实际购买了的商品，负样本表示没有购买过的商品）</strong>），取得了很好的推荐效果。</p>
<h4 id="aesthetic-based-clothing-recommendationwww-2018">Aesthetic-based Clothing Recommendation（www 2018）</h4>
<p>传统的方法只考虑 CNN 抽取的图像特征；而本文考虑了图片中的美学特征对于推荐的影响；作者利用 BDN 从图片中学习美学特征，然后将其融合到 DCF 中，增强用户-产品，产品-时间矩阵，从而提高了推荐效果。</p>
<p><img src="https://s2.loli.net/2022/02/25/linKteMfqwFL2Y1.png" /></p>
<p><img src="https://s2.loli.net/2022/02/25/Pt839Ig6mC4ZUJr.png" /></p>
<p><img src="https://s2.loli.net/2022/02/27/6ndGrospmQ7uFWj.png" /></p>
<h2 id="boosting">3 Boosting</h2>
<h4 id="回顾cart">回顾CART:</h4>
<table>
<thead>
<tr class="header">
<th>算法</th>
<th>支持模型</th>
<th>树结构</th>
<th>特征选择</th>
<th>连续值处理</th>
<th>缺失值处理</th>
<th>剪枝</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>ID3</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益</td>
<td>不支持</td>
<td>不支持</td>
<td>不支持</td>
</tr>
<tr class="even">
<td>C4.5</td>
<td>分类</td>
<td>多叉树</td>
<td>信息增益比</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
<tr class="odd">
<td>CART</td>
<td>分类回归</td>
<td>二叉树</td>
<td>基尼系数 均方差</td>
<td>支持</td>
<td>支持</td>
<td>支持</td>
</tr>
</tbody>
</table>
<p>C4.5决策树用较为复杂的熵（信息增益比）来度量，使用了相对较为复杂的多叉树，只能处理分类不能处理回归。CART(Classification And Regression Tree)做了改进，可以处理分类，也可以处理回归。</p>
<h5 id="信息度量">信息度量</h5>
<p>ID3中使用了信息增益选择特征，增益大优先选择。C4.5中，采用信息增益比选择特征，减少因特征值多导致信息增益大的问题。CART分类树算法使用基尼系数来代替信息增益比，基尼系数代表了模型的不纯度，基尼系数越小，不纯度越低，特征越好。这和信息增益（比）相反。</p>
<p>　　假设K个类别，第k个类别的概率为<span class="math inline">\(p_k\)</span>，概率分布的基尼系数表达式：</p>
<figure>
<img src="https://img2018.cnblogs.com/blog/1235684/201903/1235684-20190320142851428-1194269689.png" alt="" /><figcaption>img</figcaption>
</figure>
<p>　　如果是二分类问题，第一个样本输出概率为p，概率分布的基尼系数表达式为：</p>
<figure>
<img src="https://img2018.cnblogs.com/blog/1235684/201903/1235684-20190320143135391-255494198.png" alt="" /><figcaption>img</figcaption>
</figure>
<p>　　对于样本D，个数为<span class="math inline">\(|D|\)</span>，假设K个类别，第k个类别的数量为<span class="math inline">\(|C_k|\)</span>，则样本D的基尼系数表达式：</p>
<figure>
<img src="https://img2018.cnblogs.com/blog/1235684/201903/1235684-20190320143545086-323258879.png" alt="" /><figcaption>img</figcaption>
</figure>
<p>　　对于样本D，个数为<span class="math inline">\(|D|\)</span>，根据特征A的某个值<span class="math inline">\(a\)</span>，把D分成<span class="math inline">\(|D1|\)</span>和<span class="math inline">\(|D2|\)</span>，则在特征A的条件下，样本D的基尼系数表达式为：</p>
<figure>
<img src="https://img2018.cnblogs.com/blog/1235684/201903/1235684-20190320144250953-1096584260.png" alt="" /><figcaption>img</figcaption>
</figure>
<p>　　比较基尼系数和熵模型的表达式，二次运算比对数简单很多。尤其是二分类问题，更加简单。</p>
<p>对于二类分类，基尼系数和熵之半的曲线如下：</p>
<figure>
<img src="https://img2018.cnblogs.com/blog/1235684/201903/1235684-20190320144912554-276613838.png" alt="" /><figcaption>img</figcaption>
</figure>
<p>　　基尼系数和熵之半的曲线非常接近，因此，基尼系数可以做为熵模型的一个近似替代。</p>
<p>　　CART分类树算法每次仅对某个特征的值进行二分，而不是多分，这样CART分类树算法建立起来的是二叉树，而不是多叉树。</p>
<h5 id="特征处理">特征处理</h5>
<p><strong>对于连续的特征：</strong></p>
<p>同C4.5一样将连续特征离散化，从小到大选相邻两样本值的平均数作为划分点，找基尼系数最小的划分。但与ID3、C4.5不同的是它后面还可以参与子节点的划分过程。</p>
<p><strong>对于离散的特征：</strong></p>
<p>ID3、C4.5，特征A被选取建立决策树节点，如果它有3个类别A1,A2,A3，我们会在决策树上建立一个三叉点，这样决策树是多叉树。CART采用的是不停的二分。会考虑把特征A分成{A1}和{A2,A3}、{A2}和{A1,A3}、{A3}和{A1,A2}三种情况，找到基尼系数最小的组合，比如{A2}和{A1,A3}，然后建立二叉树节点，一个节点是A2对应的样本，另一个节点是{A1,A3}对应的样本。由于这次没有把特征A的取值完全分开，后面还有机会对子节点继续选择特征A划分A1和A3。这和ID3、C4.5不同，在ID3或C4.5的一颗子树中，离散特征只会参与一次节点的建立。</p>
<h5 id="流程">流程</h5>
<ol type="1">
<li><p><strong>分类</strong></p>
<p>输入:训练集D，基尼系数的阈值，样本个数阈值。</p>
<p>输出:决策树T。</p>
<p>递归建立CART分类树:</p>
<p>　　(1)、对于当前节点的数据集为D，如果样本个数小于阈值或没有特征，则返回决策子树，当前节点停止递归。</p>
<p>　　(2)、计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归。</p>
<p>　　(3)、计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数，对于离散值和连续值的处理方法和基尼系数的计算见第二节。缺失值的处理方法和C4.5算法里描述的相同。</p>
<p>　　(4)、在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2。</p>
<p>　　(5)、对左右的子节点递归的调用1-4步，生成决策树。</p>
<p>　　对生成的决策树做预测的时候，假如测试集里的样本A落到了某个叶子节点，而节点里有多个训练样本。则对于A的类别预测采用的是这个叶子节点里概率最大的类别。</p></li>
<li><p><strong>回归</strong></p>
<p>与分类树不同：</p>
<p>不同。</p>
<p>　　(1)、分类树与回归树的区别在样本的输出，如果样本输出是离散值，这是分类树；样本输出是连续值，这是回归树。分类树的输出是样本的类别，回归树的输出是一个实数。</p>
<p>　　(2)、连续值的处理方法不同。</p>
<p>　　(3)、决策树建立后做预测的方式不同。</p>
<p>　　分类模型：采用基尼系数的大小度量特征各个划分点的优劣。</p>
<p>　　回归模型：采用和方差度量，度量目标是对于划分特征A，对应划分点s两边的数据集D1和D2，求出使D1和D2各自集合的均方差最小，同时D1和D2的均方差之和最小。</p>
<figure>
<img src="https://img2018.cnblogs.com/blog/1235684/201903/1235684-20190325203520134-2063244829.png" alt="" /><figcaption>img</figcaption>
</figure>
<p>表达式为：</p>
<figure>
<img src="https://img2018.cnblogs.com/blog/1235684/201903/1235684-20190321224806131-1788166894.png" alt="" /><figcaption>img</figcaption>
</figure>
<p>其中，c1为D1的样本输出均值，c2为D2的样本输出均值。</p>
<p>　　对于决策树建立后做预测的方式，CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。回归树输出不是类别，采用叶子节点的均值或者中位数来预测输出结果。</p></li>
</ol>
<h3 id="boosting-1">boosting</h3>
<p>bagging和boosting是集成学习的两种代表性范式，bagging的个体弱学习器的训练集是通过随机采样得到的，学习器之间没有强依赖关系。boosting相反，每次训练都期望纠正之前的错误，学习器之间存在强依赖关系。<strong>Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。</strong></p>
<h4 id="工作机制">工作机制</h4>
<p>boosting方法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。　　</p>
<figure>
<img src="https://images2015.cnblogs.com/blog/1042406/201612/1042406-20161204194331365-2142863547.png" alt="" /><figcaption>img</figcaption>
</figure>
<p>boosting家族算法一般需要解决：</p>
<p>​ 1）如何计算学习误差率e?</p>
<p>​ 2)如何得到弱学习器权重系数<span class="math inline">\(α\)</span>?</p>
<p>　3)如何更新样本权重D?</p>
<p>​ 4)使用何种结合策略？</p>
<h4 id="adaboost">Adaboost</h4>
<h5 id="基本思路">基本思路</h5>
<p>训练样本：</p>
<p><span class="math inline">\(T=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}\)</span></p>
<p>训练集的在第k个弱学习器的输出权重为(第一个权重1/m，后面逐渐调整)</p>
<p><span class="math inline">\(D(k) = (w_{k1}, w_{k2}, ...w_{km}) ;\;\; w_{1i}=\frac{1}{m};\;\; i =1,2...m\)</span></p>
<ol type="1">
<li><p>分类问题</p>
<p>多元分类是二元分类的推广，这里首先考虑二分类，输出为{-1,1}</p>
<p>1）第k个弱分类器<span class="math inline">\(G_k(x)\)</span>在训练集上的加权误差率为</p>
<p><span class="math inline">\(e_k = P(G_k(x_i) \neq y_i) = \sum\limits_{i=1}^{m}w_{ki}I(G_k(x_i) \neq y_i)\)</span></p>
<p>2）第k个弱分类器<span class="math inline">\(G_k(x)\)</span>权重系数为：</p>
<p><span class="math inline">\(\alpha_k = \frac{1}{2}log\frac{1-e_k}{e_k}\)</span></p>
<p>从上式可以看出，如果分类误差率<span class="math inline">\(ek\)</span>越大，则对应的弱分类器权重系数<span class="math inline">\(αk\)</span>越小。也就是说，误差率小的弱分类器权重系数越大。</p>
<p>3)样本权重D:假设第k个弱分类器的样本集权重系数为<span class="math inline">\(D(k) = (w_{k1}, w_{k2}, ...w_{km})\)</span>，则对应的第k+1个弱分类器的样本集权重系数为:</p>
<p><span class="math inline">\(w_{k+1,i} = \frac{w_{ki}}{Z_K}exp(-\alpha_ky_iG_k(x_i))\)</span></p>
<p>其中规范化因子<span class="math inline">\(Z_k = \sum\limits_{i=1}^{m}w_{ki}exp(-\alpha_ky_iG_k(x_i))\)</span></p>
<p>计算公式可以看出，如果第i个样本分类错误，则<span class="math inline">\(y_iG_k(x_i) &lt; 0\)</span>，导致样本的权重在第k+1个弱分类器中增大，如果分类正确，则权重在第k+1个弱分类器中减少.</p>
<p>4)集成策略：加权表决法</p>
<p><span class="math inline">\(f(x) = sign(\sum\limits_{k=1}^{K}\alpha_kG_k(x))\)</span></p>
<p>对于Adaboost多元分类算法，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数：</p>
<p><span class="math inline">\(\alpha_k = \frac{1}{2}log\frac{1-e_k}{e_k} + log(R-1)\)</span></p>
<p>其中R为类别数</p></li>
<li><p>回归问题（Adaboost R2）</p>
<p>数据权重: <span class="math inline">\(D(1) = (w_{11}, w_{12}, ...w_{1m}) ;\;\; w_{1i}=\frac{1}{m};\;\; i =1,2...m\)</span></p>
<p>1） 对于弱学习器：计算最大误差<span class="math inline">\(E_k= max|y_i - G_k(x_i)|\;i=1,2...m\)</span></p>
<p>2）计算每个样本的相对误差:</p>
<p>线性：<span class="math inline">\(e_{ki}= \frac{|y_i - G_k(x_i)|}{E_k}\)</span></p>
<p>平方：<span class="math inline">\(e_{ki}= \frac{(y_i - G_k(x_i))^2}{E_k^2}\)</span></p>
<p>指数：<span class="math inline">\(e_{ki}= 1 - exp（\frac{-|y_i -G_k(x_i)|}{E_k}）\)</span></p>
<p>3）回归误差率为：<span class="math inline">\(e_k = \sum\limits_{i=1}^{m}w_{ki}e_{ki}\)</span></p>
<p>4）弱学习器的系数：<span class="math inline">\(\alpha_k =\frac{e_k}{1-e_k}\)</span></p>
<p>5）新的数据权重：<span class="math inline">\(w_{k+1,i} = \frac{w_{ki}}{Z_k}\alpha_k^{1-e_{ki}}\)</span>,<span class="math inline">\(Z_k = \sum\limits_{i=1}^{m}w_{ki}\alpha_k^{1-e_{ki}}\)</span></p>
<p>6）最终的集成学习器：<span class="math inline">\(f(x) =G_{k^*}(x)\)</span>, <span class="math inline">\(G_{k^*}(x)=G_k(x)ln\frac{1}{\alpha_k} , k=1,2,....K\)</span></p></li>
</ol>
<h5 id="损失函数以及各权重推导">损失函数以及各权重推导</h5>
<p>从分类器角度，adaboost是多个分类器叠加：</p>
<p>第k-1轮的强学习器为：<span class="math inline">\(f_{k-1}(x) = \sum\limits_{i=1}^{k-1}\alpha_iG_{i}(x)\)</span></p>
<p>第k轮：<span class="math inline">\(f_{k}(x) = \sum\limits_{i=1}^{k}\alpha_iG_{i}(x)\)</span></p>
<p>因此可以得到迭代关系：<span class="math inline">\(f_{k}(x) = f_{k-1}(x) + \alpha_kG_k(x)\)</span></p>
<p>adaboost的损失函数为指数函数：</p>
<p><span class="math inline">\(\underbrace{arg\;min\;}_{\alpha, G} \sum\limits_{i=1}^{m}exp(-y_if_{k}(x))\)</span></p>
<p>将迭代关系代入：<span class="math inline">\(L(\alpha_k, G_k(x)) = \underbrace{arg\;min\;}_{\alpha, G}\sum\limits_{i=1}^{m}exp[(-y_i) (f_{k-1}(x) + \alpha G(x))]\)</span></p>
<p>令<span class="math inline">\(w_{ki}^{’} = exp(-y_if_{k-1}(x))\)</span>, 可知该值与<span class="math inline">\(\alpha 、G\)</span>无关，仅与<span class="math inline">\(f_{k-1}\)</span>有关：<span class="math inline">\(L(\alpha_k, G_k(x)) = \underbrace{arg\;min\;}_{\alpha, G}\sum\limits_{i=1}^{m}w_{ki}^{’}exp[-y_i\alpha G(x)]\)</span></p>
<ol type="1">
<li><p>求<span class="math inline">\(G_k(x)\)</span>：</p>
<p><span class="math inline">\(\begin{align} \sum\limits_{i=1}^mw_{ki}^{&#39;}exp(-y_i\alpha G(x_i)) &amp;= \sum\limits_{y_i =G_k(x_i)}w_{ki}^{&#39;}e^{-\alpha} + \sum\limits_{y_i \ne G_k(x_i)}w_{ki}^{&#39;}e^{\alpha} \\&amp; = (e^{\alpha} - e^{-\alpha})\sum\limits_{i=1}^mw_{ki}^{&#39;}I(y_i \ne G_k(x_i)) + e^{-\alpha}\sum\limits_{i=1}^mw_{ki}^{&#39;} \end{align}\)</span></p>
<p>得：<span class="math inline">\(G_k(x) = \underbrace{arg\;min\;}_{G}\sum\limits_{i=1}^{m}w_{ki}^{’}I(y_i \neq G(x_i))\)</span></p></li>
<li><p>求<span class="math inline">\(\alpha\)</span>：G代入损失函数<span class="math inline">\(L\)</span>，并对$<span class="math inline">\(求导，令其为零：\)</span>_k = log$</p>
<p><span class="math inline">\(e_k = \frac{\sum\limits_{i=1}^{m}w_{ki}^{’}I(y_i \neq G(x_i))}{\sum\limits_{i=1}^{m}w_{ki}^{’}} = \sum\limits_{i=1}^{m}w_{ki}I(y_i \neq G(x_i))\)</span></p></li>
<li><p>利用迭代关系以及<span class="math inline">\(w_{ki}^{’} = exp(-y_if_{k-1}(x))\)</span>得到w迭代公式：</p>
<p><span class="math inline">\(w_{k+1,i}^{’} = w_{ki}^{’}exp[-y_i\alpha_kG_k(x)]\)</span></p></li>
</ol>
<h5 id="正则化项">正则化项</h5>
<p>对学习器迭代添加正则项：<span class="math inline">\(f_{k}(x) = f_{k-1}(x) + \nu\alpha_kG_k(x)\)</span>，<span class="math inline">\(\nu\)</span>的取值范围为<span class="math inline">\(\nu \in [0,1]\)</span>。对于同样的训练集学习效果，较小的$$意味着我们需要更多的弱学习器的迭代次数。</p>
<h5 id="总结">总结</h5>
<p>最广泛的Adaboost弱学习器是决策树和神经网络。对于决策树，Adaboost分类用了CART分类树，而Adaboost回归用了CART回归树。</p>
<p>　　　　Adaboost的主要优点有：</p>
<p>　　　　1）Adaboost作为分类器时，分类精度很高</p>
<p>　　　　2）在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。</p>
<p>　　　　3）作为简单的二元分类器时，构造简单，结果可理解。</p>
<p>　　　　4）不容易发生过拟合</p>
<p>　　　　Adaboost的主要缺点有：</p>
<p>　　　　1）对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。</p>
<h3 id="gbdt">GBDT</h3>
<p>https://zhuanlan.zhihu.com/p/89572181</p>
<p>GBDT(Gradient Boosting Decision Tree) 又叫 MART（Multiple Additive Regression Tree)，是一种迭代的决策树算法，GBDT中的树是回归树（不是分类树），GBDT用来做回归预测，调整后也可以用于分类。</p>
<p>回顾：</p>
<ol type="1">
<li><p>回归树（用最小均方误差来进行划分）：<img src="https://upload-images.jianshu.io/upload_images/967544-b768a350d5383ccb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/789/format/webp" alt="img" /></p></li>
<li><p>提升树：迭代多颗树共同决策，每一棵树是之前的所有树的结论和残差。</p>
<p>需要解决： 1）如何计算学习误差率e?（残差）</p>
<p>​ 2)如何得到弱学习器权重系数<span class="math inline">\(α\)</span>?（这一棵树的权重）</p>
<p>　3)如何更新样本权重D?（下一棵树的学习）</p>
<p>​ 4)使用何种结合策略？（决策）</p></li>
</ol>
<p>GDBT关注了如何学习残差的问题，当损失函数时平方损失和指数损失函数时，每一步的优化很简单（如adaboost），但是对于更一般的损失函数，每一步的优化就不容易了。</p>
<p>针对这一问题，Freidman提出了梯度提升算法：利用最速下降的近似方法，即利用损失函数的负梯度在当前模型的值，作为回归问题中提升树算法的残差的近似值，拟合一个回归树：</p>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/967544-37a15b71dc6f6ca3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/657/format/webp" alt="" /><figcaption>img</figcaption>
</figure>
<ol type="1">
<li><p>初始化：f0仅仅是只有一个根节点的树，<span class="math inline">\(\gamma\)</span>是划分值，让loss最小</p></li>
<li><p>循环部分：</p>
<ol type="a">
<li><p>对每一个样本计算负梯度（样本权重）</p></li>
<li><p>利用（x, r）拟合cart回归树，对应的叶子节点区域是<span class="math inline">\(R_{jm}\)</span>，j是叶子结点个数</p></li>
</ol>
<p>c.对叶子节点计算最佳的拟合值（学习残差）</p>
<p>d.更新强学习器<span class="math inline">\(f_{m}(x)=f_{m-1}(x)+\sum_{j=1}^{J_{m}} \gamma_{j m} I\left(x \in R_{j m}\right)\)</span></p></li>
<li><p>汇总：<span class="math inline">\(f(x)=f_{0}(x)+\sum_{m=1}^{M}\sum_{j=1}^{J_{m}} \gamma_{j m} I\left(x \in R_{j m}\right)\)</span></p></li>
</ol>
<p>调参时的问题：</p>
<p>树的深度很少就能达到很高的精度（6，对于普通决策树和随机森林需要15+）。泛化误差可以分解为两部分，偏差（bias)和方差(variance)。boosting关注偏差，bagging关注方差。因此对boosting而言，弱学习器的拟合能力不能太强（方差小），不然会导致过拟合。而bagging采样不同的数据来进行弱分类器的训练，因此不容易过拟合。</p>
<h4 id="xgboostgdbt的更快实现1待补充">XGBoost——GDBT的更快实现1（待补充）</h4>
<p>https://www.cnblogs.com/mantch/p/11164221.html</p>
<p>https://zhuanlan.zhihu.com/p/86816771</p>
<h4 id="lightgbmgdbt的更快实现2待补充">LightGBM——GDBT的更快实现2（待补充）</h4>
<p>https://zhuanlan.zhihu.com/p/99069186</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dsttsd.github.io/2022/02/03/week%209/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/50662067?s=400&u=b552d8b742d1e685ed0ddcc6a97d9f697535fa6b&v=4">
      <meta itemprop="name" content="DST">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DSTの杂货铺">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/02/03/week%209/" class="post-title-link" itemprop="url">第九次周报</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-02-03 10:20:55" itemprop="dateCreated datePublished" datetime="2022-02-03T10:20:55+08:00">2022-02-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-06-28 14:12:30" itemprop="dateModified" datetime="2022-06-28T14:12:30+08:00">2022-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%91%A8%E6%8A%A5/" itemprop="url" rel="index"><span itemprop="name">周报</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>week 9 内容：</p>
<ul>
<li><p>louis philippe morency 《多模态机器学习》 80%</p></li>
<li><p>论文阅读：</p>
<ol type="1">
<li>A Survey of Clustering With Deep Learning: From the Perspective of Network Architecture</li>
<li>EM、GMM</li>
<li>SpectralCoclustering</li>
</ol></li>
</ul>
<h3 id="a-survey-of-clustering-with-deep-learning-from-the-perspective-of-network-architecture">A Survey of Clustering With Deep Learning: From the Perspective of Network Architecture</h3>
<p>近些年来很多研究几种在使用深度神经网络学习更好的表示来提高聚类性能。这篇综述从网络框架的角度进行了研究，他们首先介绍了领域相关知识，然后提出了深度学习进行聚类的相关分类方法（taxonomy），最后提出未来发展的方向并做了总结。</p>
<h4 id="intorduction">Intorduction</h4>
<p>数据聚类是机器学习、模式识别、计算机视觉、数据压缩等领域的一个基本问题。聚类的目标是根据一些相似度量(如欧几里德距离)将相似数据分类成一个聚类传统的聚类方法对高维数据的聚类性能通常很差，这是由于这些方法使用的相似度进行度量的效率较低。此外，这些方法在大规模数据集上通常具有较高的计算复杂度。因此，人们广泛研究降维和特征转换方法，将原始数据映射到一个新的特征空间中，生成的数据更容易被现有的分类器分离。一般来说，现有的数据变换方法包括线性变换如主成分分析(PCA)和等非线性变换如核方法、谱方法等。然而，潜在结构高度复杂的数据仍挑战着现有的聚类方法。深度神经网络(DNNs)由于其固有的高度非线性变换特性，可以将数据转换为更有利于聚类的表示形式。</p>
<p>##### 传统聚类方法分类</p>
<ol type="1">
<li><p>基于划分的方法（partition-based）</p></li>
<li><p>基于密度的方法（density-based）</p></li>
<li><p>层次的方法（hierarchical）</p></li>
<li><p>其他</p></li>
</ol>
<p>由于深度学习聚类的本质是学习面向聚类的表示，本文基于网络结构进行了分类</p>
<h5 id="深度学习的聚类方法">深度学习的聚类方法</h5>
<ol type="1">
<li>基于autoencoder：训练解码器和编码器，解码器是训练时候重建回原始数据，编码器是训练的mapping function（降维、非线性表示）</li>
<li>ClusterDNN: 通过特定聚类loss进行约束训练的前馈神经网络</li>
<li>基于GAN和VAE:它们不仅可以执行聚类任务，还可以从获得的聚类中生成新的样本</li>
</ol>
<h4 id="preliminaries">PRELIMINARIES</h4>
<p>结构：</p>
<ol type="1">
<li>MLP:对聚类任务，好的初始化是必要的，避免训练陷入局部最优</li>
<li>CNN：不需要特定初始化，但良好的初始化能显著提高性能。</li>
</ol>
<p>clustering loss：</p>
<ol type="1">
<li>主聚类损失 Principal Clustering Loss：这类聚类损失函数包括样本的聚类中心（cluster centroids）和聚类分配(cluster assignment)。在聚类损耗引导下对网络进行训练后，可以直接得到聚类。</li>
<li>辅助性损失 Auxiliary Clustering Loss：第二类只起到引导网络学习更可行的聚类表示的作用，不能直接输出聚类。这意味着只有辅助聚类损失的深层聚类方法需要在训练完网络后运行一种聚类方法才能得到聚类。</li>
</ol>
<p>metrics:</p>
<ol type="1">
<li>聚类准确率：</li>
</ol>
<p><img src="https://s2.loli.net/2022/02/10/TXK9Y1L7qJotGF3.png" /></p>
<p><span class="math display">\[A C C=\operatorname*{max}_{m}{\frac{\int_{i=1}^{n}\sum\{y_{i}=m(c_{i})\}}{n}}\]</span></p>
<p>c是聚类分配，y是ground truth， m是mapping function</p>
<ol start="2" type="1">
<li>Normalized Mutual Information(NMI)</li>
</ol>
<p><img src="https://s2.loli.net/2022/02/10/LnUXEqzWgvAIw8i.png" /></p>
<p>I是互信息，H是信息熵</p>
<h4 id="深度聚类方法">深度聚类方法</h4>
<p><img src="https://s2.loli.net/2022/02/10/hXsr5yONnlHUej8.png" /></p>
<p><img src="https://s2.loli.net/2022/02/10/CGwORIAzbknKH4X.png" /></p>
<p><img src="https://s2.loli.net/2022/02/10/Sqe2hEYPsKCtUQH.png" /></p>
<p>深度聚类方法的损失函数(优化目标)通常由网络损失Ln和聚类损失Lc两部分组成：</p>
<p><span class="math display">\[{\cal L}=\lambda L_{n}+(1-\lambda)L_{c}\]</span></p>
<p>Ln:网络损失可以是自动编码器(AE)的重构损失、变分自编码器(VAE)的变分损失或生成式对抗网络(GAN)的对抗损失。</p>
<p>Lc：群体之间特征区分更明显</p>
<ol type="1">
<li><p>AE based</p>
<p>reconstruction loss：</p>
<p><span class="math inline">\(\operatorname*{min}_{\phi,\theta}{\cal L}_{r e c}=\operatorname*{min}_{\bigl|n\bigr|}\sum_{i=1}^{n}\left|\left|\,x_{i}-g_{\theta}(f_{\phi}(x_{i})\right\gt \right|^{2}\)</span></p>
<p>total loss:</p>
<p><span class="math inline">\(L=\lambda L_{r e c}+(1-\lambda)L_{t}\)</span></p>
<p><img src="https://s2.loli.net/2022/02/10/P1CkvleuD8j9siN.png" /></p>
<p>提升性能：</p>
<ol type="1">
<li>结构：对于含有空间不变性的数据可加入pooling</li>
<li>鲁棒性：为了避免过拟合和提高鲁棒性，在输入中加入噪声</li>
<li>引入特征限制，如正则化</li>
<li>把各层的重构损失都加入到loss中</li>
</ol>
<p>例子：</p>
<ol type="1">
<li>DCN：它结合了自动编码器和k-means算法。首先进行预训练AE，然后联合优化重建损失和k-means损失。</li>
</ol></li>
<li><p>CDNN-Based</p>
<p>没有了reconstruction loss，基于cdn的算法存在获取损坏特征空间的风险，当所有的数据点被简单地映射到紧凑的聚类上时，导致聚类损失值很小，但没有意义。因此，需要仔细设计聚类loss，对于一定的聚类loss，网络初始化很重要。因此，基于cdn的深度聚类算法按照网络初始化的方式分为三类，即无监督预训练、有监督预训练和随机初始化。</p>
<p><img src="https://s2.loli.net/2022/02/10/kCElbFKsRIJX6Lf.png" /></p>
<ol type="1">
<li><p>unsupervised</p>
<p>这些算法首先以无监督的方式训练RBM或自动编码器，然后通过聚类loss对网络(仅对自动编码器的编码器部分)进行微调。</p></li>
<li><p>supervised</p>
<p>复杂的图像数据中提取可行的特征（domain pretrain）</p></li>
<li><p>non pretrain</p>
<p>在精心设计的聚类损失的指导下，网络也可以训练提取判别特征</p></li>
</ol></li>
<li><p>VAE-BASED</p>
<p>与传统的聚类方法相比，基于ae和cdn的深度聚类方法有了显著的改进。然而，它们是专门为聚类而设计的，不能揭示数据的真正底层结构，这就妨碍了它们扩展到聚类之外的其他任务，例如生成样本。VAE可以被认为是AE的生成变体，因为它强制AE的潜在代码遵循预定义的分布。VAE将变分贝叶斯方法与神经网络的灵活性和可扩展性相结合。</p>
<p><img src="https://s2.loli.net/2022/02/10/IgbOmpMXWUtED3q.png" /></p></li>
<li><p>GAN-BASED</p>
<p><img src="https://s2.loli.net/2022/02/10/fX25QWBSota1rRH.png" /></p></li>
</ol>
<h4 id="future-opportunities">FUTURE OPPORTUNITIES</h4>
<ol type="a">
<li>future opportunities</li>
</ol>
<ol type="1">
<li>目前还没有理论分析解释深度学习聚类工作原理以及如何进一步提高聚类性能</li>
<li>其他网络架构与聚类相结合</li>
<li>引入tricks降低训练难度，提高鲁棒性</li>
<li>引入其他任务如多任务学习、迁移学习</li>
</ol>
<h3 id="em-算法">EM 算法</h3>
<p>EM算法是一种迭代算法，1977年Dempster等人总结，用于含有隐变量的概率模型极大似然估计，或者最大后验估计。有两步，E步求期望，M步求极大。</p>
<p>Q函数：完全对数似然在给定观测数据Y以及当前<span class="math inline">\(\theta^{(i)}\)</span>的情况下对Z的期望。</p>
<p><span class="math inline">\(Q(\theta, \theta^{(i)}) = E_Z [\log P(Y,Z|\theta)|Y,\theta^{(i)}]\)</span></p>
<p><span class="math inline">\(\sum_{z(i)}Q^{i}(z^{(i)}) = 1\)</span></p>
<p><span class="math inline">\(Q^{i}(z^{(i)})\geq0\)</span></p>
<h5 id="步骤">步骤</h5>
<p>分为E步和M步，即求Q函数，极大化Q函数。</p>
<p>输入：观测数据Y，隐变量数据Z，联合分布<embed src="https://private.codecogs.com/gif.latex?P%28Y%2CZ%7C%5Ctheta%29" />，条件分布<embed src="https://private.codecogs.com/gif.latex?P%28Z%7CY%2C%5Ctheta%29" />；</p>
<p>输出：模型参数<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />。</p>
<ol type="1">
<li><p>选取参数<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />的初始值在<span class="math inline">\(\theta^{(0)}\)</span>（可任意选择，但是算法对初始值敏感)；开始迭代；</p></li>
<li><p>E步：计算<embed src="https://private.codecogs.com/gif.latex?%5Cbegin%7Balign*%7D%20Q%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29%20%26%3D%20E_Z%20%5B%5Clog%20P%28Y%2CZ%7C%5Ctheta%29%7CY%2C%5Ctheta%5E%7B%28i%29%7D%5D%5C%5C%20%26%3D%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%20%5Clog%20P%28Y%2CZ%7C%5Ctheta%29%20%5Cend%7Balign*%7D" /></p></li>
<li><p>M步：最大化<span class="math inline">\(Q(\theta, \theta^{(i)})\)</span>,得到<span class="math inline">\(\theta^{(i+1)}\)</span>：</p>
<figure>
<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E%7B%28i+1%29%7D%3D%5Carg%20%5Cmax%20Q%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29" /><figcaption>^{(i+1)}=Q(, ^{(i)})</figcaption>
</figure></li>
<li><p>重复2. 3.，直到收敛</p>
<p>停止迭代的条件：<embed src="https://private.codecogs.com/gif.latex?%5Cleft%20%5C%7C%20%5Ctheta%5E%7B%28i+1%29%7D-%5Ctheta%5E%7B%28i%29%7D%20%5Cright%20%5C%7C%20%3C%5Cvarepsilon_1" /> 或<embed src="https://private.codecogs.com/gif.latex?%5Cleft%20%5C%7C%20Q%28%5Ctheta%5E%7B%28i+1%29%7D%2C%20%5Ctheta%5E%7B%28i%29%7D%29-Q%28%5Ctheta%5E%7B%28i%29%7D%2C%5Ctheta%5E%7B%28i%29%7D%29%20%5Cright%20%5C%7C%3C%5Cvarepsilon_2" /></p></li>
</ol>
<h5 id="导出">导出</h5>
<p>给定观测数据Y，目标是极大化观测数据（不完全数据）Y关于参数<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />的对数似然函数，即</p>
<figure>
<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29%3D%5Clog%20P%28Y%7C%5Ctheta%29%3D%5Clog%20%5Csum_%7BZ%7DP%28Y%2CZ%7C%5Ctheta%29%3D%5Clog%20%5Cleft%20%5C%7B%20%5Csum_Z%20P%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%5Cright%20%5C%7D" /><figcaption>L()=P(Y|)=_{Z}P(Y,Z|)={ _Z P(Y|Z,) P(Z|)}</figcaption>
</figure>
<p>其中表示在模型参数为<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />时，观测数据Y的概率分布。</p>
<p><span class="math inline">\(\begin{align*} P(Y|\theta)&amp;=\sum_Z P(Y,Z|\theta)=\sum_Z P(Z|\theta)P(Y|Z,\theta)\\ \end{align*}\)</span></p>
<p>EM算法通过逐步迭代来逐步近似极大化<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29" />。假设第i次迭代后<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />的估计值为<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E%7B%28i%29%7D" />。下一轮的估计值<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />要使<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29%3E%20L%28%5Ctheta%5E%7B%28i%29%7D%20%29" />。故</p>
<figure>
<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29-L%28%5Ctheta%5E%7B%28i%29%7D%20%29%3D%5Clog%20%5Cleft%20%5C%7B%20%5Csum_Z%20P%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%20%5Cright%20%5C%7D-%5Clog%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%20%29" /><figcaption>L()-L(^{(i)} )={ _Z P(Y|Z,)P(Z|) }-P(Y|^{(i)} )</figcaption>
</figure>
<p>利用Jensen不等式（这里用的是<span class="math inline">\(log{\bigl(}\sum_{i=1}^{M}\lambda_{i}x_{i}{\bigr)}\leq\sum_{i=1}^{M}\lambda_{i}log{\bigl(}x_{i}{\bigr)}\)</span>）得到下界：</p>
<figure>
<embed src="https://private.codecogs.com/gif.latex?%5Cbegin%7Balign*%7D%20L%28%5Ctheta%29-L%28%5Ctheta%5E%7B%28i%29%7D%20%29%20%26%3D%5Clog%20%5Cleft%5C%7B%20%5Csum_Z%20P%28Y%7CZ%2C%5Ctheta%5E%7B%28i%29%7D%20%29%20%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%7D%7BP%28Y%7CZ%2C%5Ctheta%5E%7B%28i%29%7D%20%29%7D%20%5Cright%20%5C%7D%20-%20%5Clog%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%20%29%20%5C%5C%20%26%5Cgeq%20%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%20%29%5Clog%20%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%7D%20-%20%5Clog%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%5C%5C%20%26%3D%20%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%20%29%5Clog%20%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%7D%20-%20%5Csum_ZP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%20%29%20%5Clog%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%20%5C%5C%20%26%3D%20%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%20%29%5Clog%20%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D%20%5C%5C%20%5Cend%7Balign*%7D" /><figcaption><span class="math display">\[\begin{align*} L(\theta)-L(\theta^{(i)} ) &amp;=\log \left\{ \sum_Z P(Y|Z,\theta^{(i)} ) \frac{P(Y|Z,\theta) P(Z|\theta)}{P(Y|Z,\theta^{(i)} )} \right \} - \log P(Y|\theta^{(i)} ) \\ &amp;\geq \sum_Z P(Z|Y,\theta^{(i)} )\log \frac{P(Y|Z,\theta) P(Z|\theta)}{P(Z|Y,\theta^{(i)})} - \log P(Y|\theta^{(i)})\\ &amp;= \sum_Z P(Z|Y,\theta^{(i)} )\log \frac{P(Y|Z,\theta) P(Z|\theta)}{P(Z|Y,\theta^{(i)})} - \sum_ZP(Z|Y,\theta^{(i)} ) \log P(Y|\theta^{(i)}) \\ &amp;= \sum_Z P(Z|Y,\theta^{(i)} )\log \frac{P(Y|Z,\theta) P(Z|\theta)}{P(Z|Y,\theta^{(i)}) P(Y|\theta^{(i)})} \\ \end{align*}\]</span></figcaption>
</figure>
<p>令</p>
<figure>
<embed src="https://private.codecogs.com/gif.latex?B%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29%3DL%28%5Ctheta%5E%7B%28i%29%7D%29+%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%20%29%5Clog%20%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D" /><figcaption>B(, <sup>{(i)})=L(</sup>{(i)})+_Z P(Z|Y,^{(i)} )</figcaption>
</figure>
<p><embed src="https://private.codecogs.com/gif.latex?B%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29" />是<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29" />的一个下界。任何可使<embed src="https://private.codecogs.com/gif.latex?B%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29" />增大的<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />，都可以使<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29" />增加。选择能使当前<embed src="https://private.codecogs.com/gif.latex?B%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29" />极大的<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E%7B%28i+1%29%7D" />作为新的<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />值。</p>
<figure>
<embed src="https://private.codecogs.com/gif.latex?%5Cbegin%7Balign*%7D%20%5Ctheta%5E%7B%28i+1%29%7D%20%26%3D%5Carg%20%5Cmax%20%28L%28%5Ctheta%5E%7B%28i%29%7D%29+%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%20%29%5Clog%20%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D%29%20%5C%5C%20%26%3D%5Carg%20%5Cmax%20%28%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%29%5Clog%20%28P%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%29%5C%5C%20%26%3D%5Carg%20%5Cmax%20%28%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%5Clog%28P%28Y%2CZ%7C%5Ctheta%29%29%29%20%5C%5C%20%26%3D%5Carg%20%5Cmax%20Q%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29%20%5Cend%7Balign*%7D" /><figcaption><span class="math display">\[\begin{align*} \theta^{(i+1)} &amp;=\arg \max (L(\theta^{(i)})+\sum_Z P(Z|Y,\theta^{(i)} )\log \frac{P(Y|Z,\theta) P(Z|\theta)}{P(Z|Y,\theta^{(i)}) P(Y|\theta^{(i)})}) \\ &amp;=\arg \max (\sum_Z P(Z|Y,\theta^{(i)}))\log (P(Y|Z,\theta)P(Z|\theta))\\ &amp;=\arg \max (\sum_Z P(Z|Y,\theta^{(i)})\log(P(Y,Z|\theta))) \\ &amp;=\arg \max Q(\theta, \theta^{(i)}) \end{align*}\]</span></figcaption>
</figure>
<p>所以EM算法就是通过迭代不断求Q函数，并将之极大化，直至收敛。下图为EM算法的直观解释，<embed src="https://private.codecogs.com/gif.latex?B%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29" />是<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29" />的一个下界。</p>
<figure>
<img src="https://img-blog.csdnimg.cn/20181127162559502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI0MDM0NTQ1,size_16,color_FFFFFF,t_70" alt="" /><figcaption>img</figcaption>
</figure>
<p><strong>我们首先初始化模型的参数，我们基于这个参数对每一个隐变量进行分类，此时相当于我们观测到了隐变量。有了隐变量的观测值之后，原来含有隐变量的模型变成了不含隐变量的模型(以上是E步)，因此我们可以直接使用极大似然估计来更新模型的参数（M步），再基于新的参数开始新一轮的迭代，直到参数收敛。</strong></p>
<h4 id="收敛性">收敛性</h4>
<p>定理一：<embed src="https://private.codecogs.com/gif.latex?P%28Y%7C%5Ctheta%29" />为观测数据的似然函数，<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E%7B%28i%29%7D%28i%3D1%2C2%2C...%29" />是EM算法得到的参数估计序列，<embed src="https://private.codecogs.com/gif.latex?P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29" />是对应的似然函数序列，则<embed src="https://private.codecogs.com/gif.latex?P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29" />是单调递增的。</p>
<p>定理二：<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29%3D%5Clog%20P%28Y%7C%5Ctheta%29" />是观测数据的对数似然函数，<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E%7B%28i%29%7D%28i%3D1%2C2%2C...%29" />是EM算法得到的参数估计序列，<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%5E%7B%28i%29%7D%29" />是对应的对数似然函数序列。如果<embed src="https://private.codecogs.com/gif.latex?P%28Y%7C%5Ctheta%29" />有上界，则<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%5E%7B%28i%29%7D%29%3D%5Clog%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29" />收敛到某一值<embed src="https://private.codecogs.com/gif.latex?L%5E*" />；在函数<embed src="https://private.codecogs.com/gif.latex?Q%28%5Ctheta%2C%20%7B%5Ctheta%7D%27%29" />与<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29" />满足一定条件下，EM算法得到的参数估计序列<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E%7B%28i%29%7D%28i%3D1%2C2%2C...%29" />的收敛值<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E*" />是<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29" />的稳定点。</p>
<h3 id="gmm混合高斯">GMM混合高斯</h3>
<h5 id="模型定义">模型定义：</h5>
<p><span class="math inline">\(p(x)=\sum_{k=1}^{K}\alpha_{k}\mathcal{N}(x|\mu_{k},\Sigma_{k})\)</span></p>
<p><span class="math inline">\(\sum_{k=1}^{K}\alpha_{k}=1\)</span></p>
<ol type="1">
<li>多个高斯模型的加权平均；（单个表达能力不够）</li>
<li>混合：隐变量-&gt;属于哪一个高斯分布</li>
</ol>
<h5 id="参数学习">参数学习</h5>
<p><span class="math inline">\(\gamma_{j k}\)</span>代表第j个观测来源于k个分模型（01随机变量）---------这里的隐变量</p>
<ol type="1">
<li><p>写出完全对数似然</p>
<p><span class="math inline">\(\begin{aligned} P(y, \gamma \mid \theta) &amp;=\prod_{j=1}^{N} P\left(y_{j}, \gamma_{j 1}, \gamma_{j 2}, \cdots, \gamma_{j K} \mid \theta\right) \\ &amp;=\prod_{k=1}^{K} \prod_{j=1}^{N}\left[\alpha_{k} \phi\left(y_{j} \mid \theta_{k}\right)\right]^{\gamma_{k}} \\ &amp;=\prod_{k=1}^{K} \alpha_{k}^{n_{k}} \prod_{j=1}^{N}\left[\phi\left(y_{j} \mid \theta_{k}\right)\right]^{\gamma_{k}} \\ &amp;=\prod_{k=1}^{K} \alpha_{k}^{n_{k}} \prod_{j=1}^{N}\left[\frac{1}{\sqrt{2 \pi} \sigma_{k}} \exp \left(-\frac{\left(y_{j}-\mu_{k}\right)^{2}}{2 \sigma_{k}^{2}}\right)\right]^{\gamma_{k t}} \end{aligned}\)</span></p>
<p>对数似然：</p>
<p><span class="math inline">\(\log P(y,\gamma\vert\theta)=\sum_{k=1}^{K}[n_{k}\log\alpha_{k}+\sum_{j=1}^{N}\gamma_{k}\biggl[\log(\frac{1}{\sqrt{2\pi}})-\log\sigma_{k}-\frac{1}{2\sigma_{k}^{2}}(y_{j}-\mu_{k})^{2}\biggr]]\)</span></p>
<p>其中：<span class="math inline">\(n_k = \sum_{j=1}^{N}\gamma_{jk}\)</span>, <span class="math inline">\(\sum_{k=1}^{K}n_{k}=N\)</span> （Q函数推导的时候代入）</p></li>
<li><p>确定Q函数</p>
<p><span class="math inline">\(\begin{aligned} Q\left(\theta, \theta^{(i)}\right) &amp;=E\left[\log P(y, \gamma \mid \theta) \mid y, \theta^{(i)}\right] \\ &amp;=E\left\{\sum_{k=1}^{K} [n_{k} \log \alpha_{k}+\sum_{j=1}^{N} \gamma_{j k}\left[\log \left(\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left(y_{j}-\mu_{k}\right)^{2}\right]]\right\} \\ &amp;=\sum_{k=1}^{K}\left\{\sum_{j=1}^{N}[\left(E \gamma_{j k}\right) \log \alpha_{k}+\sum_{j=1}^{N}\left(E \gamma_{j k}\right)\left[\log \left(\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left(y_{j}-\mu_{k}\right)^{2}\right]]\right\} \end{aligned}\)</span></p>
<p>需要计算期望<span class="math inline">\(E (\gamma_{j k}|y, \theta)\)</span></p>
<p><span class="math inline">\(\hat{\gamma}_{j k} &amp;=E\left(\gamma_{j k} \mid y, \theta\right)=P\left(\gamma_{j k}=1 \mid y, \theta\right) \\ &amp;=\frac{P\left(\gamma_{j k}=1, y_{j} \mid \theta\right)}{\sum_{k=1}^{K} P\left(\gamma_{j k}=1, y_{j} \mid \theta\right)}&amp;=\frac{P\left(y_{j} \mid \gamma_{j k}=1, \theta\right) P\left(\gamma_{j k}=1 \mid \theta\right)}{\sum_{k=1}^{K} P\left(y_{j} \mid \gamma_{j k}=1, \theta\right) P\left(\gamma_{j k}=1 \mid \theta\right)} &amp;=\frac{\alpha_{k} \phi\left(y_{j} \mid \theta_{k}\right)}{\sum_{k=1}^{K} \alpha_{k} \phi\left(y_{j} \mid \theta_{k}\right)}, \quad j=1,2, \cdots, N ; \quad k=1,2, \cdots, K\)</span></p>
<p>Q函数为：</p>
<p><span class="math inline">\(Q\left(\theta, \theta^{(i)}\right)=\sum_{k=1}^{K} [n_{k} \log \alpha_{k}+\sum_{k=1}^{N} \hat{\gamma}_{j k}\left[\log \left(\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left(y_{j}-\mu_{k}\right)^{2}\right]]\)</span></p></li>
<li><p>确定M步</p>
<p>这一步求Q函数对<span class="math inline">\(\theta\)</span>的极大值，分别对<span class="math inline">\(\mu \space \sigma \space \alpha\)</span>求偏导等于零（<span class="math inline">\(\alpha\)</span>需要在满足和为1，所以除个N）解得</p>
<p><span class="math inline">\({\hat{\mu}}_{k}={\frac{\sum_{j=1}^{N}{\hat{\gamma}}_{k}y_{j}}{\sum_{j=1}^{N}{\hat{\gamma}}_{k}}}\)</span></p>
<p><span class="math inline">\(\hat{\sigma}_{k}^{2}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}\left(y_{j}-\mu_{k}\right)^{2}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}}\)</span></p>
<p><span class="math inline">\(\hat{\alpha}_{k}=\frac{n_{k}}{N}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}}{N}\)</span></p></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dsttsd.github.io/2022/01/24/week%208/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/50662067?s=400&u=b552d8b742d1e685ed0ddcc6a97d9f697535fa6b&v=4">
      <meta itemprop="name" content="DST">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DSTの杂货铺">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/24/week%208/" class="post-title-link" itemprop="url">第八次周报</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-24 11:22:55" itemprop="dateCreated datePublished" datetime="2022-01-24T11:22:55+08:00">2022-01-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-06-28 14:16:34" itemprop="dateModified" datetime="2022-06-28T14:16:34+08:00">2022-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%91%A8%E6%8A%A5/" itemprop="url" rel="index"><span itemprop="name">周报</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>week 8 内容：</p>
<ul>
<li>louis philippe morency 《多模态机器学习》 70%</li>
<li>论文阅读：
<ol type="1">
<li>Variational Autoencoders for Collaborative Filtering</li>
<li>A Survey on Curriculum Learning（TPAMI 2021）</li>
<li>sampling</li>
<li>VI、EM算法、GMM</li>
</ol></li>
</ul>
<h4 id="variational-autoencoders-for-collaborative-filteringwww-2018">Variational Autoencoders for Collaborative Filtering（WWW 2018）</h4>
<h5 id="主要工作">主要工作</h5>
<p>该文作者把变分自编码器拓展应用于基于隐式反馈的协同过滤推荐任务，希望通过非线性概率模型克服线性因子模型的局限。该文提出了基于变分自编码器（Variational Autoencoder）的生成模型VAE_CF，并针对变分自编码器的正则参数和概率模型选取做了适当调整（使用了多项式分布），使其在当时推荐任务中取得SOTA结果。</p>
<h5 id="模型">模型</h5>
<p><img src="https://s2.loli.net/2022/01/15/YlUGjvMeOuE2aZT.png" /></p>
<p>如上图所示，虚线代表了采样操作，a是传统AE，b是denoising AE（论文中使用了多项式分布的Mult-dae）， c代表vae（论文中是mult-vae）。</p>
<p><span class="math inline">\(\mathbf{z}_u \sim \mathcal{N}(0, \mathbf{I}_K), \pi(\mathbf{z}_u) \propto \exp\{f_\theta (\mathbf{z}_u\},\mathbf{x}_u \sim \mathrm{Mult}(N_u, \pi(\mathbf{z}_u))\)</span></p>
<p>该模型根据标准高斯分布抽取K维隐变量 <img src="https://www.zhihu.com/equation?tex=z_%7Bu%7D+%5Csim+%5Cmathcal%7BN%7D%280%2CI_%7BK%7D%29" alt="[公式]" /> ，然后根据非线性函数 <img src="https://www.zhihu.com/equation?tex=f_%7B%5Ctheta%7D%28%5Ccdot%29" alt="[公式]" /> 生成用户 <img src="https://www.zhihu.com/equation?tex=u" alt="[公式]" /> 点击所有物品的概率分布 <img src="https://www.zhihu.com/equation?tex=%5Cpi%EF%BC%88z_%7Bu%7D%EF%BC%89%5Cpropto+exp%28f_%7B%5Ctheta%7D%28z_%7Bu%7D%29%29" alt="[公式]" /> ，最后根据多项式分布 <img src="https://www.zhihu.com/equation?tex=x_%7Bu%7D+%5Csim+Mult%28N_%7Bu%7D%2C%5Cpi%28z_%7Bu%7D%29%29" alt="[公式]" /> 重构用户点击历史（隐式反馈的用户-物品评价矩阵对应行)。</p>
<p>Multi-DAE目标函数(对数似然)：</p>
<p><span class="math inline">\(\mathcal{L}_u(\theta, \phi) = \log p_\theta(\mathbf{x}_u | g_\phi(\mathbf{x}_u))\)</span></p>
<p>Multi-VAE目标函数（ELBO）：</p>
<p><span class="math inline">\(\mathcal{L}_u(\theta, \phi) = \mathbb{E}_{q_\phi(z_u | x_u)}[\log p_\theta(x_u | z_u)] - \beta \cdot KL(q_\phi(z_u | x_u) \| p(z_u))\)</span></p>
<p>VAE_CF模型较标准的变分自编码器做了如下调整：</p>
<p>1、将正则参数调至0.2（低于常规值1，具体来说使用了annealing来逐步增大beta），称其为部分正则化（partially regularized)，实际上是$-vae $在推荐上的应用。</p>
<p>2、使用了多项式分布进行重建而非高斯分布。</p>
<ul>
<li><strong>多项式似然非常适合于隐式反馈数据的建模，并且更接近 rank loss；</strong></li>
<li><strong>无论数据的稀缺性如何，采用principled Bayesian方法都更加稳健。</strong></li>
</ul>
<p>使用SGD进行优化：</p>
<p><img src="https://s2.loli.net/2022/01/15/ySYtBvW6TKdQoZE.png" /></p>
<h5 id="代码">代码</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiVAE</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Container module for Multi-VAE.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Multi-VAE : Variational Autoencoder with Multinomial Likelihood</span></span><br><span class="line"><span class="string">    See Variational Autoencoders for Collaborative Filtering</span></span><br><span class="line"><span class="string">    https://arxiv.org/abs/1802.05814</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, p_dims, q_dims=<span class="literal">None</span>, dropout=<span class="number">0.5</span></span>):</span></span><br><span class="line">    	<span class="comment"># p_dims = [200, 600, n_items]</span></span><br><span class="line">        <span class="built_in">super</span>(MultiVAE, self).__init__()</span><br><span class="line">        <span class="comment"># q -&gt; encoder | p-&gt; decoder</span></span><br><span class="line">        self.p_dims = p_dims</span><br><span class="line">        <span class="comment"># 确定维度</span></span><br><span class="line">        <span class="keyword">if</span> q_dims:</span><br><span class="line">            <span class="keyword">assert</span> q_dims[<span class="number">0</span>] == p_dims[-<span class="number">1</span>], <span class="string">&quot;In and Out dimensions must equal to each other&quot;</span></span><br><span class="line">            <span class="keyword">assert</span> q_dims[-<span class="number">1</span>] == p_dims[<span class="number">0</span>], <span class="string">&quot;Latent dimension for p- and q- network mismatches.&quot;</span></span><br><span class="line">            self.q_dims = q_dims</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.q_dims = p_dims[::-<span class="number">1</span>] <span class="comment"># [n_items, 600, 200]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Last dimension of q- network is for mean and variance</span></span><br><span class="line">        temp_q_dims = self.q_dims[:-<span class="number">1</span>] + [self.q_dims[-<span class="number">1</span>] * <span class="number">2</span>] <span class="comment"># [ n_items, 600, 400] ]</span></span><br><span class="line">        <span class="comment"># encoder</span></span><br><span class="line">        <span class="comment"># in:[ n_items, 600]</span></span><br><span class="line">        <span class="comment"># out:[600, 400]</span></span><br><span class="line">        self.q_layers = nn.ModuleList([nn.Linear(d_in, d_out) <span class="keyword">for</span></span><br><span class="line">            d_in, d_out <span class="keyword">in</span> <span class="built_in">zip</span>(temp_q_dims[:-<span class="number">1</span>], temp_q_dims[<span class="number">1</span>:])])</span><br><span class="line">        <span class="comment"># decoder</span></span><br><span class="line">        <span class="comment"># in:[200, 600,]</span></span><br><span class="line">        <span class="comment"># out:[600, n_items]</span></span><br><span class="line">        self.p_layers = nn.ModuleList([nn.Linear(d_in, d_out) <span class="keyword">for</span></span><br><span class="line">            d_in, d_out <span class="keyword">in</span> <span class="built_in">zip</span>(self.p_dims[:-<span class="number">1</span>], self.p_dims[<span class="number">1</span>:])])</span><br><span class="line">        </span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line">        self.init_weights()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        mu, logvar = self.encode(<span class="built_in">input</span>)</span><br><span class="line">        z = self.reparameterize(mu, logvar)</span><br><span class="line">        <span class="keyword">return</span> self.decode(z), mu, logvar</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        h = F.normalize(<span class="built_in">input</span>)</span><br><span class="line">        h = self.drop(h)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.q_layers):</span><br><span class="line">            h = layer(h)</span><br><span class="line">            <span class="keyword">if</span> i != <span class="built_in">len</span>(self.q_layers) - <span class="number">1</span>:</span><br><span class="line">                h = F.tanh(h)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 最后一层分均值和方差</span></span><br><span class="line">                mu = h[:, :self.q_dims[-<span class="number">1</span>]]</span><br><span class="line">                logvar = h[:, self.q_dims[-<span class="number">1</span>]:]</span><br><span class="line">        <span class="keyword">return</span> mu, logvar</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reparameterize</span>(<span class="params">self, mu, logvar</span>):</span></span><br><span class="line">        <span class="comment"># 重参数化技巧</span></span><br><span class="line">        <span class="keyword">if</span> self.training:</span><br><span class="line">            std = torch.exp(<span class="number">0.5</span> * logvar)</span><br><span class="line">            eps = torch.randn_like(std)</span><br><span class="line">            <span class="keyword">return</span> eps.mul(std).add_(mu)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> mu</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, z</span>):</span></span><br><span class="line">        <span class="comment"># 解码</span></span><br><span class="line">        h = z</span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.p_layers):</span><br><span class="line">            h = layer(h)</span><br><span class="line">            <span class="keyword">if</span> i != <span class="built_in">len</span>(self.p_layers) - <span class="number">1</span>:</span><br><span class="line">                h = F.tanh(h)</span><br><span class="line">        <span class="keyword">return</span> h</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.q_layers:</span><br><span class="line">            <span class="comment"># Xavier Initialization for weights</span></span><br><span class="line">            size = layer.weight.size()</span><br><span class="line">            fan_out = size[<span class="number">0</span>]</span><br><span class="line">            fan_in = size[<span class="number">1</span>]</span><br><span class="line">            std = np.sqrt(<span class="number">2.0</span>/(fan_in + fan_out))</span><br><span class="line">            layer.weight.data.normal_(<span class="number">0.0</span>, std)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Normal Initialization for Biases</span></span><br><span class="line">            layer.bias.data.normal_(<span class="number">0.0</span>, <span class="number">0.001</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.p_layers:</span><br><span class="line">            <span class="comment"># Xavier Initialization for weights</span></span><br><span class="line">            size = layer.weight.size()</span><br><span class="line">            fan_out = size[<span class="number">0</span>]</span><br><span class="line">            fan_in = size[<span class="number">1</span>]</span><br><span class="line">            std = np.sqrt(<span class="number">2.0</span>/(fan_in + fan_out))</span><br><span class="line">            layer.weight.data.normal_(<span class="number">0.0</span>, std)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Normal Initialization for Biases</span></span><br><span class="line">            layer.bias.data.normal_(<span class="number">0.0</span>, <span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loss_function</span>(<span class="params">recon_x, x, mu, logvar, anneal=<span class="number">1.0</span></span>):</span></span><br><span class="line">    <span class="comment"># BCE = F.binary_cross_entropy(recon_x, x)</span></span><br><span class="line">    BCE = -torch.mean(torch.<span class="built_in">sum</span>(F.log_softmax(recon_x, <span class="number">1</span>) * x, -<span class="number">1</span>))</span><br><span class="line">    KLD = -<span class="number">0.5</span> * torch.mean(torch.<span class="built_in">sum</span>(<span class="number">1</span> + logvar - mu.<span class="built_in">pow</span>(<span class="number">2</span>) - logvar.exp(), dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> BCE + anneal * KLD</span><br></pre></td></tr></table></figure>
<h3 id="a-survey-on-curriculum-learning-tpami-2021"><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2010.13166">A Survey on Curriculum Learning</a> （TPAMI 2021）</h3>
<h4 id="主要工作-1">主要工作</h4>
<p><strong>课程学习 (Curriculum learning, CL)</strong> 是近几年逐渐热门的一个前沿方向。Bengio 首先提出了课程学习（Curriculum learning，CL）的概念，它是一种训练策略，<strong>模仿人类的学习过程，主张让模型先从容易的样本开始学习，并逐渐进阶到复杂的样本和知识</strong>（从易到难）。CL策略在计算机视觉和自然语言处理等多种场景下，在提高各种模型的泛化能力和收敛率方面表现出了强大的能力。这篇综述一共调研了<strong>147篇</strong>文献，从<strong>问题定义</strong>、<strong>有效性分析</strong>、<strong>方法总结</strong>、<strong>未来研究方向</strong>等几大方面进行了详细的概括和总结。</p>
<h4 id="问题定义"><strong>问题定义：</strong></h4>
<p><img src="https://s2.loli.net/2022/01/23/5PTeLVslwGOk8ca.png" /></p>
<ol type="1">
<li>原始的课程学习</li>
</ol>
<p>课程式学习是在 <img src="https://www.zhihu.com/equation?tex=T" alt="[公式]" /> 个训练步骤上的训练标准序列 <img src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BC%7D%3D%5Cleft%5Clangle+Q_%7B1%7D%2C+%5Cldots%2C+Q_%7Bt%7D%2C+%5Cldots%2C+Q_%7BT%7D%5Cright%5Crangle" alt="[公式]" /> ， 每个准则 <img src="https://www.zhihu.com/equation?tex=Q_t+" alt="[公式]" /> 是目标训练分布 <img src="https://www.zhihu.com/equation?tex=P%28z%29" alt="[公式]" /> 的权重。该准则包括数据/任务、模型容量、学习目标等。满足的准则：</p>
<p><img src="https://s2.loli.net/2022/01/23/xeEOzGwCpJAuyQl.png" /></p>
<p>熵增（harder）、权重增加、最后的目标训练分布达到<img src="https://www.zhihu.com/equation?tex=P%28z%29" alt="[公式]" /></p>
<ol start="2" type="1">
<li><p>从数据层面泛化的课程学习（data level）</p>
<p>在T步内对于training target分布赋权学习。</p></li>
<li><p>更加泛化的课程学习（criteria ）</p>
<p>存在“hard to easy”效果更好的examples，为了让课程学习定义更加泛化。提出课程学习是在每一个<img src="https://www.zhihu.com/equation?tex=Q_t+" alt="[公式]" />利用机器学习的所有元素进行训练的设计，比如data、目标函数等。</p></li>
</ol>
<h4 id="有效性分析"><strong>有效性分析：</strong></h4>
<p><strong>1. 模型优化角度</strong></p>
<p>CL可以看成是一种特殊的 <strong>continuation 方法</strong>（<em>continuation</em> <em>method</em>是一种思想,就是不能一口吃个胖子,一步一步的解决问题。）。这种方法首先优化比较smooth的问题，然后逐渐优化到不够smooth的问题。如下图所示，continuation 方法提供了一个优化目标序列，从一个比较平滑的目标开始，很容易找到全局最小值，并在整个训练过程中跟踪局部最小值。另外，从更容易的目标中学习到的局部最小值具有更好的泛化能力，更有可能近似于全局最小值。</p>
<p><img src="https://s2.loli.net/2022/01/23/DLtjmfJROv4UAn5.png" /></p>
<p><strong>2. 数据分布角度（降噪denoising）</strong></p>
<p>训练分布和测试分布之间存在着由噪声/错误标注的训练数据引起的偏差。直观地讲，训练分布和目标（测试）分布有一个共同的大密度高置信度标注区域，这对应于CL中比较容易的样本。如下图所示，波峰附近的数据代表高置信度的数据即干净的数据，两边（尾部）代表的是低置信度的数据即噪声数据。左图看出，训练数据 <img src="https://www.zhihu.com/equation?tex=P_%7Btrain%7D" alt="[公式]" /> 比目标数据 <img src="https://www.zhihu.com/equation?tex=P_%7Btarget%7D" alt="[公式]" /> 噪声更多（其分布尾巴更翘）；右图看出，CL通过加权，一开始分配给噪声数据较小的权重，后面慢慢才给这些数据增加权重。通过这种方式，CL就可以减少来自负样本的影响。</p>
<p>另外，CL本质上是将目标分布下的预期风险上界最小化，这个上界表明，我们可以通过CL的核心思想来处理将 <img src="https://www.zhihu.com/equation?tex=P_%7Btarget%28x%29%7D" alt="[公式]" /> 上的预期风险最小化的任务：根据课程设置逐步抽取相对容易的样本，并将这些样本的经验风险最小化。</p>
<p><img src="https://s2.loli.net/2022/01/23/7TeCL5cNJp1bQo4.png" /></p>
<p>左图为数据分布图；右图为加权后的数据分布图</p>
<h4 id="应用场景"><strong>应用场景</strong></h4>
<p><img src="https://s2.loli.net/2022/01/23/TCrvcODa5G8Ryhf.png" /></p>
<p>从动机上看，主要分为的是指导训练和降噪。</p>
<ol type="1">
<li><p>指导训练：让训练可行/更好。应用：sparse reward RL, multi-task learning, GAN training, NAS（提高效率）;domain adaption, imbalanced classification（让训练可行）</p></li>
<li><p>denoise：加速训练、让训练更泛化和鲁棒（针对噪声多的或是异质数据）。应用：weakly-supervised or unsupervised learning, NLP tasks (neural machine translation, natural language understanding, etc.)</p></li>
</ol>
<h4 id="方法总结"><strong>方法总结：</strong></h4>
<p><img src="https://s2.loli.net/2022/01/23/FDdxo2cYHOs5jtN.png" /></p>
<p>课程学习的核心问题是得到一个ranking function，该函数能够对每条数据/每个任务给出其learning priority (学习优先程度)。这个则由<strong>难度测量器（Difficulty Measurer）</strong>实现。另外，我们什么时候把 Hard data 输入训练 以及 每次放多少呢？ 这个则由<strong>训练调度器 （Training Scheduler）</strong>决定。因此，目前大多数CL都是基于"难度测量器+训练调度器 "的框架设计。根据这两个<strong>是否自动设计</strong>可以将CL分成两个大类即 <strong>Predefined CL</strong> 和 <strong>Automatic CL</strong>。</p>
<p>Predifined CL 的难度测量器和训练调度器都是利用人类先验先验知识由人类专家去设计；而Automatic CL是以数据驱动的方式设计。</p>
<p><strong>1. Predefined CL</strong></p>
<p>1.1 预定义的难度测量器</p>
<p><img src="https://s2.loli.net/2022/01/23/WS4jFlfVeUMvKXE.png" /></p>
<p>complexity：structural complexity （如句子的长度等）</p>
<p>diversity ：分布的多样性（例如用信息熵来衡量）</p>
<p>noise：噪声角度（如直接从网上爬下的照片噪声就会多）</p>
<p>domain：domain knowledge</p>
<p>IR例子：</p>
<ol type="1">
<li><p>[18]Continuation Methods and Curriculum Learning for Learning to Rank(CIKM 2018)</p>
<p>提出了CM和CL两种策略来提升λ-MART的性能。</p>
<p>CM:先针对MSE训练回归森林，然后进行λ-MART训练</p>
<p>CL:需要从训练数据采样。第一种策略根据相关性来划分数据、第二种根据难度进行划分。</p></li>
<li><p>[82]Curriculum Learning Strategies for IR An Empirical Study on Conversation Response Ranking (ecir 2020)</p>
<p><img src="https://s2.loli.net/2022/01/23/uxMX823DslzH6SE.png" /></p></li>
</ol>
<p>1.2 预定义的训练调度器</p>
<p>训练调度器可以分为<strong>离散调度器</strong>和<strong>连续调度器</strong>。两者的区别在于：离散型调度器是在每一个固定的次数（&gt;1）后调整训练数据子集，或者在当前数据子集上收敛，而连续型调度器则是在每一个epoch调整训练数据子集。</p>
<p><strong>例子：</strong></p>
<p>离散调度器：</p>
<p>Baby step：排序后分buckets，训练过程逐步引入。</p>
<p><img src="https://s2.loli.net/2022/01/23/jrD6uS4PhdAwpBl.png" /></p>
<p>One-Pass ：直接使用下一个bucket，不是引入了。</p>
<p>连续调度器：</p>
<p>设计function<span class="math inline">\(\lambda(t)\)</span>得到引入的porpotion。</p>
<p>linear（像linear lr scheduler）：</p>
<p><img src="https://s2.loli.net/2022/01/23/upjsFOraNzX2PdT.png" /></p>
<p>root function：</p>
<p>为了让新加入的samples能够被充分学习，需要让加入的速率降低。</p>
<p><img src="https://s2.loli.net/2022/01/23/13cWqAkSYNIHblX.png" /></p>
<p>1.3 predifined CL存在的问题</p>
<ol type="1">
<li><p>很难预定义CL的方法找到测量器和调度器两者最优的组合。</p></li>
<li><p>不够灵活，没有考虑模型自身的反馈在训练过程中。</p></li>
<li><p>需要专家知识，代价较高。</p></li>
<li><p>人类认为容易的样本对模型来说就不一定容易。（人和机器模型的决策边界不一定一致）</p></li>
</ol>
<p><strong>2. Automatic CL</strong></p>
<p>与predifined CL对比：</p>
<p><img src="https://s2.loli.net/2022/01/23/dZRPtX5EVUijAsW.png" /></p>
<p>自动CL的方法论分为<strong>四类</strong>，即<strong>Self-paced Learning</strong>、<strong>Transfer Teacher</strong>、<strong>RL Teacher</strong> 和 <strong>其他自动CL</strong>。</p>
<p><img src="https://s2.loli.net/2022/01/23/c3SriTuBypC8dqf.png" /></p>
<h5 id="automatic-cl的分类">Automatic CL的分类</h5>
<p>2.1 Self-paced Learning</p>
<p>Self-paced Learning 让学生自己充当老师，根据其对实例的损失来衡量训练实例的难度。这种策略类似于学生自学：根据自己的现状决定自己的学习进度。主要利用基于loss的difficulty measurer，有很强的泛化能力。</p>
<p>原始loss，总共N个样本:</p>
<p><span class="math inline">\(\operatorname*{min}_{w}\mathbb{E}(w;\lambda)\sum_{i=1}^{\mathcal{N}}l_{i}+R(w)\)</span></p>
<p>SPL引入了spl-regularization，v是一系列的weights：</p>
<p><span class="math inline">\(\operatorname*{min}_{w;v\in[0,1]^{N}}\mathbb{S}\bigl(w,v;\lambda\bigr)\sum_{i=1}v_{i}l_{i}+g\bigl(v;\lambda\bigr).\)</span></p>
<p>SPL原始的l1正则项：</p>
<p><span class="math inline">\(g(v;\lambda)=-\lambda\sum_{i=1}^{N}v_{i}.\)</span></p>
<p>固定w，我们可以得到最优的v（Eq.9）：</p>
<p><span class="math inline">\(v_{i}^{\star}=\arg\operatorname*{min}_{v_{i}\in[0,1]}v_{i}l_{i}^{\bigtriangledown}+g(v_{i};\lambda),\quad i=1,2,\cdot\cdot\cdot\cdot,n\)</span></p>
<p>同样固定v,可以优化w(Eq.10）：</p>
<p><span class="math inline">\(w^{*}=\arg\operatorname*{min}_{w}\sum_{i=1}^{N}v_{i}^{*}l_{i}.\)</span></p>
<p><img src="https://s2.loli.net/2022/01/23/UXe3jE7iuJbkxwl.png" /></p>
<p>常见的regularizer:</p>
<p><img src="https://s2.loli.net/2022/01/23/YnPGMiKtEWDzqvy.png" /></p>
<p>2.2 Transfer Teacher</p>
<p>SPL在学习初期，学生网络可能是不够成熟的，所以会影响到后面的训练</p>
<p>Transfer Teacher 则通过1个强势的教师模型来充当教师，根据教师对实例的表现来衡量训练实例的难度。教师模型经过预训练，并将其知识转移到测量学生模型训练的例子难度上。</p>
<p><img src="https://s2.loli.net/2022/01/23/4UCLQi6gDkGNpP9.png" /></p>
<p>2.3 RL Teacher</p>
<p><img src="https://s2.loli.net/2022/01/23/t27kbNWCnFSzEPI.png" /></p>
<p>RL Teacher 采用强化学习（RL）模式，教师根据学生的反馈，实现数据动态选择。这种策略是人类教育中最理想的场景，教师和学生通过良性互动共同提高：学生根据教师选择的量身定做的学习材料取得最大的进步，而教师也有效地调整自己的教学策略，更好地进行教学。</p>
<p>2.4 其他自动 CL</p>
<p>除上述方法外，其他自动CL方法包括各种自动CL策略。如采取不同的优化技术来自动寻找模型训练的最佳课程，包括贝叶斯优化、元学习、hypernetworks等。</p>
<p><img src="https://s2.loli.net/2022/01/23/3B5RFyArkxzTpn4.png" /></p>
<h4 id="未来研究方向"><strong>未来研究方向：</strong></h4>
<ol type="1">
<li><strong>评价数据集和指标</strong></li>
</ol>
<p>虽然各种CL方法已经被提出并被证明是有效的，但很少有工作用通用基准来评估它们。在现有的文献中，数据集和指标在不同的应用中是多样化的。</p>
<p><strong>2. 更完善的理论分析</strong></p>
<p>现有的理论分析为理解CL提供了不同的角度。尽管如此，我们还需要更多的理论来帮助我们揭示为什么典型的CL是有效的。</p>
<p><strong>3. 更多的CL算法以及应用</strong></p>
<p>自动CL为CL在更广泛的研究领域提供了潜在的应用价值，已经成为一个前沿方向。因此，一个很有前途的方向是设计更多的自动CL方法，这些方法可具有<strong>不同的优化方式</strong>(如：bandit 算法、元学习、超参数优化等)和<strong>不同的目标</strong>(如：数据选择/加权、寻找最佳损失函数或假设空间等)。除了方法之外，还应该探索CL在更多领域中的应用。</p>
<h4 id="总结"><strong>总结：</strong></h4>
<p>这篇主要做了以下三项工作：</p>
<ol type="1">
<li>总结了现有的基于 "难度测量器+训练调度器 "总体框架的CL设计，并进一步将自动CL的方法论分为四类，即<strong>Self-paced Learning</strong>、<strong>Transfer Teacher</strong>、<strong>RL Teacher</strong> 和 <strong>其他自动CL</strong>。</li>
<li>分析了选择不同CL设计的原则，以利于实际应用。</li>
<li>对连接CL和其他机器学习概念(包括转移学习、元学习、持续学习和主动学习等)的关系的见解，然后指出CL的挑战以及未来潜在的研究方向，值得进一步研究。</li>
</ol>
<h3 id="sampling">Sampling</h3>
<p>mote carlo integration：求定积分</p>
<p><img src="https://s2.loli.net/2022/01/23/JnlM7krEuFVONct.png" /></p>
<p>Inverse Transform Sampling ：</p>
<p>利用cdf:<span class="math inline">\(F(x) = P(X \le x)\)</span></p>
<p><img src="https://s2.loli.net/2022/01/23/1MEua9jnlGoke2V.png" /></p>
<p>算法：</p>
<p><img src="https://s2.loli.net/2022/01/23/yIbS5YH6lwUqoEW.png" /></p>
<p>Ancestral sampling（祖先采样）得到联合概率：</p>
<p><img src="https://s2.loli.net/2022/01/23/kQwiGLOM1vcgI7V.png" /></p>
<p>拒绝采样（rejection sampling）：</p>
<p>利用比较容易采样的分布，罩住需要采样分布；</p>
<p><img src="https://s2.loli.net/2022/01/24/DTG2BoOPxS1Rrgj.png" /></p>
<p>重要性采样（importance sampling）：</p>
<p>不是采样方法，而是快速得到函数期望。依然从容易采样的Q入手，通过权重进行调整。</p>
<p><img src="https://s2.loli.net/2022/01/24/u6qAjd23QIBWUcv.png" /></p>
<p><img src="https://s2.loli.net/2022/01/24/kPQHun5fj4Ev7WF.png" /></p>
<p>可以利用重要性采样的w进行重采样：</p>
<p><img src="https://s2.loli.net/2022/01/24/YErSLHKVwh5Xqng.png" /></p>
<p>如果使用拒绝采样、重要性采样，尽量保证proposal distribution是heavy tail的，如果Q密度为0时，P还有密度，那么基本上采样效率会很低（找相近的分布采样效率会高）。</p>
<h3 id="em-and-gmm">EM and GMM</h3>
<p>https://blog.csdn.net/weixin_43661031/article/details/91358990</p>
<h4 id="em-算法">EM 算法</h4>
<p>EM算法是一种迭代算法，1977年Dempster等人总结，用于含有因变量的概率模型极大似然估计，或者最大后验估计。有两步，E步求期望，M步求极大。</p>
<p>Q函数：完全对数似然在给定观测数据Y以及当前<span class="math inline">\(\theta^{(i)}\)</span>的情况下对Z的期望。</p>
<p><span class="math inline">\(Q(\theta, \theta^{(i)}) = E_Z [\log P(Y,Z|\theta)|Y,\theta^{(i)}]\)</span></p>
<p>$_{z(i)}Q<sup>{i}(z</sup>{(i)})=1 $</p>
<p><span class="math inline">\(Q^{i}(z^{(i)})\geq0\)</span></p>
<h5 id="步骤">步骤</h5>
<p>分为E步和M步，即求Q函数，极大化Q函数。</p>
<p>输入：观测数据Y，隐变量数据Z，联合分布<embed src="https://private.codecogs.com/gif.latex?P%28Y%2CZ%7C%5Ctheta%29" />，条件分布<embed src="https://private.codecogs.com/gif.latex?P%28Z%7CY%2C%5Ctheta%29" />；</p>
<p>输出：模型参数<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />。</p>
<ol type="1">
<li><p>选取参数<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />的初始值在<span class="math inline">\(\theta^{(0)}\)</span>（可任意选择，但是算法对初始值敏感)；开始迭代；</p></li>
<li><p>E步：计算<embed src="https://private.codecogs.com/gif.latex?%5Cbegin%7Balign*%7D%20Q%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29%20%26%3D%20E_Z%20%5B%5Clog%20P%28Y%2CZ%7C%5Ctheta%29%7CY%2C%5Ctheta%5E%7B%28i%29%7D%5D%5C%5C%20%26%3D%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%20%5Clog%20P%28Y%2CZ%7C%5Ctheta%29%20%5Cend%7Balign*%7D" /></p></li>
<li><p>M步：最大化<span class="math inline">\(Q(\theta, \theta^{(i)})\)</span>,得到<span class="math inline">\(\theta^{(i+1)}\)</span>：</p>
<figure>
<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E%7B%28i+1%29%7D%3D%5Carg%20%5Cmax%20Q%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29" /><figcaption>^{(i+1)}=Q(, ^{(i)})</figcaption>
</figure></li>
<li><p>重复2. 3.，直到收敛</p>
<p>停止迭代的条件：<embed src="https://private.codecogs.com/gif.latex?%5Cleft%20%5C%7C%20%5Ctheta%5E%7B%28i+1%29%7D-%5Ctheta%5E%7B%28i%29%7D%20%5Cright%20%5C%7C%20%3C%5Cvarepsilon_1" /> 或<embed src="https://private.codecogs.com/gif.latex?%5Cleft%20%5C%7C%20Q%28%5Ctheta%5E%7B%28i+1%29%7D%2C%20%5Ctheta%5E%7B%28i%29%7D%29-Q%28%5Ctheta%5E%7B%28i%29%7D%2C%5Ctheta%5E%7B%28i%29%7D%29%20%5Cright%20%5C%7C%3C%5Cvarepsilon_2" /></p></li>
</ol>
<h5 id="导出">导出</h5>
<p>给定观测数据Y，目标是极大化观测数据（不完全数据）Y关于参数<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />的对数似然函数，即</p>
<figure>
<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29%3D%5Clog%20P%28Y%7C%5Ctheta%29%3D%5Clog%20%5Csum_%7BZ%7DP%28Y%2CZ%7C%5Ctheta%29%3D%5Clog%20%5Cleft%20%5C%7B%20%5Csum_Z%20P%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%5Cright%20%5C%7D" /><figcaption>L()=P(Y|)=_{Z}P(Y,Z|)={ _Z P(Y|Z,) P(Z|)}</figcaption>
</figure>
<p>其中表示在模型参数为<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />时，观测数据Y的概率分布。</p>
<p><span class="math inline">\(\begin{align*} P(Y|\theta)&amp;=\sum_Z P(Y,Z|\theta)=\sum_Z P(Z|\theta)P(Y|Z,\theta)\\ \end{align*}\)</span></p>
<p>EM算法通过逐步迭代来逐步近似极大化<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29" />。假设第i次迭代后<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />的估计值为<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E%7B%28i%29%7D" />。下一轮的估计值<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />要使<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29%3E%20L%28%5Ctheta%5E%7B%28i%29%7D%20%29" />。故</p>
<figure>
<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29-L%28%5Ctheta%5E%7B%28i%29%7D%20%29%3D%5Clog%20%5Cleft%20%5C%7B%20%5Csum_Z%20P%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%20%5Cright%20%5C%7D-%5Clog%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%20%29" /><figcaption>L()-L(^{(i)} )={ _Z P(Y|Z,)P(Z|) }-P(Y|^{(i)} )</figcaption>
</figure>
<p>利用Jensen不等式（这里用的是<span class="math inline">\(log{\bigl(}\sum_{i=1}^{M}\lambda_{i}x_{i}{\bigr)}\leq\sum_{i=1}^{M}\lambda_{i}log{\bigl(}x_{i}{\bigr)}\)</span>）得到下界：</p>
<figure>
<embed src="https://private.codecogs.com/gif.latex?%5Cbegin%7Balign*%7D%20L%28%5Ctheta%29-L%28%5Ctheta%5E%7B%28i%29%7D%20%29%20%26%3D%5Clog%20%5Cleft%5C%7B%20%5Csum_Z%20P%28Y%7CZ%2C%5Ctheta%5E%7B%28i%29%7D%20%29%20%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%7D%7BP%28Y%7CZ%2C%5Ctheta%5E%7B%28i%29%7D%20%29%7D%20%5Cright%20%5C%7D%20-%20%5Clog%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%20%29%20%5C%5C%20%26%5Cgeq%20%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%20%29%5Clog%20%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%7D%20-%20%5Clog%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%5C%5C%20%26%3D%20%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%20%29%5Clog%20%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%7D%20-%20%5Csum_ZP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%20%29%20%5Clog%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%20%5C%5C%20%26%3D%20%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%20%29%5Clog%20%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D%20%5C%5C%20%5Cend%7Balign*%7D" /><figcaption><span class="math display">\[\begin{align*} L(\theta)-L(\theta^{(i)} ) &amp;=\log \left\{ \sum_Z P(Y|Z,\theta^{(i)} ) \frac{P(Y|Z,\theta) P(Z|\theta)}{P(Y|Z,\theta^{(i)} )} \right \} - \log P(Y|\theta^{(i)} ) \\ &amp;\geq \sum_Z P(Z|Y,\theta^{(i)} )\log \frac{P(Y|Z,\theta) P(Z|\theta)}{P(Z|Y,\theta^{(i)})} - \log P(Y|\theta^{(i)})\\ &amp;= \sum_Z P(Z|Y,\theta^{(i)} )\log \frac{P(Y|Z,\theta) P(Z|\theta)}{P(Z|Y,\theta^{(i)})} - \sum_ZP(Z|Y,\theta^{(i)} ) \log P(Y|\theta^{(i)}) \\ &amp;= \sum_Z P(Z|Y,\theta^{(i)} )\log \frac{P(Y|Z,\theta) P(Z|\theta)}{P(Z|Y,\theta^{(i)}) P(Y|\theta^{(i)})} \\ \end{align*}\]</span></figcaption>
</figure>
<p>令</p>
<figure>
<embed src="https://private.codecogs.com/gif.latex?B%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29%3DL%28%5Ctheta%5E%7B%28i%29%7D%29+%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%20%29%5Clog%20%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D" /><figcaption>B(, <sup>{(i)})=L(</sup>{(i)})+_Z P(Z|Y,^{(i)} )</figcaption>
</figure>
<p><embed src="https://private.codecogs.com/gif.latex?B%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29" />是<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29" />的一个下界。任何可使<embed src="https://private.codecogs.com/gif.latex?B%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29" />增大的<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />，都可以使<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29" />增加。选择能使当前<embed src="https://private.codecogs.com/gif.latex?B%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29" />极大的<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E%7B%28i+1%29%7D" />作为新的<embed src="https://private.codecogs.com/gif.latex?%5Ctheta" />值。</p>
<figure>
<embed src="https://private.codecogs.com/gif.latex?%5Cbegin%7Balign*%7D%20%5Ctheta%5E%7B%28i+1%29%7D%20%26%3D%5Carg%20%5Cmax%20%28L%28%5Ctheta%5E%7B%28i%29%7D%29+%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%20%29%5Clog%20%5Cfrac%7BP%28Y%7CZ%2C%5Ctheta%29%20P%28Z%7C%5Ctheta%29%7D%7BP%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29%7D%29%20%5C%5C%20%26%3D%5Carg%20%5Cmax%20%28%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%29%5Clog%20%28P%28Y%7CZ%2C%5Ctheta%29P%28Z%7C%5Ctheta%29%29%5C%5C%20%26%3D%5Carg%20%5Cmax%20%28%5Csum_Z%20P%28Z%7CY%2C%5Ctheta%5E%7B%28i%29%7D%29%5Clog%28P%28Y%2CZ%7C%5Ctheta%29%29%29%20%5C%5C%20%26%3D%5Carg%20%5Cmax%20Q%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29%20%5Cend%7Balign*%7D" /><figcaption><span class="math display">\[\begin{align*} \theta^{(i+1)} &amp;=\arg \max (L(\theta^{(i)})+\sum_Z P(Z|Y,\theta^{(i)} )\log \frac{P(Y|Z,\theta) P(Z|\theta)}{P(Z|Y,\theta^{(i)}) P(Y|\theta^{(i)})}) \\ &amp;=\arg \max (\sum_Z P(Z|Y,\theta^{(i)}))\log (P(Y|Z,\theta)P(Z|\theta))\\ &amp;=\arg \max (\sum_Z P(Z|Y,\theta^{(i)})\log(P(Y,Z|\theta))) \\ &amp;=\arg \max Q(\theta, \theta^{(i)}) \end{align*}\]</span></figcaption>
</figure>
<p>所以EM算法就是通过迭代不断求Q函数，并将之极大化，直至收敛。下图为EM算法的直观解释，<embed src="https://private.codecogs.com/gif.latex?B%28%5Ctheta%2C%20%5Ctheta%5E%7B%28i%29%7D%29" />是<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29" />的一个下界。</p>
<figure>
<img src="https://img-blog.csdnimg.cn/20181127162559502.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzI0MDM0NTQ1,size_16,color_FFFFFF,t_70" alt="" /><figcaption>img</figcaption>
</figure>
<p><strong>我们首先初始化模型的参数，我们基于这个参数对每一个隐变量进行分类，此时相当于我们观测到了隐变量。有了隐变量的观测值之后，原来含有隐变量的模型变成了不含隐变量的模型(以上是E步)，因此我们可以直接使用极大似然估计来更新模型的参数（M步），再基于新的参数开始新一轮的迭代，直到参数收敛。</strong></p>
<h4 id="收敛性">收敛性</h4>
<p>定理一：<embed src="https://private.codecogs.com/gif.latex?P%28Y%7C%5Ctheta%29" />为观测数据的似然函数，<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E%7B%28i%29%7D%28i%3D1%2C2%2C...%29" />是EM算法得到的参数估计序列，<embed src="https://private.codecogs.com/gif.latex?P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29" />是对应的似然函数序列，则<embed src="https://private.codecogs.com/gif.latex?P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29" />是单调递增的。</p>
<p>定理二：<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29%3D%5Clog%20P%28Y%7C%5Ctheta%29" />是观测数据的对数似然函数，<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E%7B%28i%29%7D%28i%3D1%2C2%2C...%29" />是EM算法得到的参数估计序列，<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%5E%7B%28i%29%7D%29" />是对应的对数似然函数序列。如果<embed src="https://private.codecogs.com/gif.latex?P%28Y%7C%5Ctheta%29" />有上界，则<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%5E%7B%28i%29%7D%29%3D%5Clog%20P%28Y%7C%5Ctheta%5E%7B%28i%29%7D%29" />收敛到某一值<embed src="https://private.codecogs.com/gif.latex?L%5E*" />；在函数<embed src="https://private.codecogs.com/gif.latex?Q%28%5Ctheta%2C%20%7B%5Ctheta%7D%27%29" />与<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29" />满足一定条件下，EM算法得到的参数估计序列<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E%7B%28i%29%7D%28i%3D1%2C2%2C...%29" />的收敛值<embed src="https://private.codecogs.com/gif.latex?%5Ctheta%5E*" />是<embed src="https://private.codecogs.com/gif.latex?L%28%5Ctheta%29" />的稳定点。</p>
<h4 id="gmm混合高斯">GMM混合高斯</h4>
<h5 id="模型定义">模型定义：</h5>
<p><span class="math inline">\(p(x)=\sum_{k=1}^{K}\alpha_{k}\mathcal{N}(x|\mu_{k},\Sigma_{k})\)</span></p>
<p><span class="math inline">\(\sum_{k=1}^{K}\alpha_{k}=1\)</span></p>
<ol type="1">
<li>多个高斯模型的加权平均；（单个表达能力不够）</li>
<li>混合：隐变量-&gt;属于哪一个高斯分布</li>
</ol>
<h5 id="参数学习">参数学习</h5>
<p><span class="math inline">\(\gamma_{j k}\)</span>代表第j个观测来源于k个分模型（01随机变量）---------这里的隐变量</p>
<ol type="1">
<li><p>写出完全对数似然</p>
<p><span class="math inline">\(\begin{aligned} P(y, \gamma \mid \theta) &amp;=\prod_{j=1}^{N} P\left(y_{j}, \gamma_{j 1}, \gamma_{j 2}, \cdots, \gamma_{j K} \mid \theta\right) \\ &amp;=\prod_{k=1}^{K} \prod_{j=1}^{N}\left[\alpha_{k} \phi\left(y_{j} \mid \theta_{k}\right)\right]^{\gamma_{k}} \\ &amp;=\prod_{k=1}^{K} \alpha_{k}^{n_{k}} \prod_{j=1}^{N}\left[\phi\left(y_{j} \mid \theta_{k}\right)\right]^{\gamma_{k}} \\ &amp;=\prod_{k=1}^{K} \alpha_{k}^{n_{k}} \prod_{j=1}^{N}\left[\frac{1}{\sqrt{2 \pi} \sigma_{k}} \exp \left(-\frac{\left(y_{j}-\mu_{k}\right)^{2}}{2 \sigma_{k}^{2}}\right)\right]^{\gamma_{k t}} \end{aligned}\)</span></p>
<p>对数似然：</p>
<p><span class="math inline">\(\log P(y,\gamma\vert\theta)=\sum_{k=1}^{K}[n_{k}\log\alpha_{k}+\sum_{j=1}^{N}\gamma_{k}\biggl[\log(\frac{1}{\sqrt{2\pi}})-\log\sigma_{k}-\frac{1}{2\sigma_{k}^{2}}(y_{j}-\mu_{k})^{2}\biggr]]\)</span></p>
<p>其中：<span class="math inline">\(n_k = \sum_{j=1}^{N}\gamma_{jk}\)</span>, <span class="math inline">\(\sum_{k=1}^{K}n_{k}=N\)</span> （Q函数推导的时候代入）</p></li>
<li><p>确定Q函数</p>
<p><span class="math inline">\(\begin{aligned} Q\left(\theta, \theta^{(i)}\right) &amp;=E\left[\log P(y, \gamma \mid \theta) \mid y, \theta^{(i)}\right] \\ &amp;=E\left\{\sum_{k=1}^{K} [n_{k} \log \alpha_{k}+\sum_{j=1}^{N} \gamma_{j k}\left[\log \left(\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left(y_{j}-\mu_{k}\right)^{2}\right]]\right\} \\ &amp;=\sum_{k=1}^{K}\left\{\sum_{j=1}^{N}[\left(E \gamma_{j k}\right) \log \alpha_{k}+\sum_{j=1}^{N}\left(E \gamma_{j k}\right)\left[\log \left(\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left(y_{j}-\mu_{k}\right)^{2}\right]]\right\} \end{aligned}\)</span></p>
<p>需要计算期望<span class="math inline">\(E (\gamma_{j k}|y, \theta)\)</span></p>
<p><span class="math inline">\(\hat{\gamma}_{j k} &amp;=E\left(\gamma_{j k} \mid y, \theta\right)=P\left(\gamma_{j k}=1 \mid y, \theta\right) \\ &amp;=\frac{P\left(\gamma_{j k}=1, y_{j} \mid \theta\right)}{\sum_{k=1}^{K} P\left(\gamma_{j k}=1, y_{j} \mid \theta\right)}&amp;=\frac{P\left(y_{j} \mid \gamma_{j k}=1, \theta\right) P\left(\gamma_{j k}=1 \mid \theta\right)}{\sum_{k=1}^{K} P\left(y_{j} \mid \gamma_{j k}=1, \theta\right) P\left(\gamma_{j k}=1 \mid \theta\right)} &amp;=\frac{\alpha_{k} \phi\left(y_{j} \mid \theta_{k}\right)}{\sum_{k=1}^{K} \alpha_{k} \phi\left(y_{j} \mid \theta_{k}\right)}, \quad j=1,2, \cdots, N ; \quad k=1,2, \cdots, K\)</span></p>
<p>Q函数为：</p>
<p><span class="math inline">\(Q\left(\theta, \theta^{(i)}\right)=\sum_{k=1}^{K} [n_{k} \log \alpha_{k}+\sum_{k=1}^{N} \hat{\gamma}_{j k}\left[\log \left(\frac{1}{\sqrt{2 \pi}}\right)-\log \sigma_{k}-\frac{1}{2 \sigma_{k}^{2}}\left(y_{j}-\mu_{k}\right)^{2}\right]]\)</span></p></li>
<li><p>确定M步</p>
<p>这一步求Q函数对<span class="math inline">\(\theta\)</span>的极大值，分别对<span class="math inline">\(\mu \space \sigma \space \alpha\)</span>求偏导等于零（<span class="math inline">\(\alpha\)</span>需要在满足和为1，所以除个N）解得</p>
<p><span class="math inline">\({\hat{\mu}}_{k}={\frac{\sum_{j=1}^{N}{\hat{\gamma}}_{k}y_{j}}{\sum_{j=1}^{N}{\hat{\gamma}}_{k}}}\)</span></p>
<p><span class="math inline">\(\hat{\sigma}_{k}^{2}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}\left(y_{j}-\mu_{k}\right)^{2}}{\sum_{j=1}^{N} \hat{\gamma}_{j k}}\)</span></p>
<p><span class="math inline">\(\hat{\alpha}_{k}=\frac{n_{k}}{N}=\frac{\sum_{j=1}^{N} \hat{\gamma}_{j k}}{N}\)</span></p></li>
</ol>
<p>整体算法：</p>
<h3 id="vi与em算法关系">VI与EM算法关系</h3>
<p>https://zhuanlan.zhihu.com/p/97284299</p>
<p>Variational inference （变分推断，以下简称VI）和Expectation maximization（期望极大化，以下简称EM）这两种算法实际上是密切相关的。<strong>实际上我们可以将EM看作VI的特殊形式。</strong></p>
<p>对于 <img src="https://www.zhihu.com/equation?tex=z+%5Crightarrow+x" alt="[公式]" /> ，我们有 <img src="https://www.zhihu.com/equation?tex=p%28x%2Cz%3B%5Ctheta%29+%3D+p%28x%7Cz%3B%5Ctheta%29p%28z%3B%5Ctheta%29" alt="[公式]" /> 。<strong>如果我们想根据</strong> <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]" /> <strong>推测隐变量 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]" /> 的概率，我们就需要计算如下的积分： <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx%3B%5Ctheta%29+%3D+%5Cfrac%7Bp%28x%2Cz%3B%5Ctheta%29%7D%7B%5Cint+dz%5C+p%28x%2Cz%3B%5Ctheta%29%7D" alt="[公式]" /> 。然而 <img src="https://www.zhihu.com/equation?tex=%5Cint+dz%5C+p%28x%2Cz%3B%5Ctheta%29" alt="[公式]" /> 有时难以计算，尤其是对于高维的系统，因为高维系统里面的积分复杂度很高。</strong>因此我们需要发展一种更加方便的方法来近似表达 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx%3B%5Ctheta%29" alt="[公式]" /> 。VI就是用函数 <img src="https://www.zhihu.com/equation?tex=q%28z%29" alt="[公式]" /> 来近似表达 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx%3B%5Ctheta%29" alt="[公式]" /></p>
<h4 id="jensens-inequality">Jensen's inequality</h4>
<p>对于任何的凸函数（convex function） <img src="https://www.zhihu.com/equation?tex=f%28x%29" alt="[公式]" /> ，我们都有 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%28f%28x%29%29%5Cgeq+f%28%5Cmathbb%7BE%7D%28x%29%29" alt="[公式]" /> 。Fig 1是一个直观的特例，当自变量 <img src="https://www.zhihu.com/equation?tex=X" alt="[公式]" /> 的分布是在 <img src="https://www.zhihu.com/equation?tex=a%EF%BC%8Cb" alt="[公式]" /> 两点的均匀分布的时候，从图中可以明显看出 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%28f%28X%29%29%5Cgeq+f%28%5Cmathbb%7BE%7D%28X%29%29" alt="[公式]" /> 。Jensen's inequality <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%28f%28x%29%29%5Cgeq+f%28%5Cmathbb%7BE%7D%28x%29%29" alt="[公式]" /> 取等号的条件是 <img src="https://www.zhihu.com/equation?tex=x%3D%5Cmathbb%7BE%7D%28x%29" alt="[公式]" /> ，即 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]" /> 是一个常数。</p>
<p><img src="https://pic4.zhimg.com/80/v2-e09c6f9ace0555fe656c618eeff469b7_1440w.jpg" alt="img" />Fig 1. 一个凸函数以及E[(f(X))]和f[E(X)]的取值比较，这里X的分布是在a和b两点的均匀分布 （图片来自于Andrew Ng的讲义，见Reference &amp;amp;amp;amp;amp;amp;amp;amp;amp;amp; Acknowledgement）</p>
<p><strong>由于 <img src="https://www.zhihu.com/equation?tex=%5Clog%28%5Ccdot%29" alt="[公式]" /> 是一个凹函数，即 <img src="https://www.zhihu.com/equation?tex=-%5Clog%28%5Ccdot%29" alt="[公式]" /> 是一个凸函数，所以满足 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D%28%5Clog%28x%29%29%5Cleq+%5Clog%28%5Cmathbb%7BE%7D%28x%29%29" alt="[公式]" /> ，取等号的条件是自变量 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]" /> 是一个常数。</strong>之后我们要用到有关 <img src="https://www.zhihu.com/equation?tex=%5Clog%28%5Ccdot%29" alt="[公式]" /> 的这些性质。</p>
<h4 id="kl-divergence-elbo">KL divergence &amp; ELBO</h4>
<p>先证明一个数学结论：对于任意的概率 <img src="https://www.zhihu.com/equation?tex=p%28x%3B%5Ctheta%29" alt="[公式]" /> （ <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]" /> 是变量， <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]" /> 是参数）， <img src="https://www.zhihu.com/equation?tex=%5Cforall+%5C++q%28z%29" alt="[公式]" /> （这里 <img src="https://www.zhihu.com/equation?tex=q%28z%29" alt="[公式]" /> 是某个关于 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]" /> 的概率密度函数），我们有</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Bsplit%7D+%5Clog+p%28x%3B%5Ctheta%29+%3D+%26+%5Clog+%5Cint+dz%5C+p%28x%2Cz%3B%5Ctheta%29+%5C%5C+%3D%26%5Clog+%5Cint+dz%5C+q%28z%29+%5Cfrac%7Bp%28x%2Cz%3B%5Ctheta%29%7D%7Bq%28z%29%7D+%5C%5C++%3D%26+%5Clog+%5Cmathbb%7BE%7D_%7Bz%5Csim+q%7D%5Cleft%28+%5Cfrac%7Bp%28x%2Cz%3B%5Ctheta%29%7D%7Bq%28z%29%7D+%5Cright%29+%5C%5C+%5Cgeq+%26+%5Cmathbb%7BE%7D_%7Bz%5Csim+q%7D+%5Cleft%28+%5Clog%5Cleft%28+%5Cfrac%7Bp%28x%2Cz%3B%5Ctheta%29%7D%7Bq%28z%29%7D+%5Cright%29+%5Cright%29++%5C%5C++%3D+%26+%5Cint+dz+%5C+q%28z%29+%5Clog%5Cfrac%7Bp%28x%2Cz%3B%5Ctheta%29%7D%7Bq%28z%29%7D+%5C%5C++++%5Cend%7Bsplit%7D+%5Cend%7Bequation%7D" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>其中的不等号来自于Jensen's inequality应用在凹函数 <img src="https://www.zhihu.com/equation?tex=%5Clog%28%5Ccdot%29" alt="[公式]" /> 上的结论。由此我们知道，<strong>对于上面的式子取等号的条件是 <img src="https://www.zhihu.com/equation?tex=p%28x%2Cz%3B%5Ctheta%29+%5Cpropto+q%28z%29" alt="[公式]" /> （也就是说在给定 <img src="https://www.zhihu.com/equation?tex=x" alt="[公式]" /> 的情况下二者的比例是一个常数，无论 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]" /> 取什么值），所以取等号的时候 <img src="https://www.zhihu.com/equation?tex=q%28z%29+%3D+%5Cfrac%7Bp%28x%2Cz%3B%5Ctheta%29%7D%7B%5Cint+dz%5C+p%28x%2Cz%3B%5Ctheta%29%7D%3Dp%28z%7Cx%3B%5Ctheta%29" alt="[公式]" /> （这是由于 <img src="https://www.zhihu.com/equation?tex=q%28z%29" alt="[公式]" /> 是概率密度，因此满足归一化条件）。</strong>由此我们可以定义KL divergence（记为 <img src="https://www.zhihu.com/equation?tex=J_%7BKL%7D%28x%3B%5Ctheta%29" alt="[公式]" /> ）和Evidence Lower BOund（简称为ELBO，记为 <img src="https://www.zhihu.com/equation?tex=L%28x%3B%5Ctheta%29" alt="[公式]" /> ）：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=J_%7BKL%7D%28x%3B%5Ctheta%2Cq%29%3D%5Clog+p%28x%3B%5Ctheta%29+-+%5Cint+dz+%5C+q%28z%29+%5Clog%5Cfrac%7Bp%28x%2Cz%3B%5Ctheta%29%7D%7Bq%28z%29%7D+%5Cgeq+0" alt="" /><figcaption>[公式]</figcaption>
</figure>
<figure>
<img src="https://www.zhihu.com/equation?tex=L%28x%3B%5Ctheta%2Cq%29+%3D+%5Cint+dz+%5C+q%28z%29+%5Clog%5Cfrac%7Bp%28x%2Cz%3B%5Ctheta%29%7D%7Bq%28z%29%7D+" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>由此可见KL divergence一定非负，而ELBO是 <img src="https://www.zhihu.com/equation?tex=%5Clog+p%28x%3B%5Ctheta%29" alt="[公式]" /> 的下界。<strong>KL divergence最小的时候等于0，此时 <img src="https://www.zhihu.com/equation?tex=q%28z%29%3Dp%28z%7Cx%3B%5Ctheta%29" alt="[公式]" /> ，这是根据Jensen不等式取等号的条件得出的。并且KL divergence越小， <img src="https://www.zhihu.com/equation?tex=q%28z%29" alt="[公式]" /> 越接近 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx%3B%5Ctheta%29" alt="[公式]" /> 。减小KL divergence等价于增大ELBO，因为我们要优化的目标函数是 <img src="https://www.zhihu.com/equation?tex=q%28z%29" alt="[公式]" /> ，而 <img src="https://www.zhihu.com/equation?tex=%5Clog+P%28x%3B%5Ctheta%29" alt="[公式]" /> 可以看作给定的量。</strong></p>
<p><strong>由此我们得到一个重要的结论：优化 <img src="https://www.zhihu.com/equation?tex=q%28z%29" alt="[公式]" /> 来增大ELBO等价于优化 <img src="https://www.zhihu.com/equation?tex=q%28z%29" alt="[公式]" /> 来逼近 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx%3B%5Ctheta%29" alt="[公式]" /> 。</strong></p>
<p><strong>以下我们通过极大化ELBO，从而训练得到一个好的函数 <img src="https://www.zhihu.com/equation?tex=q%28z%29" alt="[公式]" /> 来近似表达 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx%3B%5Ctheta%29" alt="[公式]" /> 。</strong>我们可以将ELBO的形式等价变换一下，得到另一种等价形式：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Bsplit%7D+L%28x%3B%5Ctheta%2Cq%29+%3D%26+%5Cint+dz+%5C+q%28z%29+%5Clog%5Cfrac%7Bp%28x%2Cz%3B%5Ctheta%29%7D%7Bq%28z%29%7D+%5C%5C+%3D%26+%5Cint+dz%5C+q%28z%29+%5Clog+p%28x%2Cz%3B%5Ctheta%29+-+%5Cint+dz%5C+q%28z%29%5Clog+q%28z%29+%5C%5C+%3D%26+%5Cmathbb%7BE%7D_q%5B%5Clog+p%28x%2Cz%3B%5Ctheta%29%5D+%2B+S%28q%28z%29%29+%5Cend%7Bsplit%7D+%5Cend%7Bequation%7D" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>其中 <img src="https://www.zhihu.com/equation?tex=S%28q%28z%29%29+%3D+-+%5Cint+dz%5C+q%28z%29%5Clog+q%28z%29" alt="[公式]" /> 是著名的Gibbs entropy。</p>
<h4 id="variational-inference">Variational Inference</h4>
<p>在实际操作中，首先我们有已经观察到的 <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]" /> 个数据点 <img src="https://www.zhihu.com/equation?tex=x_i%5C+%28i%3D1%2C%5Ccdots%2CN%29" alt="[公式]" /> ，我们需要通过优化 <img src="https://www.zhihu.com/equation?tex=q_i" alt="[公式]" /> 来极大化ELBO，即对于每个数据点 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]" /> ，我们希望得到函数 <img src="https://www.zhihu.com/equation?tex=q_i%28z%29+%5Capprox+p%28z%7Cx_i%3B%5Ctheta%29" alt="[公式]" /> 。每个 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]" /> 对应的ELBO是</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=L%28x_i%3B%5Ctheta%2Cq_i%29%3D%5Cint+dz%5C+q_i%28z%29+%5Clog+p%28x_i%2Cz%3B%5Ctheta%29+-+%5Cint+dz%5C+q_i%28z%29%5Clog+q_i%28z%29" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>我们的目标是极大化 <img src="https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5EN+L%28x_i%3B%5Ctheta%2Cq_i%29" alt="[公式]" /> ：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+%5Cbegin%7Bsplit%7D+%5Cmax+%5Csum_%7Bi%3D1%7D%5EN+L%28x_i%3B%5Ctheta%2Cq_i%29+%3D%26++%5Cmax+%5Csum_%7Bi%3D1%7D%5EN+%5Cleft%5B%5Cint+dz%5C+q_i%28z%29+%5Clog+p%28x_i%2Cz%3B%5Ctheta%29+-+%5Cint+dz%5C+q_i%28z%29%5Clog+q_i%28z%29%5Cright%5D+%5C%5C++%3D%26+%5Cmax+%5Csum_%7Bi%3D1%7D%5EN+%5Cleft%5C%7B+%5Cmathbb%7BE%7D_%7Bq_i%7D%5B%5Clog+p%28x_i%2Cz%3B%5Ctheta%29%5D+%2B+S%28q_i%28z%29%29+%5Cright%5C%7D+%5Cend%7Bsplit%7D+%5Cend%7Bequation%7D" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>注意到对于不同的 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]" /> ，我们选取的函数形式 <img src="https://www.zhihu.com/equation?tex=q_i%28z%29" alt="[公式]" /> 不同，因为 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx_i%3B%5Ctheta%29" alt="[公式]" /> 在 <img src="https://www.zhihu.com/equation?tex=x_i" alt="[公式]" /> 不同的时候是不同的关于 <img src="https://www.zhihu.com/equation?tex=z" alt="[公式]" /> 的函数。一般来说，通过选取合适的、简单的 <img src="https://www.zhihu.com/equation?tex=q_i%28z%29" alt="[公式]" /> 的形式，例如exponential family，<img src="https://www.zhihu.com/equation?tex=S%28q_i%28z%29%29" alt="[公式]" /> 可以解析地计算出来；对于 <img src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%7Bq_i%7D%5B%5Clog+p%28x_i%2Cz%3B%5Ctheta%29%5D" alt="[公式]" /> 则需要用stochastic optimization进行极大化，从而优化函数 <img src="https://www.zhihu.com/equation?tex=q_i" alt="[公式]" /> （也就是优化函数 <img src="https://www.zhihu.com/equation?tex=q_i" alt="[公式]" /> 的表达式里面包含的参数）。总结一下，<strong>VI就是通过选取一些形式简单的、具有良好性质的函数 <img src="https://www.zhihu.com/equation?tex=q%28z%29" alt="[公式]" /> 来近似表达 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx%3B%5Ctheta%29" alt="[公式]" /> ，具体的操作方法是极大化ELBO。</strong></p>
<h4 id="expectation-maximization">Expectation Maximization</h4>
<p>EM实际上可以看作一个简化的VI（通过一定的适用条件和一个类似于“作弊”的方法来简化计算）。<strong>EM的适用条件是函数 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx_i%3B%5Ctheta%29" alt="[公式]" /> 的形式不太复杂，我们可以显式地表达出 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx_i%3B%5Ctheta%29" alt="[公式]" /> 的时候。从适用条件上来看EM就是VI的一个特例。</strong>回忆第1节Motivation中我们说过，有时候 <img src="https://www.zhihu.com/equation?tex=%5Cint+dz%5C+p%28x%2Cz%3B%5Ctheta%29" alt="[公式]" /> 难以计算，所以难以直接写出 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx%2C%5Ctheta%29" alt="[公式]" /> 的具体形式。<strong>而EM就是适用于 <img src="https://www.zhihu.com/equation?tex=%5Cint+dz%5C+p%28x%2Cz%3B%5Ctheta%29" alt="[公式]" /> 不难计算的情况，此时我们可以直接写出 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx%2C%5Ctheta%29" alt="[公式]" /> 的具体形式。</strong></p>
<p><strong>当函数 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx_i%2C%5Ctheta%29" alt="[公式]" /> 的形式不太复杂的时候，我们自然会选择 <img src="https://www.zhihu.com/equation?tex=q_i%28z%29+%3D+p%28z%7Cx_i%3B%5Ctheta%29" alt="[公式]" /> 来训练，因此我们可以说，EM就是选取<img src="https://www.zhihu.com/equation?tex=q_i%28z%29+%3D+p%28z%7Cx_i%3B%5Ctheta%29" alt="[公式]" />的VI，这和EM的适用条件是自洽的。</strong>以下首先写出EM的算法，然后指出其中关键的“作弊”一步在哪里。</p>
<p><strong><em>EM Algorithm</em></strong></p>
<p><strong><em>BEGIN</em></strong></p>
<p><em>我们有 <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]" /> 个数据点 <img src="https://www.zhihu.com/equation?tex=x_i%5C+%28i%3D1%2C%5Ccdots%2CN%29" alt="[公式]" /> ，并且我们有函数 <img src="https://www.zhihu.com/equation?tex=p%28x%2Cz%3B%5Ctheta%29" alt="[公式]" /> 和 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx%3B%5Ctheta%29" alt="[公式]" /> 的表达式。</em></p>
<p><em>Step 0，初始化：初始随机猜测参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_0" alt="[公式]" /></em></p>
<p><em>Step 1, E-step：在第 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]" /> 步的时候 <img src="https://www.zhihu.com/equation?tex=%28t%3D0%2C1%2C%5Ccdots%2CT%29" alt="[公式]" /> ，计算 <img src="https://www.zhihu.com/equation?tex=p%28z%7Cx_i%2C%5Ctheta_t%29" alt="[公式]" /></em></p>
<p><em>Step2, M-step: 按照如下规则得到新的参数 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bt%2B1%7D" alt="[公式]" /> ：</em></p>
<p><em><img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bt%2B1%7D+%3D+%7B%5Crm+argmax%7D_%7B%5Ctheta%7D%5C++%5Csum_%7Bi%3D1%7D%5EN+%5Cint+dz%5C+p%28z%7Cx_i%3B%5Ctheta_t%29+%5Clog+p%28x_i%2Cz%3B%5Ctheta%29" alt="[公式]" /></em></p>
<p><em>Step3, 重复 Step 1 &amp; 2（迭代）直到收敛。</em></p>
<p><strong><em>END</em></strong></p>
<p>其中关键的“作弊”步骤是Step2，因为*<strong>严格来说我们应该这样计算*</strong> <img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bt%2B1%7D" alt="[公式]" /> ：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bt%2B1%7D+%3D+%7B%5Crm+argmax%7D_%7B%5Ctheta%7D%5C++%5Csum_%7Bi%3D1%7D%5EN+%5Cleft%5B+%5Cint+dz%5C+p%28z%7Cx_i%3B%5Ctheta%29+%5Clog+p%28x_i%2Cz%3B%5Ctheta%29++-+%5Cint+dz%5C+p%28z%7Cx_i%3B%5Ctheta%29%5Clog+p%28z%7Cx_i%3B%5Ctheta%29+%5Cright%5D" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>也就是将 <img src="https://www.zhihu.com/equation?tex=q_i%28z%29" alt="[公式]" /> 替换成 <img src="https://www.zhihu.com/equation?tex=+p%28z%7Cx_i%3B%5Ctheta%29" alt="[公式]" /> 后maximize ELBO。但是*<strong>实际上这个算法却是这么做的*</strong>：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bt%2B1%7D+%3D+%7B%5Crm+argmax%7D_%7B%5Ctheta%7D%5C++%5Csum_%7Bi%3D1%7D%5EN+%5Cleft%5B+%5Cint+dz%5C+p%28z%7Cx_i%3B%5Ctheta_t%29+%5Clog+p%28x_i%2Cz%3B%5Ctheta%29++-+%5Cint+dz%5C+p%28z%7Cx_i%3B%5Ctheta_t%29%5Clog+p%28z%7Cx_i%3B%5Ctheta_t%29+%5Cright%5D" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>也就是<strong>将ELBO表达式中的某几个 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]" /> 固定在了 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_t" alt="[公式]" /> 的值（其中 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_t" alt="[公式]" /> 是第 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]" /> 次优化得到的参数），只优化其余位置的 <img src="https://www.zhihu.com/equation?tex=%5Ctheta" alt="[公式]" /> 值，从而使得极大化算法更加简便</strong>。<strong>同时通过迭代方法确保最后能收敛到一个local maximum。</strong>由于 <img src="https://www.zhihu.com/equation?tex=%5Ctheta_t" alt="[公式]" /> 相当于常数，因此 <img src="https://www.zhihu.com/equation?tex=+-+%5Cint+dz%5C+p%28z%7Cx_i%3B%5Ctheta_t%29%5Clog+p%28z%7Cx_i%3B%5Ctheta_t%29" alt="[公式]" /> 相当于常数项，在极大化的时候可以去掉，由此我们可以得到</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bt%2B1%7D+%3D+%7B%5Crm+argmax%7D_%7B%5Ctheta%7D%5C++%5Csum_%7Bi%3D1%7D%5EN+%5Cint+dz%5C+p%28z%7Cx_i%3B%5Ctheta_t%29+%5Clog+p%28x_i%2Cz%3B%5Ctheta%29" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>并且<strong>通过迭代求解找到 local maximum。</strong></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dsttsd.github.io/2022/01/16/week%207/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/50662067?s=400&u=b552d8b742d1e685ed0ddcc6a97d9f697535fa6b&v=4">
      <meta itemprop="name" content="DST">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DSTの杂货铺">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/16/week%207/" class="post-title-link" itemprop="url">第七次周报</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-16 15:34:55" itemprop="dateCreated datePublished" datetime="2022-01-16T15:34:55+08:00">2022-01-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-06-28 11:50:48" itemprop="dateModified" datetime="2022-06-28T11:50:48+08:00">2022-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%91%A8%E6%8A%A5/" itemprop="url" rel="index"><span itemprop="name">周报</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>week 7 内容：</p>
<ul>
<li>louis philippe morency 《多模态机器学习》 50%</li>
<li>论文阅读：
<ol type="1">
<li>Collaborative Denoising Auto-Encoders for Top-N Recommender Systems</li>
<li>Semantic Image Synthesis with Spatially-Adaptive Normalization</li>
<li>DAGAN: Dual Attention GANs for Semantic Image Synthesis</li>
</ol></li>
</ul>
<h4 id="collaborative-denoising-auto-encoders-for-top-n-recommender-systemswsdm-2016">Collaborative Denoising Auto-Encoders for Top-N Recommender Systems(WSDM 2016)</h4>
<h5 id="主要工作">主要工作</h5>
<p>top-N推荐任务具有较多应用场景。该文作者基于降噪自编码器（Denoising Autoencoder）提出了系统过滤推荐模型CDAE，用于完成基于用户偏好的top-N推荐任务。CDAE模型在特定情况下可对多种经典协同过滤模型做泛化，具有较强的可解释性。 ##### 模型</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-ce4584251df115e579a57291001f85f7_720w.jpg" alt="" /><figcaption>img</figcaption>
</figure>
<p>如上图所示，CDAE模型以用户-物品评价矩阵的行作为输入（ <img src="https://www.zhihu.com/equation?tex=I-autorec" alt="[公式]" /> 式模型），通过一层神经网络编码得到用户的隐藏表示，再通过一层神经网络还原用户的交互行为（隐式反馈）。与最简单的 <img src="https://www.zhihu.com/equation?tex=I-autorec" alt="[公式]" /> 模型不同，CDAE模型在编码得到隐藏表示时加入了对用户特征的考量，语义更丰富。为了使模型更具鲁棒性，CDAE模型对输入特征做了噪声处理（通过dropout或者加入高斯噪声实现)。</p>
<p>代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CDAE</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, nb_item, nb_user, nb_hidden, drop_rate=<span class="number">0.3</span></span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        :param nb_item: 项目的数量</span></span><br><span class="line"><span class="string">        https://github.com/stormDL/CDAE_with_Pytorch</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        <span class="built_in">super</span>(CDAE, self).__init__()</span><br><span class="line">        self.item2hidden = nn.Sequential(</span><br><span class="line">            nn.Linear(nb_item, nb_hidden),</span><br><span class="line">            nn.Dropout(drop_rate)</span><br><span class="line">        )</span><br><span class="line">        self.id2hidden = nn.Embedding(nb_user, nb_hidden)</span><br><span class="line">        self.hidden2out = nn.Linear(nb_hidden, nb_item)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, uid, purchase_vec</span>):</span></span><br><span class="line">        <span class="comment"># 加入了用户特征</span></span><br><span class="line">        hidden = self.sigmoid(self.id2hidden(uid).squeeze(dim=<span class="number">1</span>)+self.item2hidden(purchase_vec))</span><br><span class="line">        out = self.hidden2out(hidden)</span><br><span class="line">        out = self.sigmoid(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://img-blog.csdn.net/20180726234836423?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Rhb3lhZmFu/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="" /><figcaption>img</figcaption>
</figure>
<h4 id="semantic-image-synthesis-with-spatially-adaptive-normalizationcvpr-2019-oral">Semantic Image Synthesis with Spatially-Adaptive Normalization（CVPR 2019 Oral）</h4>
<p>这篇文章提出了新的归一化层，从而让生成效果更好。由于相同语义像素的值趋同，分割图经过传统normalization往往会变为0（(x-u)/v），因此输入不同的语义信息，BN等传统normalization会“wash away”语义信息。这篇文章通过提出SPADE结构来保留语义信息。</p>
<p><img src="https://s2.loli.net/2022/01/13/7DC4Uu1H2bqAxjX.png" /></p>
<h5 id="network">Network</h5>
<p><img src="https://s2.loli.net/2022/01/13/TV71QYAaBd9k8rW.png" /></p>
<p>GauGan整体结构使用了上图所示SPADE ResBlk代替pix2pixHD中的resnetblock结构（SPADE替换了InstanceNorm）</p>
<h5 id="structure">Structure<img src="https://s2.loli.net/2022/01/13/DaCtQmcen6BASpy.png" /></h5>
<figure>
<img src="C:\Users\shtduan\AppData\Roaming\Typora\typora-user-images\image-20220113192316739.png" alt="" /><figcaption>image-20220113192316739</figcaption>
</figure>
<p><img src="https://s2.loli.net/2022/01/13/tcl6qGkhL9NyZuo.png" /></p>
<p>$<span class="math inline">\(和\)</span> $代表着feature map上每个点的斜率和偏移量， 他们是通过两个卷积操作得到的；除此之外，剩下的就是按照每个通道做normalization。</p>
<h4 id="dagan-dual-attention-gans-for-semantic-image-synthesisacm-mm-2020">DAGAN: Dual Attention GANs for Semantic Image Synthesis（ACM MM 2020）</h4>
<h5 id="intro">Intro</h5>
<p>这篇文章关心的问题在于通过语义分割图生成真实图像（phot-realistic images），该任务是conditional gan的一个延伸。现在相关的迁移工作都没有利用足够的语义信息限制生成过程，同时忽略了空间和通道的结构相关性（structure correlation），导致生成的照片大多缺乏细节同时生成痕迹明显（涂抹等）。</p>
<h5 id="network-1">Network</h5>
<p>这篇文章backbone network使用的是和GauGan相同的特征提取结构， 即backbone B。在Gaugan基础上引入空间注意力（SAM结构）和通道注意力（CAM结构）。</p>
<p><img src="https://s2.loli.net/2022/01/12/yzl9QBcn2gCHrJF.png" /></p>
<p>SAM将B的输出特征作为输入，输出空间相关特征图（语义模块在空间上的联系）；CAM将B的每层特征作为输入，输出特征为通道相关特征图。通过元素加整合后输入卷积，得到fake image；对判别器D，它需要输入真实pair和假的pair，实现过程中将photo和语义分割图concat即可。</p>
<p>SAM：</p>
<p><img src="https://s2.loli.net/2022/01/12/OiKU4WlEIpHvuSs.png" /></p>
<p>backbone输出语义特征图首先分别进行max和avg pooling，将结果concate后输入卷积层和Sigmoid层，得到As是attention的weight，将As与Fb进行元素积，得到的Fs中相同label的pixel就有了联系（mutual gain）， 这样生成的图片就可以有着语义上的持续性（一个object内的pixel应该是同样类别）</p>
<p>CAM：</p>
<p><img src="https://s2.loli.net/2022/01/12/6Dc1P43uKl2MvEk.png" /></p>
<p>对于同样的objects，不同尺度的特征应该有联系。因此CAM将backbone中不同特征图做channel wised attention，有利于捕获这样的信息。实现中作者只用了上采样过程中的一个特征图(倒数第二个加conv)，和输出做channel-wised concatnation（所以是2C x H x W）。Fb通过AdaptiveAvgPool再经过两个卷积层得到Cx1x1的</p>
<p>channel weights。最后的输出Fc通过两个部分元素加得到，即赋权后的<span class="math inline">\(F_b^l\)</span>以及<span class="math inline">\(F_b^{l-1}\)</span></p>
<p>loss：</p>
<ol type="1">
<li><p>实现角度，gen loss</p>
<p>判别器：</p>
<p><img src="https://s2.loli.net/2022/01/12/weLNyujH3EPfAQW.png" /></p>
<p>使用hinge loss衡量判别器效果，这样限制判别器去找最优的超平面。</p>
<p>生成器：</p>
<p><img src="https://s2.loli.net/2022/01/12/UhCwxY6m2PXRptk.png" /></p>
<p><span class="math inline">\(L_{FM}\)</span>代表discriminator feature matching loss，它讲判别器的每层特征图抽出，确保real和fake特征图相近，借此优化G(discriminator部分使用detach()，确保更新G时D不动)</p>
<p><span class="math inline">\(L_P\)</span>为perceptual loss，它使用一个pretrain 的VGG进行特征提取，同样，确保fake和real的特征相近。（感觉这个是站在自然图像角度，确保两个pair）</p></li>
<li><p>整体看，实际上loss是分为了conditional adversarial loss，discriminator feature matching loss，perceptual loss</p>
<p><img src="https://s2.loli.net/2022/01/12/fLsCmak2b7FOIiR.png" /></p></li>
</ol>
<h4 id="variational-autoencoders-for-collaborative-filteringwww-2018">Variational Autoencoders for Collaborative Filtering（WWW 2018）</h4>
<h5 id="主要工作-1">主要工作</h5>
<p>该文作者把变分自编码器拓展应用于基于隐式反馈的协同过滤推荐任务，希望通过非线性概率模型克服线性因子模型的局限。该文提出了基于变分自编码器（Variational Autoencoder）的生成模型VAE_CF，并针对变分自编码器的正则参数和概率模型选取做了适当调整（使用了多项式分布），使其在当时推荐任务中取得SOTA结果。</p>
<h5 id="模型">模型</h5>
<p><img src="https://s2.loli.net/2022/01/15/YlUGjvMeOuE2aZT.png" /></p>
<p>如上图所示，虚线代表了采样操作，a是传统AE，b是denoising AE， c代表论文提出的vae。该模型根据标准高斯分布抽取K维隐藏因子 <img src="https://www.zhihu.com/equation?tex=z_%7Bu%7D+%5Csim+%5Cmathcal%7BN%7D%280%2CI_%7BK%7D%29" alt="[公式]" /> ，然后根据非线性函数 <img src="https://www.zhihu.com/equation?tex=f_%7B%5Ctheta%7D%28%5Ccdot%29" alt="[公式]" /> 生成用户 <img src="https://www.zhihu.com/equation?tex=u" alt="[公式]" /> 点击所有物品的概率分布 <img src="https://www.zhihu.com/equation?tex=%5Cpi%EF%BC%88z_%7Bu%7D%EF%BC%89%5Cpropto+exp%28f_%7B%5Ctheta%7D%28z_%7Bu%7D%29%29" alt="[公式]" /> ，最后根据多项式分布 <img src="https://www.zhihu.com/equation?tex=x_%7Bu%7D+%5Csim+Mult%28N_%7Bu%7D%2C%5Cpi%28z_%7Bu%7D%29%29" alt="[公式]" /> 重构用户点击历史（隐式反馈的用户-物品评价矩阵对应行)。</p>
<p>其目标函数（ELBO）：</p>
<figure>
<img src="https://upload-images.jianshu.io/upload_images/8615414-520fdae67f14c914.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/708/format/webp" alt="" /><figcaption>img</figcaption>
</figure>
<p>VAE_CF模型较标准的变分自编码器做了如下调整：</p>
<p>1、将正则参数调至0.2（低于常规值1），称其为部分正则化（partially regularized)</p>
<p>2、使用了多项式分布而非高斯分布。</p>
<ul>
<li><strong>多项式似然非常适合于隐式反馈数据的建模，并且更接近 rank loss；</strong></li>
<li><strong>无论数据的稀缺性如何，采用principled Bayesian方法都更加稳健。</strong></li>
</ul>
<p>使用SGD进行优化：</p>
<p><img src="https://s2.loli.net/2022/01/15/ySYtBvW6TKdQoZE.png" /></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dsttsd.github.io/2022/01/09/week%206/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/50662067?s=400&u=b552d8b742d1e685ed0ddcc6a97d9f697535fa6b&v=4">
      <meta itemprop="name" content="DST">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DSTの杂货铺">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/09/week%206/" class="post-title-link" itemprop="url">第六次周报</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-09 14:07:08" itemprop="dateCreated datePublished" datetime="2022-01-09T14:07:08+08:00">2022-01-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-06-28 12:08:03" itemprop="dateModified" datetime="2022-06-28T12:08:03+08:00">2022-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%91%A8%E6%8A%A5/" itemprop="url" rel="index"><span itemprop="name">周报</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>week 6 内容：</p>
<ul>
<li><p>louis philippe morency 《多模态机器学习》 35%</p></li>
<li><p>论文阅读：</p>
<ol type="1">
<li>DeepCoclustering</li>
<li>AutoRec: Autoencoders Meet Collaborative Filtering</li>
<li>AugGAN：Cross Domain Adaptation with GAN based DataAugmentation</li>
</ol></li>
</ul>
<h3 id="deepcoclusteringsdm-2019">DeepCoclustering（SDM 2019）</h3>
<h4 id="主要内容">主要内容</h4>
<figure>
<img src="https://s2.loli.net/2021/12/29/IVcB2K56R9LFMeG.png" alt="" /><figcaption>framework</figcaption>
</figure>
<p>这篇文章提出了一种深度学习框架下的共聚类（co-clustering）方法，在聚类精度上超过了其他传统方法。</p>
<p>其中X代表instance（行，对应user)， Y代表features(列，对应item)， 分别进入Autoencoders进行降维（先进行pretrain 1000 epoch，然后利用encoder接入后面network进行端到端训练），将结果输入inference network（一个multi-layer neural network), 利用GMM框架进行cluster assignment。</p>
<h4 id="优化目标">优化目标</h4>
<h5 id="cluster-assignment">cluster assignment</h5>
<p>参数和概率表示：</p>
<p>inference network：<span class="math inline">\(\eta_{r}\)</span>、<span class="math inline">\(\eta_{c}\)</span> ——》 <span class="math inline">\(Q_{\eta_{r}}(k|h_i)\)</span> <span class="math inline">\(Q_{\eta_{c}}(k|v_j)\)</span> (cluster assignment distributions)</p>
<p>GMM：<span class="math inline">\(\phi_{r}\)</span>、<span class="math inline">\(\phi_{c}\)</span> ——》<span class="math inline">\(P_{\phi_{r}}(k|h_i)\)</span> <span class="math inline">\(P_{\phi_{c}}(k|v_j)\)</span> (cluster assignment posterior)</p>
<p>对于AE输出的隐变量<span class="math inline">\(z_i\)</span>, 进入inference network中的MLN，得到<span class="math inline">\(h_i\)</span>:</p>
<p>​ <span class="math inline">\(\mathbf{h}_{i}=\operatorname{Softmax}\left(M L N\left(\mathbf{z}_{i} ; \eta_{r}\right)\right)\)</span></p>
<p>而后GMM第k个参数：<span class="math inline">\(\phi_{r}=\left\{\pi_{r}^{k}, \mu_{r}^{k}, \Sigma_{r}^{k}\right\}\)</span>,可以被表示为：</p>
<p>​ <span class="math inline">\(\begin{array}{l} \pi_{r}^{k}=N_{r}^{k} / N_{r}, \quad \mu_{r}^{k}=\frac{1}{N_{r}^{k}} \sum_{i=1}^{N_{r}^{k}} h_{i k} \mathbf{h}_{i} \\ \Sigma_{r}^{k}=\frac{1}{N_{r}^{k}} \sum_{i=1}^{N_{r}} h_{i k}\left(\mathbf{h}_{i}-\mu_{r}^{k}\right)\left(\mathbf{h}_{i}-\mu_{r}^{k}\right)^{T} \end{array}\)</span></p>
<p>其中<span class="math inline">\(N_{r}\)</span>是instance的数目（即user），<span class="math inline">\(N_{r}^{k}\)</span>是所有instance对应 <span class="math inline">\(h_i\)</span>的 k-dim的和，这样第i个instance分配给第k个cluster的概率可以表示为：</p>
<p>​ <span class="math inline">\(\gamma_{r(i)}^{k}=\frac{\pi_{r}^{k} \mathcal{N}\left(\mathbf{h}_{i} \mid \mu_{r}^{k}, \Sigma_{r}^{k}\right)}{\sum_{k^{\prime}=1}^{g} \pi_{r}^{k^{\prime}} \mathcal{N}\left(\mathbf{h}_{i} \mid \mu_{r}^{k^{\prime}}, \Sigma_{r}^{k^{\prime}}\right)}\)</span></p>
<p>似然函数（取<span class="math inline">\(\gamma_{r(i)}^{k}\)</span>上边求和）：</p>
<p>​ <span class="math inline">\(\log \left\{\prod_{i=1}^{N_{r}} P_{\phi_{r}}\left(\mathbf{h}_{i}\right)\right\}=\sum_{i=1}^{N_{r}} \log P_{\phi_{r}}\left(\mathbf{h}_{i}\right)=\sum_{i=1}^{N_{r}} \log \left\{\sum_{k=1}^{K} \pi_{r}^{k} \mathcal{N}\left(\mathbf{h}_{i} \mid \mu_{r}^{k}, \Sigma_{r}^{k}\right)\right\}\)</span></p>
<p>DeepCC并没有直接进行MLE，而是最大化变分lower bound，有两点优势：</p>
<ol type="1">
<li><p>通过最小化Q P之间的KL散度，让GMM更准确</p></li>
<li><p>与对数似然绑定，让训练过程更加effective</p>
<p>lower bound:</p></li>
</ol>
<p>​ <span class="math inline">\(\begin{array}{l} \sum_{i=1}^{N_{r}} \log P\left(\mathbf{h}_{i}\right)=\sum_{i=1}^{N_{r}} \log \int_{k} P\left(k, \mathbf{h}_{i}\right) \\ =\sum_{i=1}^{N_{r}} \log \int_{k} \frac{P\left(k, \mathbf{h}_{i}\right)}{Q\left(k \mid \mathbf{h}_{i}\right)} Q\left(k \mid \mathbf{h}_{i}\right) \\ =\sum_{i=1}^{N_{r}} \log \left(E_{Q}\left[\frac{P\left(k, \mathbf{h}_{i}\right)}{Q\left(k \mid \mathbf{h}_{i}\right)}\right]\right) \\ \geq \sum_{i=1}^{N_{r}} E_{Q}\left[\log \frac{P\left(k, \mathbf{h}_{i}\right)}{Q\left(k \mid \mathbf{h}_{i}\right)}\right] \\ =\sum_{i=1}^{N_{r}}\left\{E_{Q}\left[\log P\left(k, \mathbf{h}_{i}\right)\right]+H\left(k \mid \mathbf{h}_{i}\right)\right\} \\ =\mathcal{L}_{r} \end{array}\)</span></p>
<p>中间把log拿入期望使用了jensen ‘s inequality</p>
<p>更进一步，可以得到Q与P之间KL散度与lower bound关系(第二步减了一个<span class="math inline">\(logP(h_i)\)</span>，又加了一个)：</p>
<p><span class="math inline">\(\begin{array}{l} \mathcal{L}_{r}=\sum_{i=1}^{N_{r}}\left\{E_{Q}\left[\log P\left(k, \mathbf{h}_{i}\right)\right]-E_{Q}\left(\log Q\left(k \mid \mathbf{h}_{i}\right)\right)\right\} \\ =\sum_{i=1}^{N_{r}}\left\{\int_{k} Q\left(k \mid \mathbf{h}_{i}\right) \log \frac{P\left(k, \mathbf{h}_{i}\right)}{Q\left(k \mid \mathbf{h}_{i}\right)}-\right. \left.\int_{k} Q\left(k \mid \mathbf{h}_{i}\right) \log P\left(\mathbf{h}_{i}\right)+\log P\left(\mathbf{h}_{i}\right)\right\} \\ =\sum_{i=1}^{N_{r}}\left\{\int_{k} Q\left(k \mid \mathbf{h}_{i}\right) \log \frac{P\left(k, \mathbf{h}_{i}\right)}{Q\left(k \mid \mathbf{h}_{i}\right) P\left(\mathbf{h}_{i}\right)}+\log P\left(\mathbf{h}_{i}\right)\right\} \\ =\sum_{i=1}^{N_{r}}\left\{\int_{k} Q\left(k \mid \mathbf{h}_{i}\right) \log \frac{P\left(k \mid \mathbf{h}_{i}\right)}{Q\left(k \mid \mathbf{h}_{i}\right)}+\log P\left(\mathbf{h}_{i}\right)\right\} \\ =\sum_{i=1}^{N_{r}}\left\{-K L\left(Q\left(k \mid \mathbf{h}_{i}\right) \| P\left(k \mid \mathbf{h}_{i}\right)\right)+\log P\left(\mathbf{h}_{i}\right)\right\} \end{array}\)</span></p>
<p>对应可以得到feature的assignment loss：</p>
<p><span class="math inline">\(\mathcal{L}_{c}=\sum_{j=1}^{N_{c}}\left\{E_{Q}\left[\log P\left(k, \mathbf{v}_{j}\right)\right]-E_{Q}\left(\log Q\left(k \mid \mathbf{v}_{j}\right)\right)\right\}\)</span></p>
<h5 id="instance-feature-cross-loss">Instance-Feature Cross Loss</h5>
<p>对联合概率<span class="math inline">\(P(X,Y)\)</span>设计loss进行优化</p>
<p>assignment到cluster的概率：</p>
<p>instance: <span class="math inline">\(\gamma_{r(i)}=\left(\gamma_{r(i)}^{1}, \cdots, \gamma_{r(i)}^{g}\right)^{T}\)</span></p>
<p>feature：<span class="math inline">\(\gamma_{c(i)}=\left(\gamma_{c(i)}^{1}, \cdots, \gamma_{c(i)}^{g}\right)^{T}\)</span></p>
<p>第i个instance和第j个instance的联合概率（划分前）表示为：</p>
<p><span class="math inline">\(p\left(\mathbf{x}_{i}, \mathbf{y}_{j}\right)=\mathcal{J}\left(\boldsymbol{\gamma}_{r(i)}, \boldsymbol{\gamma}_{c(j)}\right)\)</span></p>
<p>第s个instance cluster和第t个feature cluster的联合概率（划分后）为：</p>
<p><span class="math inline">\(p\left(\hat{\mathbf{x}}_{s}, \hat{\mathbf{y}}_{t}\right)=\sum\left\{p\left(\mathbf{x}_{i}, \mathbf{y}_{j}\right) \mid \mathbf{x}_{i} \in \hat{\mathbf{x}}_{s}, \mathbf{y}_{j} \in \hat{\mathbf{y}}_{t}\right\}\)</span></p>
<p>在论文中，作者使用点积来表示<span class="math inline">\(\mathcal{J}\)</span>,其实是可以尝试改变的。这样做的原因有两点：</p>
<ol type="1">
<li>大部分co-cluster， instance cluster以及feature cluster 数量是相等的</li>
<li>相近的instance 有着相似的 feature</li>
</ol>
<p>各自的互信息：</p>
<p><span class="math inline">\(I(X ; Y)=\sum_{\mathbf{x}_{i}} \sum_{\mathbf{y}_{j}} p\left(\mathbf{x}_{i}, \mathbf{y}_{j}\right) \log \frac{p\left(\mathbf{x}_{i}, \mathbf{y}_{j}\right)}{p\left(\mathbf{x}_{i}\right) p\left(\mathbf{y}_{j}\right)}\)</span></p>
<p><span class="math inline">\(I(\hat{X} ; \hat{Y})=\sum_{\hat{\mathbf{x}}_{s}} \sum_{\hat{\mathbf{y}}_{t}} p\left(\hat{\mathbf{x}}_{s}, \hat{\mathbf{y}}_{t}\right) \log \frac{p\left(\hat{\mathbf{x}}_{s}, \hat{\mathbf{y}}_{t}\right)}{p\left(\hat{\mathbf{x}}_{s}\right), p\left(\hat{\mathbf{y}}_{t}\right)}\)</span></p>
<p>互信息之间的差：<span class="math inline">\(I(X ; Y)-I(\hat{X} ; \hat{Y})\)</span></p>
<p><span class="math inline">\(\begin{array}{l} =\sum_{\hat{\mathbf{x}}_{s}} \sum_{\hat{\mathbf{y}}_{t}} \sum_{\mathbf{x}_{i} \in \hat{\mathbf{x}}_{s}} \sum_{\mathbf{y}_{j} \in \hat{\mathbf{y}}_{t}} p\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right) \log \frac{p\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)}{p\left(\mathbf{x}_{i}\right) p\left(\mathbf{x}_{j}\right)} \\ -\sum_{\hat{\mathbf{x}}_{s}} \sum_{\hat{\mathbf{y}}_{t}}\left(\sum_{\mathbf{x}_{i} \in \hat{\mathbf{x}}_{s}} \sum_{\mathbf{y}_{j} \in \hat{\mathbf{y}}_{t}} p\left(\mathbf{x}_{i}, \mathbf{y}_{j}\right)\right) \log \frac{p\left(\hat{\mathbf{x}}_{s}, \hat{\mathbf{y}}_{t}\right)}{p\left(\hat{\mathbf{x}}_{s}\right) p\left(\hat{\mathbf{y}}_{t}\right)} \\ =\sum_{\hat{\mathbf{x}}_{s}} \sum_{\hat{\mathbf{y}}_{t}} \sum_{\mathbf{x}_{i} \in \hat{\mathbf{x}}_{s}} \sum_{\mathbf{y}_{j} \in \hat{\mathbf{y}}_{t}} p\left(\mathbf{x}_{i}, \mathbf{y}_{j}\right) \log \frac{p\left(\mathbf{x}_{i}, \mathbf{y}_{j}\right)}{q\left(\mathbf{x}_{i}, \mathbf{y}_{j}\right)} \\ =K L(p(X, Y) \| q(X, Y)) \\ \geq 0 \end{array}\)</span></p>
<p>因此为了让差最小化（划分后损失最少），设计loss为：</p>
<p>​ <span class="math inline">\(1-\frac{I(\hat{X} ; \hat{Y})}{I(X ; Y)}\)</span></p>
<h5 id="整体loss">整体loss</h5>
<p>综合上述loss：</p>
<p><span class="math inline">\(\begin{array}{c}  \quad \min _{\theta_{r}, \theta_{c}, \eta_{r}, \eta_{c}} J=J_{1}+J_{2}+J_{3} \\ J_{1}=\frac{\lambda_{1}}{n} \sum_{i=1}^{n} l\left(\mathbf{x}_{i}, g_{r}\left(\mathbf{z}_{i}\right)\right)+\lambda_{2} P_{a e}\left(\theta_{r}\right)+\lambda_{3}\left(-\mathcal{L}_{r}\right)+P_{i n f}\left(\Sigma_{r}\right) \\ J_{2}=\frac{\lambda_{1}}{d} \sum_{j=1}^{i} l\left(\mathbf{y}_{j}, g_{c}\left(\mathbf{w}_{j}\right)\right)+\lambda_{2} P_{a e}\left(\theta_{c}\right)+\lambda_{3}\left(-\mathcal{L}_{c}\right)+P_{\text {inf }}\left(\Sigma_{c}\right) \\ J_{3}=\lambda_{4}\left(1-\frac{I(\hat{X} ; \hat{Y})}{I(X ; X)}\right) \end{array}\)</span></p>
<h3 id="autorec-autoencoders-meet-collaborative-filteringwww-2015">AutoRec: Autoencoders Meet Collaborative Filtering（WWW 2015）</h3>
<p><strong>1、写作动机</strong></p>
<p>2015年前后，深度神经网络在视觉（vision）和对话（speech）数据建模方面取得了显著的突破，该文作者由此产生了把深度神经网络应用到协同过滤模型的想法。该文基于自编码器提出了协同过滤模型AutoRec，并通过实验证明了该模型较此前的基于神经网络的协同过滤模型具有更强的表示能力和更高的计算效率。</p>
<p><strong>2、问题定义</strong></p>
<p>给定用户集合 <img src="https://www.zhihu.com/equation?tex=U%3D%5C%7B1%2C...%2Cm+%5C%7D" alt="[公式]" /> 、物品集合<img src="https://www.zhihu.com/equation?tex=I%3D%5C%7B1%2C...%2Cn+%5C%7D" alt="[公式]" /> 、部分用户-物品评分记录构成的用户-物品评价矩阵<img src="https://www.zhihu.com/equation?tex=R+%5Cin+%5Cmathcal%7BR%7D%5E%7Bm+%5Ctimes+n%7D" alt="[公式]" />，对未知用户-物品评价行为做评分预测，根据均方根误差（RMSE)做评估。</p>
<p>其模型也可以写成重建函数形式：</p>
<p><img src="https://www.zhihu.com/equation?tex=h%5Cleft%28+%5Cmathbf%7Br%7D%3B%5Ctheta+%5Cright%29%3Df%5Cleft%28+%5Cmathbf%7BW%7D+%5Ccdot+g%5Cleft%28+%5Cmathbf%7BVr%7D%2B%5Cmu+%5Cright%29+%2Bb+%5Cright%29" alt="[公式]" /> (1)</p>
<p>其中 <img src="https://www.zhihu.com/equation?tex=f%5Cleft%28+%5Ccdot+%5Cright%29" alt="[公式]" /> 和 <img src="https://www.zhihu.com/equation?tex=g%5Cleft%28+%5Ccdot+%5Cright%29" alt="[公式]" /> 分别为输出层和隐藏层的激活函数，参数集 <img src="https://www.zhihu.com/equation?tex=%5Ctheta+%3D+%5Cleft%5C%7B+%5Cmathbf%7BW%7D%2C%5Cmathbf%7BV%7D%2C%5Cmathbf%7B%5Cmu%7D%2Cb+%5Cright%5C%7D" alt="[公式]" /> , <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BW%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bd%5Ctimes+k%7D" alt="[公式]" /> , <img src="https://www.zhihu.com/equation?tex=%5Cmathbf%7BV%7D+%5Cin+%5Cmathbb%7BR%7D%5E%7Bk%5Ctimes+d%7D" alt="[公式]" /> , <img src="https://www.zhihu.com/equation?tex=%5Cmu+%5Cin+%5Cmathbb%7BR%7D%5Ek" alt="[公式]" /> , <img src="https://www.zhihu.com/equation?tex=b+%5Cin+%5Cmathbb%7BR%7D%5Ed" alt="[公式]" /> . 对应 <img src="https://www.zhihu.com/equation?tex=m" alt="[公式]" /> 个用户和 <img src="https://www.zhihu.com/equation?tex=n" alt="[公式]" /> 个条目, <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]" /> 维隐藏层。</p>
<p>跟AutoEncoder类似地，其损失函数为</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cmathop%7Bmin%7D%5Climits_%7B%5Ctheta%7D+%5Csum_%7B%5Cmathbf%7Br%7D+%5Cin+%5Cmathbf%7BS%7D%7D%7B+%5ClVert+%5Cmathbb%7Br%7D-h%5Cleft%28+%5Cmathbf%7Br%7D%3B%5Ctheta+%5Cright%29+%5CrVert%7D_2%5E2" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>不过考虑到对模型参数的限制，比如加入L2正则，损失函数变化为：</p>
<figure>
<img src="https://www.zhihu.com/equation?tex=%5Cmathop%7Bmin%7D%5Climits_%7B%5Ctheta%7D+%5Csum_%7Bi%3D1%7D%5En%7B+%5ClVert+%5Cmathbb%7Br%7D%5E%7B%28i%29%7D-h%5Cleft%28+%5Cmathbf%7Br%7D%5E%7B%28i%29%7D%3B%5Ctheta+%5Cright%29%5CrVert%7D_%7B%5Cmathcal%7BO%7D%7D%5E2+%2B%5Cfrac%7B%5Clambda%7D%7B2%7D%5Ccdot+%5Cleft%28+%5ClVert+%5Cmathbf%7BW%7D%5CrVert_F%5E2+%2B%5ClVert+%5Cmathbf%7BV%7D%5CrVert_F%5E2++%5Cright%29" alt="" /><figcaption>[公式]</figcaption>
</figure>
<p>其中 <img src="https://www.zhihu.com/equation?tex=%5ClVert+%5Ccdot%5CrVert_F" alt="[公式]" /> 为Frobenius范数.</p>
<p><strong>3、具体模型</strong></p>
<figure>
<img src="https://pic3.zhimg.com/80/v2-f34b01b4a9b37014e3fe77e37cca5b3a_720w.jpg" alt="" /><figcaption>img</figcaption>
</figure>
<p>如上图所示，该文提出的AutoRec模型采取了最简单的自编码器结构，通过一层神经网络编码输入特征得到隐藏层表示，再通过一层神经网络表示对隐藏层表示做解码还原输入特征。根据输入特征的不同，该模型可分为 <img src="https://www.zhihu.com/equation?tex=I-autorec" alt="[公式]" /> 和 <img src="https://www.zhihu.com/equation?tex=U-autorec" alt="[公式]" /> ：输入特征为用户-物品评价矩阵的行，则为基于物品特征学习的自编码器 <img src="https://www.zhihu.com/equation?tex=I-autorec" alt="[公式]" /> ；输入特征为用户-物品评价矩阵的列，则为基于用户特征学习的自编码器 <img src="https://www.zhihu.com/equation?tex=U-autorec" alt="[公式]" /> 。</p>
<h3 id="cyclegan-iccv-2017">CycleGAN（ ICCV 2017）</h3>
<p>pix2pix使用了pair的data进行训练，然而现实中往往是unpaired-data，CycleGAN就是希望能够在目标域和源域之间，不用建立一对一的映射，就可以完成迁移的训练。</p>
<figure>
<img src="https://pic3.zhimg.com/80/v2-dde91dabff84db3086a4c8b761e46eb6_720w.jpg" alt="" /><figcaption>img</figcaption>
</figure>
<p>做到这一点：</p>
<ol type="1">
<li><p>需要有两个判别器， 分别判断X到Y与Y到X的生成，以及Y到X的生成。</p></li>
<li><p>cycle-consistency loss ，生成器可能会直接生成一张目标域的图像而无视源域图像，因此需要保证能够映射回X</p>
<figure>
<img src="https://pic4.zhimg.com/80/v2-f7691582129c9cdec1c3e454ebd80cf7_720w.jpg" alt="" /><figcaption>img</figcaption>
</figure></li>
</ol>
<h3 id="auggan基于gan的图像数据增强eccv-2018">AugGAN：基于GAN的图像数据增强（ECCV 2018）</h3>
<h4 id="主要内容-1">主要内容</h4>
<p>augGan利用语义分割的结构意识（structure-aware）进行数据域的迁移。与cycleGan等进行风格迁移的image2image translation不同，这篇文章利用结构意识强调生成照片的真实性而非艺术性。</p>
<p>AugGAN的训练数据包含：1. 分割mask；2，不同领域的图像(春天和冬天，白天和黑夜)。</p>
<figure>
<img src="https://img-blog.csdnimg.cn/20181101160051217.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xldmlvcGt1,size_16,color_FFFFFF,t_70" alt="" /><figcaption>img</figcaption>
</figure>
<p>主要结构：</p>
<figure>
<img src="https://img-blog.csdnimg.cn/20181026131632437.JPG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xldmlvcGt1,size_27,color_FFFFFF,t_70" alt="" /><figcaption>img</figcaption>
</figure>
<p>AugGan结构如上图：</p>
<ol type="1">
<li>整体迁移是有方向的，X代表源域图像，Y代表目标域图像。$ E_x, E_y<span class="math inline">\(分别代表X和Y的encoder，将图像降维后会得到特征域Z，Z会对接两个decoder，分别输出预测的分割图\)</span><em>{},</em>{}<span class="math inline">\(，以及生成的fake image\)</span> {X},{Y}$。</li>
<li>augGan使用了cycleGan类似的循环结构，保证循环一致性，<span class="math inline">\(\bar{Y}\)</span>经过<span class="math inline">\(E_y，G_y\)</span>得到重构后的$ X_{rec}<span class="math inline">\(，对\)</span>{X}$同理。</li>
<li>有两个判别器<span class="math inline">\(D_x,D_y\)</span></li>
<li>augGan对于decoder中的残差结构使用了hard-share权重共享，对于上采样的结构使用soft-share</li>
</ol>
<h4 id="loss-设计">loss 设计</h4>
<p>总体loss:</p>
<p><span class="math inline">\(\begin{aligned} \mathcal{L}_{\text {full }} &amp;=\mathcal{L}_{G A N}\left(E_{x}, G_{x}, D_{x}, X, Y\right)+\mathcal{L}_{G A N}\left(E_{y}, G_{y}, D_{y}, Y, X\right) \\ &amp;+\lambda_{\text {cyc }} * \mathcal{L}_{\text {cyc }}\left(E_{x}, G_{x}, E_{y}, G_{y}, X, Y\right) \\ &amp;+\lambda_{\text {seg }} *\left(\mathcal{L}_{\text {seg }}\left(E_{x}, P_{x}, X, \hat{X}\right)+\mathcal{L}_{\text {seg }}\left(E_{y}, P_{y}, Y, \hat{Y}\right)\right) \\ &amp;+\lambda_{\omega} *\left(\mathcal{L}_{\omega_{x}}\left(\omega_{G_{x}}, \omega_{P_{x}}\right)+\mathcal{L}_{\omega_{y}}\left(\omega_{G_{y}}, \omega_{P_{y}}\right)\right) \end{aligned}\)</span></p>
<ol type="1">
<li><p>结构意识（structure-aware）</p>
<p>在转换图像的时候，让模型明白不同位置是什么样的背景(语义特征)有利于迁移，AugGan使用分割的标签来实现。</p>
<p><span class="math inline">\(\begin{aligned} \mathcal{L}_{\text {seg }-x}\left(P_{x}, E_{x}, X, \hat{X}\right) &amp;=\lambda_{\text {seg-L1 }} \mathbb{E}_{x \sim p_{\text {data }(x)}}\left[\left\|P_{x}\left(E_{x}(x)\right)-\hat{x}\right\|_{1}\right] \\ &amp;+\lambda_{\text {seg-crossentropy }} \mathbb{E}_{x \sim p_{\text {data }(x)}}\left[\left\|\log \left(P_{x}\left(E_{x}(x)\right)-\hat{x}\right)\right\|_{1}\right] \end{aligned}\)</span></p>
<p>L1loss以及交叉熵损失</p></li>
<li><p>权值共享</p>
<p>hard-sharing: 完全共享一样的权值</p>
<p>soft-sharing：通过loss进行约束</p>
<p>$ <em>{}(</em>{G}, <em>{P})=-((</em>{G_{x}} <em>{P</em>{x}} /|<em>{G</em>{x}}|<em>{2}|</em>{P_{x}}|_{2})^{2}) $</p></li>
<li><p>循环一致性(Cycle consistency)</p>
<p>生成的图片经过重新编码解码能够还原回原来的图片，这样可以保证P(z|X)不会退化为P(z)（不会随机生成目标域的图像）。</p>
<p><span class="math inline">\(\begin{aligned} \mathcal{L}_{c y c}\left(E_{x}, G_{x}, E_{y}, G_{y}, X, Y\right) &amp;=\mathbb{E}_{x \sim p_{\text {data }(x)}}\left[\left\|G_{y}\left(E_{y}\left(G_{x}\left(E_{x}(x)\right)\right)\right)-\mathrm{x}\right\|_{1}\right] \\ &amp;+\mathbb{E}_{y \sim p_{\text {data }(y)}}\left[\left\|G_{x}\left(E_{x}\left(G_{y}\left(E_{y}(y)\right)\right)\right)-y\right\|_{1}\right] \end{aligned}\)</span></p></li>
<li><p>对抗训练</p></li>
</ol>
<p>​ 论文中对抗训练使用了原始的GanLoss：</p>
<p>​ <span class="math inline">\(\begin{aligned} \mathcal{L}_{G A N_{1}}\left(E_{x}, G_{x}, D_{x}, X, Y\right) &amp;=\mathbb{E}_{y \sim p_{\text {data }(y)}}\left[\log D_{x}(y)\right] \\ &amp;+\mathbb{E}_{x \sim p_{\text {data }(x)}}\left[\log \left(1-D_{x}\left(G_{x}\left(E_{x}(x)\right)\right)\right)\right] \end{aligned}\)</span></p>
<h4 id="迁移效果">迁移效果</h4>
<figure>
<img src="https://img-blog.csdnimg.cn/2018110116401050.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xldmlvcGt1,size_16,color_FFFFFF,t_70" alt="" /><figcaption>img</figcaption>
</figure>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dsttsd.github.io/2022/01/01/leetcode/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/50662067?s=400&u=b552d8b742d1e685ed0ddcc6a97d9f697535fa6b&v=4">
      <meta itemprop="name" content="DST">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DSTの杂货铺">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/01/leetcode/" class="post-title-link" itemprop="url">leetcode每日一题</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-01 12:15:55" itemprop="dateCreated datePublished" datetime="2022-01-01T12:15:55+08:00">2022-01-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-06-28 15:01:06" itemprop="dateModified" datetime="2022-06-28T15:01:06+08:00">2022-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">算法</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="连续整数求和">2022/6/3 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/consecutive-numbers-sum/">829. 连续整数求和</a></h1>
<p>type：数学</p>
<figure>
<img src="https://s2.loli.net/2022/06/28/nvNXxetwJCQq9L8.png" alt="" /><figcaption>image-20220628142602227.png</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2022/06/28/95LK3mn2XIYrQiU.png" alt="" /><figcaption>image-20220628142629543.png</figcaption>
</figure>
<h1 id="独特的电子邮件地址">2022/6/4 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/unique-email-addresses/">929. 独特的电子邮件地址</a></h1>
<p>type:模拟、字符串</p>
<figure>
<img src="https://s2.loli.net/2022/06/28/oekSq91PAO43C25.png" alt="" /><figcaption>image-20220628142802950.png</figcaption>
</figure>
<p>https://www.runoob.com/java/java-string.html</p>
<h1 id="在圆内随机生成点">2022/6/5 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/generate-random-point-in-a-circle/">478. 在圆内随机生成点</a></h1>
<p>type:数学、采样</p>
<ol type="1">
<li>拒绝采样， 过滤红色部分的点</li>
<li>从pdf到cdf推导</li>
</ol>
<figure>
<img src="https://s2.loli.net/2022/06/28/C6jnNoJUGiIxa9E.png" alt="" /><figcaption>image-20220628143039943.png</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2022/06/28/imQXR3xqg1NDvbH.png" alt="" /><figcaption>image-20220628143102635.png</figcaption>
</figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    Random random;</span><br><span class="line">    <span class="keyword">double</span> xc, yc, r;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Solution</span><span class="params">(<span class="keyword">double</span> radius, <span class="keyword">double</span> x_center, <span class="keyword">double</span> y_center)</span> </span>&#123;</span><br><span class="line">        random = <span class="keyword">new</span> Random();</span><br><span class="line">        xc = x_center;</span><br><span class="line">        yc = y_center;</span><br><span class="line">        r = radius;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">double</span>[] randPoint() &#123;</span><br><span class="line">        <span class="keyword">double</span> u = random.nextDouble();</span><br><span class="line">        <span class="keyword">double</span> theta = random.nextDouble() * <span class="number">2</span> * Math.PI;</span><br><span class="line">        <span class="keyword">double</span> r = Math.sqrt(u);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">double</span>[]&#123;xc + r * Math.cos(theta) * <span class="keyword">this</span>.r, yc + r * Math.sin(theta) * <span class="keyword">this</span>.r&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="我的日程安排表-iii">2022/6/6 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/my-calendar-iii/">732. 我的日程安排表 III</a></h1>
<p>type: 差分数组、线段树</p>
<figure>
<img src="https://s2.loli.net/2022/06/28/WUcSTKIdF4Ylotq.png" alt="" /><figcaption>image-20220628143224257.png</figcaption>
</figure>
<figure>
<img src="https://s2.loli.net/2022/06/28/eKlFa52RYWMZb8q.png" alt="" /><figcaption>image-20220628143241743.png</figcaption>
</figure>
<h1 id="爱吃香蕉的珂珂">2022/6/7 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/koko-eating-bananas/">875. 爱吃香蕉的珂珂</a></h1>
<p>type: 二分查找</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minEatingSpeed</span><span class="params">(<span class="keyword">int</span>[] piles, <span class="keyword">int</span> h)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> l_speed = <span class="number">1</span>, r_speed = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> pile: piles)&#123;</span><br><span class="line">            r_speed = Math.max(r_speed, pile);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> k = r_speed;</span><br><span class="line">        <span class="keyword">while</span>(l_speed &lt; r_speed)&#123;</span><br><span class="line">            <span class="keyword">int</span> speed = (r_speed - l_speed) / <span class="number">2</span> + l_speed;</span><br><span class="line">            <span class="comment">// calculate time consumpution</span></span><br><span class="line">            <span class="keyword">long</span> time = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> pile: piles)&#123;</span><br><span class="line">                <span class="comment">//(pile + speed - 1) / speed;</span></span><br><span class="line">                <span class="keyword">int</span> tmp = (<span class="keyword">int</span>)Math.ceil((<span class="keyword">double</span>)pile / speed); </span><br><span class="line">                time += tmp;</span><br><span class="line">            &#125;</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span>(time &lt;= h)&#123;</span><br><span class="line">                k = speed;</span><br><span class="line">                r_speed = speed;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                l_speed = speed + <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> k;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="非重叠矩形中的随机点">2022/6/9 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/random-point-in-non-overlapping-rectangles/">497. 非重叠矩形中的随机点</a></h1>
<p>type：数学、采样</p>
<p>先随机使用哪个矩形，再随机该矩形内的点</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span>[][] rs;</span><br><span class="line">    <span class="keyword">int</span>[] sum;</span><br><span class="line">    <span class="keyword">int</span> size;</span><br><span class="line">    Random random = <span class="keyword">new</span> Random();</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Solution</span><span class="params">(<span class="keyword">int</span>[][] rects)</span> </span>&#123;</span><br><span class="line">        rs = rects;</span><br><span class="line">        size = rs.length;</span><br><span class="line">        sum = <span class="keyword">new</span> <span class="keyword">int</span> [size+<span class="number">1</span>];</span><br><span class="line">        <span class="comment">// 构建前缀和数组</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=size;++i)&#123;</span><br><span class="line">            <span class="keyword">int</span> area = (rs[i - <span class="number">1</span>][<span class="number">2</span>] - rs[i - <span class="number">1</span>][<span class="number">0</span>] + <span class="number">1</span>) * (rs[i - <span class="number">1</span>][<span class="number">3</span>] - rs[i - <span class="number">1</span>][<span class="number">1</span>] + <span class="number">1</span>);</span><br><span class="line">            sum[i] = sum[i-<span class="number">1</span>] + area;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] pick() &#123;</span><br><span class="line">        <span class="keyword">int</span> val = random.nextInt(sum[size]) + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">int</span> l = <span class="number">0</span>, r = size;</span><br><span class="line">        <span class="comment">// 二分找第一个使得面积小于val的矩形</span></span><br><span class="line">        <span class="keyword">while</span>(l &lt; r)&#123;</span><br><span class="line">            <span class="keyword">int</span> mid = l + (r-l) / <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">if</span>(sum[mid] &gt;= val) r=mid;</span><br><span class="line">            <span class="keyword">else</span> l = mid + <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> [] cur = rs[r-<span class="number">1</span>];</span><br><span class="line">        <span class="comment">// nextInt的取值是[0,n) ，不包括n</span></span><br><span class="line">        <span class="keyword">int</span> x = random.nextInt(cur[<span class="number">2</span>] - cur[<span class="number">0</span>] + <span class="number">1</span>) + cur[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> y = random.nextInt(cur[<span class="number">3</span>] - cur[<span class="number">1</span>] + <span class="number">1</span>) + cur[<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span> [] &#123;x, y&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="对角线遍历">2022/6/14 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/diagonal-traverse/">498. 对角线遍历</a></h1>
<p>type: 矩阵、找规律</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">每层的索引和相等：</span><br><span class="line">1. 假设矩阵无限大；</span><br><span class="line">2. 索引和为&#123;偶&#125;数，向上遍历，&#123;横&#125;索引值递减，遍历值依次是(x,0),(x-1,1),(x-2,2),...,(0,x)</span><br><span class="line">3. 索引和为&#123;奇&#125;数，向下遍历，&#123;纵&#125;索引值递减，遍历值依次是(0,y),(1,y-1),(2,y-2),...,(y,0)</span><br><span class="line"></span><br><span class="line">   每层的索引和:</span><br><span class="line">            0:              (00)</span><br><span class="line">            1:            (01)(10)</span><br><span class="line">            2:          (20)(11)(02)</span><br><span class="line">            3:        (03)(12)(21)(30)</span><br><span class="line">            4:      (40)(31)(22)(13)(04)</span><br><span class="line">            5:    (05)(14)(23)(32)(41)(50)</span><br><span class="line">            6:  (60)(51)................(06)</span><br><span class="line"></span><br><span class="line">        按照“层次”遍历，依次append在索引边界内的值即可</span><br></pre></td></tr></table></figure>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span>[] findDiagonalOrder(<span class="keyword">int</span>[][] mat) &#123;</span><br><span class="line">        <span class="keyword">if</span> (mat == <span class="keyword">null</span> || mat.length == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;&#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> m = mat.length, n = mat[<span class="number">0</span>].length;</span><br><span class="line">        <span class="keyword">int</span>[] res = <span class="keyword">new</span> <span class="keyword">int</span>[m * n];</span><br><span class="line">        <span class="keyword">int</span> sum = <span class="number">0</span>, idx = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(sum &lt; m+n)&#123;</span><br><span class="line">            <span class="keyword">if</span>(sum % <span class="number">2</span> == <span class="number">0</span>)&#123;</span><br><span class="line">                <span class="comment">// 向上遍历</span></span><br><span class="line">                <span class="keyword">int</span> r = sum&lt;m?sum:(m-<span class="number">1</span>);</span><br><span class="line">                <span class="keyword">int</span> c = sum - r;</span><br><span class="line">                <span class="keyword">while</span>(r&gt;=<span class="number">0</span> &amp;&amp; c&gt;=<span class="number">0</span> &amp;&amp; r&lt;m &amp;&amp; c&lt;n)&#123;</span><br><span class="line">                    res[idx++] = mat[r][c];</span><br><span class="line">                    r--;c++;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                <span class="comment">// 向下遍历</span></span><br><span class="line">                <span class="keyword">int</span> c = sum&lt;n?sum:(n-<span class="number">1</span>);</span><br><span class="line">                <span class="keyword">int</span> r = sum - c;</span><br><span class="line">                <span class="keyword">while</span>(r&gt;=<span class="number">0</span> &amp;&amp; c&gt;=<span class="number">0</span> &amp;&amp; r&lt;m &amp;&amp; c&lt;n)&#123;</span><br><span class="line">                    res[idx++] = mat[r][c];</span><br><span class="line">                    r++;c--;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            sum++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res; </span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="找树左下角的值">2022/6/22 <a target="_blank" rel="noopener" href="https://leetcode.cn/problems/find-bottom-left-tree-value/">513. 找树左下角的值</a></h1>
<p>type: 二叉树</p>
<ol type="1">
<li><p>DFS</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> max_depth, res;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findBottomLeftValue</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        dfs(root, <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(TreeNode node, <span class="keyword">int</span> depth)</span></span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(node == <span class="keyword">null</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(depth&gt;max_depth)&#123;</span><br><span class="line">            <span class="comment">// 保证每个depth存的都是最左边的结点</span></span><br><span class="line">            max_depth = depth;</span><br><span class="line">            res = node.val;</span><br><span class="line">        &#125;</span><br><span class="line">        dfs(node.left, depth + <span class="number">1</span>);</span><br><span class="line">        dfs(node.right, depth + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>BFS <figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> max_depth, res;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">findBottomLeftValue</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">        Deque&lt;TreeNode&gt; q = <span class="keyword">new</span> ArrayDeque&lt;&gt;();</span><br><span class="line">        q.addLast(root);</span><br><span class="line">        <span class="keyword">int</span> res = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(!q.isEmpty())&#123;</span><br><span class="line">            <span class="keyword">int</span> num = q.size();</span><br><span class="line">            res = q.peek().val;</span><br><span class="line">            <span class="keyword">while</span>(num&gt;<span class="number">0</span>)&#123;</span><br><span class="line">                TreeNode now = q.pollFirst();</span><br><span class="line">                <span class="keyword">if</span>(now.left != <span class="keyword">null</span>) q.addLast(now.left);</span><br><span class="line">                <span class="keyword">if</span>(now.right != <span class="keyword">null</span>) q.addLast(now.right);</span><br><span class="line">                num--;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://dsttsd.github.io/2021/10/31/week%202/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/50662067?s=400&u=b552d8b742d1e685ed0ddcc6a97d9f697535fa6b&v=4">
      <meta itemprop="name" content="DST">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DSTの杂货铺">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/10/31/week%202/" class="post-title-link" itemprop="url">第二次周报</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-10-31 14:35:57" itemprop="dateCreated datePublished" datetime="2021-10-31T14:35:57+08:00">2021-10-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-06-28 11:33:58" itemprop="dateModified" datetime="2022-06-28T11:33:58+08:00">2022-06-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%91%A8%E6%8A%A5/" itemprop="url" rel="index"><span itemprop="name">周报</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>week 2 内容：</p>
<ul>
<li><p>Jure leskovec cs224w 课程进度80%（作业进度60%）</p></li>
<li><p>论文阅读：FM、NGCF、light-gcn、cluster-GCN</p></li>
<li><p>毕设准备</p></li>
</ul>
<h2 id="gnn">GNN</h2>
<h3 id="gcn">GCN</h3>
<p>就像"卷积"这个名字所指代的那样，这个想法来自于图像，之后引进到图（Graphs）中。然而，图像有固定的结构时，而图（Graphs）就复杂得多，结构不定。</p>
<figure>
<img src="https://static.leiphone.com/uploads/new/sns/article/202009/1599636339941872.png" alt="" /><figcaption>img</figcaption>
</figure>
<p><img src="https://i.loli.net/2020/05/16/uNVmrZ6aywn9QGj.png" alt="computation_graph_for_a" style="zoom:67%;" /></p>
<p>两个步骤：</p>
<ol type="1">
<li><p>propagation（message passing)</p></li>
<li><p>aggregation</p></li>
</ol>
<p><span class="math display">\[
   h_v^{(l)} = W_l\cdot h_v^{(l-1)} + W_r \cdot AGG(\{h_u^{(l-1)}, \forall u \in N(v) \})
\]</span></p>
<p><span class="math display">\[
   AGG(\{h_u^{(l-1)}, \forall u \in N(v) \}) = \frac{1}{|N(v)|} \sum_{u\in N(v)} h_u^{(l-1)}
\]</span></p>
<figure>
<img src="https://img-blog.csdnimg.cn/20200723144122270.png#pic_center" alt="" /><figcaption>在这里插入图片描述</figcaption>
</figure>
<figure>
<img src="https://s1.ax1x.com/2020/03/31/GQNyxe.png" alt="" /><figcaption>GQNyxe.png</figcaption>
</figure>
<h3 id="graphsage">GraphSage</h3>
<ul>
<li><p>在 aggregate 的形式上做了进一步扩展—GraphSAGE的想法是采用一个通用的aggregation函数来表示黑箱的运算，可以采用不同的方法聚合其邻居，然后再将其与自身特征拼接</p></li>
<li><p>除了 mean，其实任意的将多个 embedding 映射到一个 embedding 的函数都可以用来做聚合，例如 pool，例如 LSTM，</p>
<figure>
<img src="https://img-blog.csdnimg.cn/2020072317002071.png#pic_center" alt="" /><figcaption>在这里插入图片描述</figcaption>
</figure></li>
</ul>
<h3 id="cluster-gcn">cluster-GCN</h3>
<p>https://blog.csdn.net/sinat_26811377/article/details/97810302</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_34384681/article/details/91691305">聚类总结</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/IvanSSSS/p/5824938.html">metis</a></p>
<p>目前基于SGD的gcn算法，大规模GCN的训练面临问题：</p>
<p>1）面临随着gcn层数增长，计算复杂度呈指数增长；</p>
<p>2）需要保存整个Graph和每个node的embedding，存储量巨大。</p>
<p>Cluster-GCN通过graph聚类算法来筛选联系紧密的sub-graph，从而在sub-graph中的一组node进行采样，并且限制该sub-graph中的邻居搜索，可以显著提高搜索效率。</p>
<figure>
<img src="https://img-blog.csdnimg.cn/20200130214534466.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2ppYW5qaWFuX2NvbWVfb24=,size_16,color_FFFFFF,t_70" alt="" /><figcaption>在这里插入图片描述</figcaption>
</figure>
<ol type="1">
<li>metis和graclus谱聚类</li>
<li>for循环，每次随机选一个cluster（without replacement，不放回）计算subgraph的梯度进行更新</li>
<li>直至优化完成</li>
</ol>
<h2 id="gnn在推荐协同过滤领域应用">GNN在推荐(协同过滤)领域应用</h2>
<h3 id="neural-collaborative-filtering">Neural Collaborative Filtering</h3>
<p><a target="_blank" rel="noopener" href="https://www.guyuehome.com/23647/notice.html">CF发展</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/396722911">NCF不如MF？</a></p>
<figure>
<img src="https://pic2.zhimg.com/80/v2-4c24fb24a8800b221704a39deef32129_720w.jpg" alt="" /><figcaption>img</figcaption>
</figure>
<p>研究点：</p>
<ul>
<li><p>关于推荐系统的深度学习研究不多；</p></li>
<li><p>对user和item的交互行为（interaction）的建模大多使用MF，对user和item的隐特征使用内积计算，而这是一种线性方式。而通过引入user/item偏置，提高MF效果也说明<strong>内积不足以捕捉到用户交互数据中的复杂结构信息</strong>。</p></li>
</ul>
<p>模型框架：</p>
<figure>
<img src="https://pica.zhimg.com/v2-f2325c6555f1f578c7f888529cbbc727_1440w.jpg?source=172ae18b" alt="" /><figcaption>【导读】Neural Collaborative Filtering</figcaption>
</figure>
<p>这里关注的问题：</p>
<p>并没有把user和item的交互信息本身编码进 embedding 中，而NGCF希望<strong>显式建模User-Item 之间的高阶连接性来提升 embedding</strong>。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_39388410/article/details/106970194">图神经网络应用</a></p>
<h3 id="ngcfsigir-2019">NGCF（SIGIR 2019）</h3>
<figure>
<img src="https://img-blog.csdnimg.cn/20200626153725528.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5Mzg4NDEw,size_16,color_FFFFFF,t_70" alt="" /><figcaption>在这里插入图片描述</figcaption>
</figure>
<p>推荐领域的user-item交互可以被看做是一个二部图。</p>
<p><strong>High-order Connectivity</strong> 解释高阶连通性如上图，图1左边为一般CF中user-item交互的二部图，双圆圈表示此时需要预测的用户u1，对于u1我们可以把有关他的连接扩展成右图的树形结构，l是能到达的路径长度（或者可以叫跳数），l=1表明能一步到达u1的item，此时可以看到最外层的跳数相同的i4跟i5相比（l都为3），用户u1对i4的兴趣可能要比i5高，因为i4-&gt;u2-&gt;i2-&gt;u1、i4-&gt;u3-&gt;i3-&gt;u1有两条路径，而i5-&gt;u2-&gt;i2-&gt;u1只有一条，所以i4的相似性会更高。所以如果能扩展成这样的路径连通性来解释用户的兴趣，就是高阶连通性。</p>
<p>完整模型：</p>
<figure>
<img src="https://img-blog.csdnimg.cn/20200626153736173.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5Mzg4NDEw,size_16,color_FFFFFF,t_70" alt="" /><figcaption>在这里插入图片描述</figcaption>
</figure>
<p>NGCF的完整模型如下图，可以分为三个部分来看：</p>
<p>Embeddings：对user和item的嵌入向量，普通的用id来嵌入就可以了 Embedding Propagation Layers：挖掘高阶连通性关系来捕捉交互以细化Embedding的多个嵌入传播层 Prediction Layer：用更新之后带有交互信息的 user 和 item Embedding来进行预测</p>
<p>message passing：</p>
<p>$ m_{u i}=(W_{1} e_{i}+W_{2}(e_{i} e_{u})) $</p>
<p>使用embedding后的user，item的特征$ e_u $ 、<span class="math inline">\(e_i\)</span> 作为输入，然后两者计算内积相似度来控制邻域的信息，再加回到item上，用权重W控制权重，最后的N是u和i的度用来归一化系数，可以看做是折扣系数，随着传播路径长度的增大，信息慢慢衰减。</p>
<p>aggregation：</p>
<p><span class="math inline">\(e_{u}^{(1)}=\operatorname{LeakyReLU}\left(m_{u \leftarrow u}+\sum_{i \in N_{u}} m_{u \leftarrow i}\right)\)</span></p>
<p>3-hop计算图：</p>
<figure>
<img src="https://img-blog.csdnimg.cn/20200626160412372.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5Mzg4NDEw,size_16,color_FFFFFF,t_70" alt="" /><figcaption>在这里插入图片描述</figcaption>
</figure>
<p><span class="math inline">\(E^{(l)}=\sigma\left((L+I) E^{(l-1)} W_{1}^{(l)}+L E^{(l-1)} \odot E^{(l-1)} W_{2}^{(l)}\right)\)</span></p>
<h3 id="light-gcnsigir-2020">light-GCN(SIGIR 2020)</h3>
<figure>
<img src="https://img-blog.csdnimg.cn/20200626162933411.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5Mzg4NDEw,size_16,color_FFFFFF,t_70" alt="" /><figcaption>在这里插入图片描述</figcaption>
</figure>
<p>NGCF主要遵循标准GCN变形得到，包括使用非线性激活函数和特征变换矩阵w1和W2。然而作者认为实际上这两种操作对于CF并没什么0用，理由在于不管是user还是item，他们的输入都只是ID嵌入得到的，即根本没有具体的语义（一般在GCN的应用场景中每个节点会带有很多的其他属性），所以在这种情况下，执行多个非线性转换不会有助于学习更好的特性；更糟糕的是，它可能会增加训练的困难，降低推荐的结果。</p>
<p><span class="math inline">\(\begin{aligned} e_{u}^{(k+1)} &amp;=\sum_{i \in N_{u}} \frac{1}{\sqrt{\left|N_{u}\right|} \sqrt{\left|N_{i}\right|}} e_{i}^{(k)} \\ e_{i}^{(k+1)} &amp;=\sum_{u \in N_{i}} \frac{1}{\sqrt{\left|N_{i}\right|} \sqrt{\left|N_{u}\right|}} e_{u}^{(k)} \end{aligned}\)</span></p>
<p>只聚合连接的邻居，连自连接都没有。最后的K层就直接组合在每个层上获得的嵌入，以形成用户（项）的最终表示:</p>
<p><span class="math inline">\(\begin{array}{c} e_{u}=\sum_{k=0}^{K} \alpha_{k} e_{u}^{(k)} \\ e_{i}=\sum_{k=0}^{K} \alpha_{k} e_{i}^{(k)} \end{array}\)</span></p>
<p>其中<span class="math display">\[ α_k=1 / (k+1) \]</span></p>
<p><strong>为什么要组合所有层？</strong></p>
<ul>
<li>GCN随着层数的增加会过平滑，直接用最后一层不合理</li>
<li>不同层的嵌入捕获不同的语义，而且更高层能捕获更高阶的信息，结合起来更加全面</li>
<li>将不同层的嵌入与加权和结合起来，可以捕获具有自连接的图卷积的效果，这是GCNs中的一个重要技巧</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DST</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","perpage":true,"js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
