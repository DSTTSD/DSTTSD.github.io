<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"dsttsd.github.io","root":"/","images":"/images","scheme":"Pisces","darkmode":false,"version":"8.8.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="内容：  mlapp进度：第一章、第二章的65% BPR: Bayesian Personalized Ranking from Implicit Feedback  (精读) Jure leskovec cs224w 课程进度50%  1 简介1.1 是什么以及为什么数据的泛滥要求有对数据进行自动分析的方法，这便是机器学习所能解决的问题。我们定义机器学习（Machine Learning）为一系">
<meta property="og:type" content="article">
<meta property="og:title" content="第一次周报">
<meta property="og:url" content="https://dsttsd.github.io/2021/10/24/week%201/index.html">
<meta property="og:site_name" content="DSTの杂货铺">
<meta property="og:description" content="内容：  mlapp进度：第一章、第二章的65% BPR: Bayesian Personalized Ranking from Implicit Feedback  (精读) Jure leskovec cs224w 课程进度50%  1 简介1.1 是什么以及为什么数据的泛滥要求有对数据进行自动分析的方法，这便是机器学习所能解决的问题。我们定义机器学习（Machine Learning）为一系">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p%28x%29+%3D+%5Cdfrac%7B1%7D%7B%5Csigma%5Csqrt%7B2%5Cpi%7D%7D+exp%28-%5Cfrac%7B%28x-%5Cmu%29%5E2%7D%7B2%5Csigma%5E2%7D%29+%5Ctag%7B3%7D+">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p%28%5Ctextbf%7Bv%7D%29+%3D+%5Cfrac%7B1%7D%7B%282%5Cpi%29%5E%7Bn%2F2%7D+%7C%5CSigma%7C%5E%7B1%2F2%7D%7D+exp%28-%5Cfrac%7B1%7D%7B2%7D%28%5Ctextbf%7Bx%7D-%5Cmu%29%5ET+%5CSigma%5E%7B-1%7D+%28%5Ctextbf%7Bx%7D-%5Cmu%29%29+%5C%5C+%5Ctextbf%7Bv%7D+%5Cin+%5Cmathbb%7BR%7D%5En+%5Ctag%7B9%7D+">
<meta property="og:image" content="https://s2.loli.net/2021/12/25/RxcE9dSCweP2hrq.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/25/t4MYvKdoPnZVxsf.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/25/93xZ1VeATiSnRK2.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/25/sWPdSw24yDfNBZ7.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/25/V7v3ZFnQOoEyRA1.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/25/PE9vbrFj1VO3l4a.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/25/4UraMKjloQF6bS9.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/25/ja1suDdom6r5PM9.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/25/GylDqE3hiC1aLS9.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/25/4ofJgiPLahOwFpd.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/25/hbUq4kFBErZDLt1.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/25/3SkqcXDU8fdrlib.png">
<meta property="og:image" content="https://s2.loli.net/2021/12/25/Qay7klbJnVrOt6o.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20200103104441558.png">
<meta property="article:published_time" content="2021-10-24T10:34:57.000Z">
<meta property="article:modified_time" content="2021-12-25T07:40:13.104Z">
<meta property="article:author" content="DST">
<meta property="article:tag" content="周报">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.zhihu.com/equation?tex=p%28x%29+%3D+%5Cdfrac%7B1%7D%7B%5Csigma%5Csqrt%7B2%5Cpi%7D%7D+exp%28-%5Cfrac%7B%28x-%5Cmu%29%5E2%7D%7B2%5Csigma%5E2%7D%29+%5Ctag%7B3%7D+">


<link rel="canonical" href="https://dsttsd.github.io/2021/10/24/week%201/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://dsttsd.github.io/2021/10/24/week%201/","path":"2021/10/24/week 1/","title":"第一次周报"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>第一次周报 | DSTの杂货铺</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">DSTの杂货铺</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">学习笔记</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">2</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">2</span></a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">2</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E7%AE%80%E4%BB%8B"><span class="nav-number">1.</span> <span class="nav-text">1 简介</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E6%98%AF%E4%BB%80%E4%B9%88%E4%BB%A5%E5%8F%8A%E4%B8%BA%E4%BB%80%E4%B9%88"><span class="nav-number">1.0.1.</span> <span class="nav-text">1.1 是什么以及为什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%EF%BC%88Supervised-learning%EF%BC%89"><span class="nav-number">1.0.2.</span> <span class="nav-text">1.2 监督学习（Supervised learning）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-%E5%88%86%E7%B1%BB"><span class="nav-number">1.0.2.1.</span> <span class="nav-text">1.2.1 分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-1-%E5%9B%9E%E5%BD%92"><span class="nav-number">1.0.2.2.</span> <span class="nav-text">1.2.1 回归</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.0.3.</span> <span class="nav-text">1.3无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-1-%E5%8F%91%E7%8E%B0%E8%81%9A%E7%B1%BB"><span class="nav-number">1.0.3.1.</span> <span class="nav-text">1.3.1 发现聚类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-2-%E5%8F%91%E7%8E%B0%E6%BD%9C%E5%9C%A8%E5%9B%A0%E5%AD%90-Discovering-latent-factors"><span class="nav-number">1.0.3.2.</span> <span class="nav-text">1.3.2 发现潜在因子(Discovering latent factors)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-3-%E5%8F%91%E7%8E%B0%E5%9B%BE%E7%BB%93%E6%9E%84-Discovering-graph-structure"><span class="nav-number">1.0.3.3.</span> <span class="nav-text">1.3.3 发现图结构(Discovering graph structure)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-4%E7%9F%A9%E9%98%B5%E8%A1%A5%E5%85%A8-Matrix-completion"><span class="nav-number">1.0.3.4.</span> <span class="nav-text">1.3.4矩阵补全(Matrix completion)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">1.0.4.</span> <span class="nav-text">1.4  机器学习中的一些基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-1%E5%8F%82%E6%95%B0%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%9D%9E%E5%8F%82%E6%95%B0%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%8F%82%E6%95%B0%E9%87%8F%E7%9A%84%E5%8C%BA%E5%88%AB%EF%BC%89"><span class="nav-number">1.0.4.1.</span> <span class="nav-text">1.4.1参数模型和非参数模型（参数量的区别）</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%B4%E5%BA%A6%E7%81%BE%E9%9A%BE%EF%BC%88curse-of-dimensionality%EF%BC%89"><span class="nav-number">1.0.4.1.1.</span> <span class="nav-text">维度灾难（curse of dimensionality）</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-2-%E5%88%86%E7%B1%BB%E5%92%8C%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.0.4.2.</span> <span class="nav-text">1.4.2 分类和回归中的参数模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-3-%E8%BF%87%E6%8B%9F%E5%90%88-Overfitting"><span class="nav-number">1.0.4.3.</span> <span class="nav-text">1.4.3 过拟合(Overfitting)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-4-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="nav-number">1.0.4.4.</span> <span class="nav-text">1.4.4 模型选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-5-%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E7%9A%84%E5%8D%88%E9%A4%90%E7%90%86%E8%AE%BA-No-free-lunch-theorem"><span class="nav-number">1.0.4.5.</span> <span class="nav-text">1.4.5 没有免费的午餐理论(No free lunch theorem)</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E6%A6%82%E7%8E%87"><span class="nav-number">2.</span> <span class="nav-text">2 概率</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-%E5%9F%BA%E6%9C%AC%E5%AE%9A%E7%90%86"><span class="nav-number">2.0.0.1.</span> <span class="nav-text">2.2.2 基本定理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-3-%E7%94%9F%E6%88%90%E5%BC%8F%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">2.0.0.2.</span> <span class="nav-text">2.2.3 生成式分类器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-4-%E7%8B%AC%E7%AB%8B%E6%80%A7%E5%92%8C%E6%9D%A1%E4%BB%B6%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="nav-number">2.0.0.3.</span> <span class="nav-text">2.2.4 独立性和条件独立性</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-5-%E8%BF%9E%E7%BB%AD%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="nav-number">2.0.0.4.</span> <span class="nav-text">2.2.5 连续随机变量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-6-%E5%88%86%E4%BD%8D%E6%95%B0"><span class="nav-number">2.0.0.5.</span> <span class="nav-text">2.2.6 分位数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-7-%E6%9C%9F%E6%9C%9B%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="nav-number">2.0.0.6.</span> <span class="nav-text">2.2.7 期望和方差</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E5%B8%B8%E8%A7%81%E7%9A%84%E7%A6%BB%E6%95%A3%E5%88%86%E5%B8%83"><span class="nav-number">2.0.1.</span> <span class="nav-text">2.3 常见的离散分布</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BPR-Bayesian-Personalized-Ranking-from-Implicit-Feedback"><span class="nav-number">2.1.</span> <span class="nav-text">BPR: Bayesian Personalized Ranking from Implicit Feedback</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Uncertainty-in-Artificial-Intelligence-UAI-2009-%E2%80%94%E2%80%94%E7%9B%AE%E5%89%8DCCF-B"><span class="nav-number">2.1.1.</span> <span class="nav-text">Uncertainty in Artificial Intelligence (UAI) 2009 ——目前CCF B</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%9C%E8%80%85%EF%BC%9A-Steffen-Rendle-%EF%BC%88google-%E6%8E%A8%E8%8D%90%E5%A4%A7%E4%BD%AC%EF%BC%89"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">作者： Steffen Rendle  （google 推荐大佬）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%83%8C%E6%99%AF"><span class="nav-number">2.1.2.</span> <span class="nav-text">背景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B4%A1%E7%8C%AE"><span class="nav-number">2.1.3.</span> <span class="nav-text">贡献</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E9%80%9F%E6%88%90"><span class="nav-number">2.1.4.</span> <span class="nav-text">算法速成</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1%E6%95%B0%E6%8D%AEpair%E5%8C%96%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9A"><span class="nav-number">2.1.4.1.</span> <span class="nav-text">1数据pair化预处理：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2%E5%81%87%E8%AE%BE"><span class="nav-number">2.1.4.2.</span> <span class="nav-text">2假设</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-BPR%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87%E6%8E%A8%E5%AF%BC"><span class="nav-number">2.1.4.3.</span> <span class="nav-text">3 BPR优化目标推导*</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E4%BA%BF%E4%BA%9B%E7%BB%86%E8%8A%82"><span class="nav-number">2.1.5.</span> <span class="nav-text">4 亿些细节</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#BPRLoss"><span class="nav-number">2.1.5.1.</span> <span class="nav-text">BPRLoss</span></a></li></ol></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="DST"
      src="https://avatars.githubusercontent.com/u/50662067?s=400&u=b552d8b742d1e685ed0ddcc6a97d9f697535fa6b&v=4">
  <p class="site-author-name" itemprop="name">DST</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/DSTTSD" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;DSTTSD" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://dsttsd.github.io/2021/10/24/week%201/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://avatars.githubusercontent.com/u/50662067?s=400&u=b552d8b742d1e685ed0ddcc6a97d9f697535fa6b&v=4">
      <meta itemprop="name" content="DST">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DSTの杂货铺">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          第一次周报
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2021-10-24 18:34:57" itemprop="dateCreated datePublished" datetime="2021-10-24T18:34:57+08:00">2021-10-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2021-12-25 15:40:13" itemprop="dateModified" datetime="2021-12-25T15:40:13+08:00">2021-12-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%91%A8%E6%8A%A5/" itemprop="url" rel="index"><span itemprop="name">周报</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>内容：</p>
<ol>
<li>mlapp进度：第一章、第二章的65%</li>
<li>BPR: Bayesian Personalized Ranking from Implicit Feedback  (精读)</li>
<li>Jure leskovec cs224w 课程进度50%</li>
</ol>
<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><h3 id="1-1-是什么以及为什么"><a href="#1-1-是什么以及为什么" class="headerlink" title="1.1 是什么以及为什么"></a>1.1 是什么以及为什么</h3><p><strong>数据的泛滥要求有对数据进行自动分析的方法</strong>，<strong>这便是机器学习所能解决的问题。</strong>我们定义<strong>机器学习（</strong>Machine Learning<strong>）为一系列可以自动挖掘数据中潜在模式的方法</strong>，并且利用发现的模式去预测未来的数据，或者在不确定的情况下执行其他的决策</p>
<p>在很多领域中，数据都呈现出一种<strong>长尾</strong>（long tail）效应，意味着经常发生的那些事件往往是很少的，大部分事件是很少发生的。比如说，我们经常遇到的那些单词往往也就是那么一小部分（比如“the”和“and”），但是大部分单词（比如“pareidolia”）却很少使用。类似的，有些电影和书很有名，但大部分并非如此。章节2.4.7将进一步介绍长尾效应。长尾效应所导致的一个结果是：<strong>我们只需要较少的数据就可以预测或者理解大部分行为</strong>。</p>
<p>分类：监督方法——分类和回归任务</p>
<p>​            无监督方法：模型的目标是在数据中找到“感兴趣的模式”。有时也称这种方法为<strong>知识发掘</strong>（knowledge discovery）</p>
<p>​            强化学习：在随机的奖励或者惩罚信号的情况下，模型可以更好地学习采取什么的行为。</p>
<h3 id="1-2-监督学习（Supervised-learning）"><a href="#1-2-监督学习（Supervised-learning）" class="headerlink" title="1.2 监督学习（Supervised learning）"></a>1.2 监督学习（Supervised learning）</h3><h4 id="1-2-1-分类"><a href="#1-2-1-分类" class="headerlink" title="1.2.1 分类"></a>1.2.1 分类</h4><ol>
<li><p>二分类</p>
</li>
<li><p>多分类</p>
</li>
<li><p>多标签分类：类别的标签彼此之间互不排斥（比如某人可以被归类为高的和强壮的），更好的方式是将它理解为预测多个相关二元类标签的问题（即所谓的<strong>多变量输出模型</strong>)</p>
<p>MAP：</p>
<script type="math/tex; mode=display">
\hat{y}=\hat{f}(\mathbf{x})=\arg \max_{c=1}^C\ p(y=c|\mathbf{x},\mathcal{D}) \tag{1.1}</script><p>当P远小于1的情况下，这种情况下，我们对这个选择并不是很确定，所以相较于给出这个不信任的答案，直接指出“我不知道”可能会更好，<strong>这在医疗或者金融等需要规避风险的领域尤其重要。</strong></p>
</li>
</ol>
<h4 id="1-2-1-回归"><a href="#1-2-1-回归" class="headerlink" title="1.2.1 回归"></a>1.2.1 回归</h4><p>回归所输出的变量是连续的。</p>
<p>在现实世界中的一些回归问题。</p>
<ul>
<li><p>基于当前市场的情况和一些其他的辅助信息预测明天的股票价格；</p>
</li>
<li><p>预测一个观看YouTube上特定视频的观众的年龄；</p>
</li>
<li><p>给定机器人各电机的控制信号(扭矩)，预测机器人手臂末端执行器在三维空间中的位置；</p>
</li>
<li><p>根据多种不同临床测量数据预测前列腺特异性抗原(PSA)在体内的数量；</p>
</li>
<li><p>使用天气数据，时间等信息预测某建筑内任意位置的温度。</p>
</li>
</ul>
<h3 id="1-3无监督学习"><a href="#1-3无监督学习" class="headerlink" title="1.3无监督学习"></a>1.3无监督学习</h3><ol>
<li><p>非监督学习是一个无条件概率估计</p>
</li>
<li><p>往往需要针对特征向量建立一个多变量概率模型</p>
</li>
</ol>
<h4 id="1-3-1-发现聚类"><a href="#1-3-1-发现聚类" class="headerlink" title="1.3.1 发现聚类"></a>1.3.1 发现聚类</h4><ol>
<li>在监督学习中，我们会被告知预测值有多少个类别，但在非监督学习中，我们可以自由选择簇（类）的数目。P(K|D)</li>
<li>估计每个数据点属于哪一个簇——P(zi=k|xi, D)</li>
</ol>
<h4 id="1-3-2-发现潜在因子-Discovering-latent-factors"><a href="#1-3-2-发现潜在因子-Discovering-latent-factors" class="headerlink" title="1.3.2 发现潜在因子(Discovering latent factors)"></a>1.3.2 发现潜在因子(Discovering latent factors)</h4><p> 降维背后的动力在于尽管实际的数据可能是高维的，但其可能只有少量的主导因素，这些潜在的主导因素被称为<strong>潜在因子</strong>（latent factors）。</p>
<h4 id="1-3-3-发现图结构-Discovering-graph-structure"><a href="#1-3-3-发现图结构-Discovering-graph-structure" class="headerlink" title="1.3.3 发现图结构(Discovering graph structure)"></a>1.3.3 发现图结构(Discovering graph structure)</h4><p>其中图节点表示变量，边表示变量之间的直接依赖关系。稀疏图的学习主要有两种应用:发现新知识和获得更好的联合概率密度估计器。</p>
<h4 id="1-3-4矩阵补全-Matrix-completion"><a href="#1-3-4矩阵补全-Matrix-completion" class="headerlink" title="1.3.4矩阵补全(Matrix completion)"></a>1.3.4矩阵补全(Matrix completion)</h4><ol>
<li><p>图像修复(Image inpainting)</p>
</li>
<li><p>协同过滤(Collaborative filtering)——共现矩阵的补全</p>
</li>
<li><p>购物篮分析(Market basket analysis)——<strong>频繁项集挖掘</strong>（frequent itemset mining）</p>
<p>基于概率的方法不好解释</p>
<p>这是数据挖掘和机器学习的典型区别:在数据挖掘中，更强调可解释模型，而在机器学习中，更强调准确的模型。</p>
</li>
</ol>
<h3 id="1-4-机器学习中的一些基本概念"><a href="#1-4-机器学习中的一些基本概念" class="headerlink" title="1.4  机器学习中的一些基本概念"></a>1.4  机器学习中的一些基本概念</h3><h4 id="1-4-1参数模型和非参数模型（参数量的区别）"><a href="#1-4-1参数模型和非参数模型（参数量的区别）" class="headerlink" title="1.4.1参数模型和非参数模型（参数量的区别）"></a>1.4.1参数模型和非参数模型（参数量的区别）</h4><p>模型是否具备固定的参数数量，或者说参数数量是否随训练样本的增加而增加。前者被称为<strong>参数模型</strong>（parametric model），后者被称为<strong>非参数模型</strong>（non-parametric model）。</p>
<ol>
<li><p>非参数分类器：K近邻（K-nearest neighbors）</p>
<p>参考距离该点最近的K个训练集中的样本（投票选出class）</p>
</li>
</ol>
<script type="math/tex; mode=display">
xp(y=c|\mathbf{x},\mathcal{D},K)=\frac{1}{K}\sum_{i\in N_K(\mathbf{x},\mathcal{D})}\mathbb{I}(y_i=c) \tag{1.2}</script><script type="math/tex; mode=display">
 \mathbb{I}(e)=\begin{cases}1, & \mbox{if }e\mbox{ is true} \\0, & \mbox{if }e\mbox{ is false}\end{cases} \tag{1.3}</script><p>被称为<strong>基于存储的学习</strong>（memory-based learning）或者<strong>基于样例的学习</strong>（instance-based learning）。在这个方法中，最常用的距离测度为欧氏距离（该距离将这个技术限制在了实数领域），尽管其他的测度也可以使用。当K=1时，KNN分类器将得到关于所有点的<strong>泰森多边形图</strong>.</p>
<p>推荐系统中item-KNN 用到了基于社交的相似度计算法 。计算A与B的相似度，则是找到所有买过A的又买过B的用户，考虑评价偏差，计算完成后我们得到k个最相似的item。</p>
<h5 id="维度灾难（curse-of-dimensionality）"><a href="#维度灾难（curse-of-dimensionality）" class="headerlink" title="维度灾难（curse of dimensionality）"></a>维度灾难（curse of dimensionality）</h5><p>KNN分类器十分简单且效果很好，其前提是需要一个好的距离测度以及足够多的标记数据。然而，KNN的劣势在于它不适用于高维输入。造成这个问题的原因是<strong>维度灾难</strong>（curse of dimensionality）。当维度上升是，数据分布会相对稀疏，丧失“局部性”</p>
<h4 id="1-4-2-分类和回归中的参数模型"><a href="#1-4-2-分类和回归中的参数模型" class="headerlink" title="1.4.2 分类和回归中的参数模型"></a>1.4.2 分类和回归中的参数模型</h4><ol>
<li>线性回归</li>
</ol>
<script type="math/tex; mode=display">
y(\mathbf{x})=\mathbf{w}^T\mathbf{x}+\epsilon =\sum_{j=1}^Dw_jx_j+\epsilon \tag{1.4}</script><p>epsilon 服从高斯分布N(mu, sigma^2), 重写为：</p>
<script type="math/tex; mode=display">
 p(y|\mathbf{x},\mathbf{\theta})=\mathcal{N}(y|\mu(\mathbf{x}),\sigma^2(\mathbf{x})) \tag{1.5}</script><p>噪声固定的情况下：</p>
<script type="math/tex; mode=display">
x\mu(\mathbf{x})=w_0+w_1x=\mathbf{w}^T\mathbf{x} \tag{1.6}</script><p>线性回归模型同样可以对非线性关系进行建模（基函数拓展）：</p>
<script type="math/tex; mode=display">
 p(y|\mathbf{x},\mathbf{\theta})=\mathcal{N}(y|\mathbf{w}^T\phi(\mathbf{x}),\sigma^2) \tag{1.7}</script><ol>
<li>逻辑回归</li>
</ol>
<p>将y分布转换为伯努利分布：</p>
<script type="math/tex; mode=display">
p(y|\mathbf{x},\mathbf{w})={\rm{Ber}}(y|\mu(\mathbf{x}))\tag{1.8}</script><script type="math/tex; mode=display">
\mu(x) = {\rm{sigm}}(\eta)</script><script type="math/tex; mode=display">
{\rm{sigm}}(\eta)\triangleq\frac{1}{1+\exp(-\eta)}=\frac{e^\eta}{e^\eta+1} \tag{1.10}</script><p>术语”sigmoid”表示<strong>S型</strong>：如图1.19(a)所示。它又被称为<strong>压缩函数</strong>，因为它将整个实轴映射到了区间[0,1]</p>
<script type="math/tex; mode=display">
p(y|\mathbf{x},\mathbf{w})={\rm{Ber}}(y|{\rm{sigm}}(\mathbf{w}^T\mathbf{x})) \tag{1.11}</script><h4 id="1-4-3-过拟合-Overfitting"><a href="#1-4-3-过拟合-Overfitting" class="headerlink" title="1.4.3 过拟合(Overfitting)"></a>1.4.3 过拟合(Overfitting)</h4><p>应该避免对输入中的任何微小的变化进行建模，因为那些微小的变化很有可能是噪音而非真实的信号。</p>
<h4 id="1-4-4-模型选择"><a href="#1-4-4-模型选择" class="headerlink" title="1.4.4 模型选择"></a>1.4.4 模型选择</h4><h4 id="1-4-5-没有免费的午餐理论-No-free-lunch-theorem"><a href="#1-4-5-没有免费的午餐理论-No-free-lunch-theorem" class="headerlink" title="1.4.5 没有免费的午餐理论(No free lunch theorem)"></a>1.4.5 没有免费的午餐理论(No free lunch theorem)</h4><blockquote>
<p>All models are wrong, but some models are useful.—George Box</p>
</blockquote>
<p>对于每个模型，可能有很多不同算法去训练它们，且不同算法都在速度—精度—复杂度之间权衡</p>
<h1 id="2-概率"><a href="#2-概率" class="headerlink" title="2 概率"></a>2 概率</h1><p>概率的两种解释：</p>
<ol>
<li><p>频率派：概率代表事件在长时间实验的情况下出现的频率。</p>
</li>
<li><p>贝叶斯派：概率是用来量化我们对于某些事件的<strong>不确定度</strong>（uncertainty）<br>贝叶斯解释的一个重要优势在于，它可以用来衡量那些无法进行重复试验的事件的不确定度。</p>
</li>
</ol>
<h4 id="2-2-2-基本定理"><a href="#2-2-2-基本定理" class="headerlink" title="2.2.2 基本定理"></a>2.2.2 基本定理</h4><ol>
<li><p>并集发生概率</p>
<script type="math/tex; mode=display">
\begin{align}p(A\or B) &= p(A)+p(B)-p(A\or B)  \tag{2.1} \\&=p(A)+p(B)  \ 如果A和B互斥         \tag{2.2} \\\end{align}</script></li>
<li><p>联合概率</p>
<script type="math/tex; mode=display">
 p(A,B)=p(A\and B)=p(A|B)p(B) \tag{2.3}</script><p>边缘概率（离散求和，连续积分）：<br>（<strong>求和法则</strong>（sum rule）或者叫<strong>全概率法则</strong>（rule of total probability）</p>
<script type="math/tex; mode=display">
p(A)=\sum_b p(A,B)=\sum_b p(A|B=b)p(B=b) \tag{2.4}</script><p> <strong>链式法则</strong>（chain rule）</p>
<script type="math/tex; mode=display">
 p(X_{1:D})=p(X_1)p(X_2|X_1)p(X_3|X_2,X_1)...p(X_D|X_{1:D-1}) \tag{2.5}</script><p> 1:D代表集合{1,2,3,…D}</p>
</li>
<li>条件概率</li>
</ol>
<script type="math/tex; mode=display">
p(A|B)=\frac{p(A,B)}{p(B)} \ {\rm{if}} \ p(B)\gt 0 \tag{2.6}</script><ol>
<li>贝叶斯定理<script type="math/tex; mode=display">
p(X=x|Y=y)=\frac{p(X=x,Y=y)}{p(Y=y)}=\frac{p(X=x)p(Y=y|X=x)}{\sum_{x^\prime}p(X=x^\prime)p(Y=y|X=x^\prime)} \tag{2.7}</script></li>
</ol>
<h4 id="2-2-3-生成式分类器"><a href="#2-2-3-生成式分类器" class="headerlink" title="2.2.3 生成式分类器"></a>2.2.3 生成式分类器</h4><script type="math/tex; mode=display">
p(y=c|\mathbf{x})=\frac{p(y=c)p(\mathbf{x}|y=c)}{\sum_{c^\prime}p(y=c^\prime|\mathbf{\theta})p(\mathbf{x}|y=c^\prime)} \tag{2.13}</script><p><strong>生成式分类器</strong>(generative classifier)，因为它通过使用<strong>类条件概率密度</strong>$p(\mathbf{x}|y=c)$和类先验分布$p(y=c)$来指定如何生成样本。</p>
<h4 id="2-2-4-独立性和条件独立性"><a href="#2-2-4-独立性和条件独立性" class="headerlink" title="2.2.4 独立性和条件独立性"></a>2.2.4 独立性和条件独立性</h4><ol>
<li><p>独立：</p>
<p>如果两个随机变量x和Y的联合概率分布可以表示为边缘分布的乘积，则称X和Y是<strong>无条件独立</strong>(unconditionally independent)或者<strong>边缘独立</strong>(marginally independent)的</p>
</li>
<li><p>条件独立：联合分布等于条件边缘分布的乘积,已知z情况下：x,y是独立的</p>
<script type="math/tex; mode=display">
p(X,Y|Z)=p(X|Z)p(Y|Z) \tag{2.15}</script><p> X  Y| Z 则意味着存在着函数 g 和 h ，对全部的x,y,z，在p(z)&gt;0的情况下满足下列关系：</p>
<script type="math/tex; mode=display">
p(x,y|z) = g(x,z)h(y,z) \tag{2.16}</script><p>条件独立性假设允许我们通过一些局部信息去构建大规模的概率模型。</p>
</li>
</ol>
<h4 id="2-2-5-连续随机变量"><a href="#2-2-5-连续随机变量" class="headerlink" title="2.2.5 连续随机变量"></a>2.2.5 连续随机变量</h4><p>X是个连续随机变量，有：</p>
<script type="math/tex; mode=display">
\begin{aligned}A & =(X\le a)\\B & =(X\le B)\\W & =(a\le X\le b)\end{aligned}</script><p>有:</p>
<script type="math/tex; mode=display">
B=A\bigvee W</script><script type="math/tex; mode=display">
p(B)=p(A)+p(W)</script><script type="math/tex; mode=display">
p(w)=p(B)-p(A)</script><p>X 的连续分布函数（cumulative distribution function，缩写为 cdf）:</p>
<script type="math/tex; mode=display">
F(q) \overset\triangle{=} p(X\le q)</script><p>这个 cdf 是一个单调递增函数（monotonically increasing function）</p>
<h4 id="2-2-6-分位数"><a href="#2-2-6-分位数" class="headerlink" title="2.2.6 分位数"></a>2.2.6 分位数</h4><p>cdf反函数，F^{-1}(0.5)为分布中位数：一半的概率质量分布在左边，另一半分布在右边。 </p>
<h4 id="2-2-7-期望和方差"><a href="#2-2-7-期望和方差" class="headerlink" title="2.2.7 期望和方差"></a>2.2.7 期望和方差</h4><script type="math/tex; mode=display">
\mathrm{E}[X] \overset\triangle{=} \sum_{x\in X}x p(x)</script><script type="math/tex; mode=display">
\begin{aligned}var[X]  & =E[(X-\mu)^2]=\int(x-\mu)^2p(x)dx      &\text{           (2.24)}\\& =  \int x^2p(x)dx  +\mu^2 \int p(x)dx-2\mu\int xp(x)dx=E[X^2]-\mu^2         &    \text{           (2.25)}\\\end{aligned}</script><script type="math/tex; mode=display">
E[X^2]= \mu^2+\sigma^2</script><script type="math/tex; mode=display">
std[X]\overset\triangle{=} \sqrt {var[X]}</script><h3 id="2-3-常见的离散分布"><a href="#2-3-常见的离散分布" class="headerlink" title="2.3 常见的离散分布"></a>2.3 常见的离散分布</h3><ol>
<li><p>二项分布</p>
<p>pmf:</p>
<script type="math/tex; mode=display">{X\sim Bin(n,\theta)}
Bin(k|n,\theta)\overset\triangle{=} \binom{n}{k} \theta ^k  (1- \theta)^{n-k}</script><p>均值方差：</p>
<script type="math/tex; mode=display">
mean=\theta, var =n\theta(1-\theta)</script></li>
</ol>
<ol>
<li><p>伯努利分布</p>
<p>伯努利分布只是二项分布中 n=1 的特例</p>
<script type="math/tex; mode=display">
Ber(x|\theta)=\begin{cases} \theta &\text{ if x =1} \\
1-\theta &\text{ if x =0} \end{cases}</script></li>
<li><p>多项式分布:对有 K 个可能结果的事件进行建模，就要用到多项分布（multinomial distribution）</p>
<script type="math/tex; mode=display">
 Mu(x|n,\theta) = \binom {n}{x_1,..,x_K}\prod^K_{j=1}\theta^{x_j}_j</script></li>
<li><p>多重伯努利分布</p>
</li>
<li><p>泊松分布</p>
</li>
<li><p>…</p>
</li>
</ol>
<h2 id="BPR-Bayesian-Personalized-Ranking-from-Implicit-Feedback"><a href="#BPR-Bayesian-Personalized-Ranking-from-Implicit-Feedback" class="headerlink" title="BPR: Bayesian Personalized Ranking from Implicit Feedback"></a>BPR: <strong>Bayesian</strong> <strong>Personalized Ranking</strong> from <strong>Implicit Feedback</strong></h2><h3 id="Uncertainty-in-Artificial-Intelligence-UAI-2009-——目前CCF-B"><a href="#Uncertainty-in-Artificial-Intelligence-UAI-2009-——目前CCF-B" class="headerlink" title="Uncertainty in Artificial Intelligence (UAI) 2009 ——目前CCF B"></a>Uncertainty in Artificial Intelligence (UAI) 2009 ——目前CCF B</h3><h4 id="作者：-Steffen-Rendle-（google-推荐大佬）"><a href="#作者：-Steffen-Rendle-（google-推荐大佬）" class="headerlink" title="作者： Steffen Rendle  （google 推荐大佬）"></a>作者： Steffen Rendle  （google 推荐大佬）</h4><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><ol>
<li><p>senario： implicit feedback</p>
</li>
<li><p>当时的item-recommendation方法没有针对排序（ranking）进行排序</p>
</li>
</ol>
<p><strong>BPR本身并不优化用户对物品的评分，而只借由评分来优化用户对物品的排序 即：it is a real ranking algorithm.</strong></p>
<h3 id="贡献"><a href="#贡献" class="headerlink" title="贡献"></a>贡献</h3><ol>
<li>后验概率推出BPR优化目标； 探讨和AUC的关系</li>
<li>优化算法learnBPR - SGD</li>
<li>展示如何应用到item-Knn以及MF；对比了WR-MF、MMMF</li>
<li>实验证明比其他方法好（包括比non-personalized 理论值）</li>
</ol>
<h3 id="算法速成"><a href="#算法速成" class="headerlink" title="算法速成"></a>算法速成</h3><h4 id="1数据pair化预处理："><a href="#1数据pair化预处理：" class="headerlink" title="1数据pair化预处理："></a>1数据pair化预处理：</h4><p>BPR算法将用户对物品的评分（显示反馈“1”，隐式反馈“0”）处理为一个pair对的集合<i,j>，其中i为评分为1的物品，j为评分为0的物品。假设某用户有M个“1”的评分，N个“0”的评分，则该用户共有M*N个pair对。这样数据集就由三元组 <u,i,j>表示，该三元组的物理含义为：相对于物品“j”，用户“u”更喜欢物品“i”。</p>
<h4 id="2假设"><a href="#2假设" class="headerlink" title="2假设"></a>2假设</h4><ol>
<li><p>一是每个用户之间的偏好行为相互独立，即用户u在商品i和j之间的偏好和其他用户无关</p>
</li>
<li><p>是同一用户对不同物品的偏序相互独立，也就是用户u在商品i和j之间的偏好和其他的商品无关。</p>
</li>
<li><em>P(θ) 服从高斯分布</em>（？）</li>
</ol>
<p><strong>问题：假设是否合理？</strong></p>
<p>我觉得现在来看前两个假设太强了</p>
<h4 id="3-BPR优化目标推导"><a href="#3-BPR优化目标推导" class="headerlink" title="3 BPR优化目标推导*"></a>3 BPR优化目标推导*</h4><p>排序关系符号$ &gt;u $:        </p>
<p>完整性：$∀i,j∈I:i≠j⇒i&gt;uj∪j&gt;ui ∀i,j∈I:i≠j⇒i&gt;uj∪j&gt;ui$  (i,j 不等，i &gt;u j 或j&gt;u i至少有一个)</p>
<p>反对称性：$∀i,j∈I:i&gt;uj∩j&gt;ui⇒i=j ∀i,j∈I:i&gt;uj∩j&gt;ui⇒i=j$ （如果既有i&gt;u j 又有 j&gt;ui, 说明i,j是同一个）</p>
<p>传递性：$∀i,j,k∈I:i&gt;u j∩j&gt;uk⇒i&gt;uk$ </p>
<p>BPR 基于最大后验估计P(W,H|&gt;u)来求解模型参数W,H,用θ来表示参数,&gt;u代表用户u对应的所有商品的全序关系,则优化目标是P(θ|&gt;u)。根据贝叶斯公式，我们有：</p>
<script type="math/tex; mode=display">
P(\theta|>_u) = \frac{P(>_u|\theta)P(\theta)}{P(>_u)}</script><script type="math/tex; mode=display">
P(\theta|>_u) \propto P(>_u|\theta)P(\theta)</script><ul>
<li><p>第一部分：优化目标转化为两部分。第一部分和样本数据集D有关，第二部分和样本数据集D无关。</p>
<script type="math/tex; mode=display">
\prod_{u \in U}P(>_u|\theta) = \prod_{(u,i,j) \in (U \times I \times I)}P(i >_u j|\theta)^{\delta((u,i,j) \in D)}(1-P(i >_u j|\theta))^{\delta((u,j,i) \not\in D) }</script><script type="math/tex; mode=display">
\delta(b)= \begin{cases} 1& {if\; b\; is \;true}\\ 0& {else} \end{cases}</script></li>
</ul>
<p>根据<strong>完整性和反对称性</strong>，优化目标的第一部分可以简化为：</p>
<script type="math/tex; mode=display">
\prod_{u \in U}P(>_u|\theta) = \prod_{(u,i,j) \in D}P(i >_u j|\theta)</script><p>  尝试用$sigmoid$表示$P(i&gt;uj|θ)$这个概率:</p>
<script type="math/tex; mode=display">
P(i >_u j|\theta) = \sigma(\overline{x}_{uij}(\theta))</script><script type="math/tex; mode=display">
\begin{align*} \sigma(x)&=\frac{1}{1+e^{-x}} \end{align*}</script><ul>
<li>第二部分：作者大胆使用了贝叶斯假设，即这个概率分布符合正太分布，且对应的均值是0，协方差矩阵是$\lambda_{\theta}I$</li>
</ul>
<script type="math/tex; mode=display">
P(\theta) \sim N(0, \lambda_{\theta}I)</script><p>高斯分布：<br><img src="https://www.zhihu.com/equation?tex=p%28x%29+%3D+%5Cdfrac%7B1%7D%7B%5Csigma%5Csqrt%7B2%5Cpi%7D%7D+exp%28-%5Cfrac%7B%28x-%5Cmu%29%5E2%7D%7B2%5Csigma%5E2%7D%29+%5Ctag%7B3%7D+" alt="[公式]"></p>
<p><img src="https://www.zhihu.com/equation?tex=p%28%5Ctextbf%7Bv%7D%29+%3D+%5Cfrac%7B1%7D%7B%282%5Cpi%29%5E%7Bn%2F2%7D+%7C%5CSigma%7C%5E%7B1%2F2%7D%7D+exp%28-%5Cfrac%7B1%7D%7B2%7D%28%5Ctextbf%7Bx%7D-%5Cmu%29%5ET+%5CSigma%5E%7B-1%7D+%28%5Ctextbf%7Bx%7D-%5Cmu%29%29+%5C%5C+%5Ctextbf%7Bv%7D+%5Cin+%5Cmathbb%7BR%7D%5En+%5Ctag%7B9%7D+" alt="[公式]"></p>
<p><strong>为什么使用0均值正态分布假设？</strong></p>
<p>1.简化计算？  $lnP(\theta) = \lambda||\theta||^2$</p>
<p>2.引出正则项？</p>
<p><strong>前两项结合，优化目标推导：</strong></p>
<p><img src="https://s2.loli.net/2021/12/25/RxcE9dSCweP2hrq.png" alt="image-20211021194242238"></p>
<p>对参数求导：</p>
<p><img src="https://s2.loli.net/2021/12/25/t4MYvKdoPnZVxsf.png" alt="image-20211021200205448"></p>
<p>注：<em>f</em>′(<em>x</em>)=<em>f</em>(<em>x</em>)∗(1−<em>f</em>(<em>x</em>))</p>
<script type="math/tex; mode=display">
\begin{align*} sigmoid^{'}(x)&=(\frac{1}{1+e^{-x}})^{'} \\ &=\frac{1}{1+e^{-x}}e^{-x}(-1)\\ &=\frac{e^{-x}}{(1+e^{-x})^2}\\ &=\frac{1}{1+e^{-x}}(1-\frac{1}{1+e^{-x}})\\ &=sigmoid(x)(1-sigmoid(x)) \end{align*}</script><p>使用SGD进行优化<br><img src="https://s2.loli.net/2021/12/25/93xZ1VeATiSnRK2.png" alt="image-20211021200345228"></p>
<h3 id="4-亿些细节"><a href="#4-亿些细节" class="headerlink" title="4 亿些细节"></a>4 亿些细节</h3><ol>
<li><p>与AUC的关系</p>
<p>AUC：</p>
<p><img src="https://s2.loli.net/2021/12/25/sWPdSw24yDfNBZ7.png" alt=""></p>
<p><img src="https://s2.loli.net/2021/12/25/V7v3ZFnQOoEyRA1.png" alt="image-20211021230825086"></p>
<p>使用论文notation：</p>
<p><img src="https://s2.loli.net/2021/12/25/PE9vbrFj1VO3l4a.png" alt="image-20211021230915429"></p>
<p><img src="https://s2.loli.net/2021/12/25/4UraMKjloQF6bS9.png" alt="image-20211021231013168"></p>
<p><img src="https://s2.loli.net/2021/12/25/ja1suDdom6r5PM9.png" alt="image-20211021232005411"></p>
</li>
</ol>
<ol>
<li><p>应用于knn和MF</p>
<p>MF：</p>
<p>用了和funkSVD类似的矩阵分解模型，这里BPR对于用户集U和物品集I的对应的$U×I$的预测排序矩阵$\overline{X}，$我们期望得到两个分解后的用户矩阵$W(|U|×k)$和物品矩阵$H(|I|×k)$，满足</p>
<script type="math/tex; mode=display">
\overline{X}=WHT</script><script type="math/tex; mode=display">
\overline{x}_{ui} = w_u \bullet h_i = \sum\limits_{f=1}^kw_{uf}h_{if}</script><script type="math/tex; mode=display">
\frac{\partial (\overline{x}_{ui})}{\partial \theta} = \begin{cases} (h_{if}-h_{jf})& {if\; \theta = w_{uf}}\\ w_{uf}& {if\;\theta = h_{if}} \\ -w_{uf}& {if\;\theta = h_{jf}}\end{cases}</script></li>
</ol>
<p>   itemKNN：</p>
<p>   <img src="https://s2.loli.net/2021/12/25/GylDqE3hiC1aLS9.png" alt="image-20211022182610996"></p>
<p>   相似度矩阵C : I × I  </p>
<p>   <img src="https://s2.loli.net/2021/12/25/4ofJgiPLahOwFpd.png" alt="image-20211021232313915"></p>
<ol>
<li><p>实验结果</p>
<p>使用了Rossann和Netflix数据集进行实验。使用AUC测评：</p>
<p><img src="https://s2.loli.net/2021/12/25/hbUq4kFBErZDLt1.png" alt="image-20211021225104841"></p>
</li>
</ol>
<p>   <img src="https://s2.loli.net/2021/12/25/3SkqcXDU8fdrlib.png" alt="image-20211021204544627"></p>
<p><img src="https://s2.loli.net/2021/12/25/Qay7klbJnVrOt6o.png" alt="image-20211021205021673"></p>
<p>问题：bootstrap应该会有没采到的吧？会有影响吗？</p>
<h4 id="BPRLoss"><a href="#BPRLoss" class="headerlink" title="BPRLoss"></a>BPRLoss</h4><p>BPR Loss 的思想很简单，就是让正样本和负样本的得分之差尽可能达到最大.</p>
<p><img src="https://img-blog.csdnimg.cn/20200103104441558.png" alt="BPRLoss"></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%91%A8%E6%8A%A5/" rel="tag"># 周报</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
            </div>
            <div class="post-nav-item">
                <a href="/2021/12/18/hello-world/" rel="next" title="Hello World">
                  Hello World <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DST</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","perpage":true,"js":{"url":"https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js","integrity":"sha256-r+3itOMtGGjap0x+10hu6jW/gZCzxHsoKrOd7gyRSGY="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
